
@book{kraut_building_2011,
	location = {Cambridge, {MA}},
	title = {Building Successful Online Communities},
	publisher = {{MIT} Press},
	author = {Kraut, Robert E. and Resnick, Paul},
	date = {2011},
	keywords = {{citedDissProp}, crowdsourcing}
}

@article{ryan_intrinsic_2000,
	title = {Intrinsic and Extrinsic Motivations: Classic Definitions and New Directions},
	volume = {25},
	issn = {0361-{476X}},
	url = {http://www.sciencedirect.com/science/article/pii/S0361476X99910202},
	doi = {10.1006/ceps.1999.1020},
	shorttitle = {Intrinsic and Extrinsic Motivations},
	abstract = {Intrinsic and extrinsic types of motivation have been widely studied, and the distinction between them has shed important light on both developmental and educational practices. In this review we revisit the classic definitions of intrinsic and extrinsic motivation in light of contemporary research and theory. Intrinsic motivation remains an important construct, reflecting the natural human propensity to learn and assimilate. However, extrinsic motivation is argued to vary considerably in its relative autonomy and thus can either reflect external control or true self-regulation. The relations of both classes of motives to basic human needs for autonomy, competence and relatedness are discussed.},
	pages = {54-67},
	number = {1},
	journaltitle = {Contemporary Educational Psychology},
	shortjournal = {Contemporary Educational Psychology},
	author = {Ryan, Richard M. and Deci, Edward L.},
	urldate = {2014-03-19},
	date = {2000},
	keywords = {{citedDissProp}, extrinsic motivation, Intrinsic motivation, motivation},
	file = {ScienceDirect Full Text PDF:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\Z97ITKP9\Ryan and Deci - 2000 - Intrinsic and Extrinsic Motivations Classic Defin.pdf:application/pdf;ScienceDirect Snapshot:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\QUNSM9WG\S0361476X99910202.html:text/html}
}

@article{zwass_cocreation:_2010,
	title = {{Co‑Creation:} Toward a Taxonomy and an Integrated Research Perspective},
	volume = {15},
	doi = {10.2753/JEC1086-4415150101},
	pages = {11-48},
	number = {1},
	journaltitle = {International Journal of Electronic Commerce},
	author = {Zwass, Vladamir},
	date = {2010},
	keywords = {{citedDissProp}, co-creation, crowdsourcing, Taxonomy},
	file = {p011full.pdf:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\M6RBJRK5\p011full.pdf:application/pdf}
}

@inproceedings{trant_investigating_2006,
	title = {Investigating social tagging and folksonomy in art museums with steve. museum},
	url = {http://www.ra.ethz.ch/cdstore/www2006/www.rawsugar.com/www2006/4.pdf},
	booktitle = {Proceedings of the {WWW’06} Collaborative Web Tagging Workshop},
	author = {Trant, Jennifer and Wyman, Bruce},
	urldate = {2013-12-16},
	date = {2006},
	keywords = {{citedDissProp}},
	file = {[PDF] from ethz.ch:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\URAIAPWJ\Trant and Wyman - 2006 - Investigating social tagging and folksonomy in art.pdf:application/pdf}
}

@inproceedings{organisciak_incidental_2013,
	location = {Lincoln, Nebraska},
	title = {Incidental Crowdsourcing: Crowdsourcing in the Periphery},
	url = {http://dh2013.unl.edu/abstracts/ab-273.html},
	abstract = {As the customs of the Internet grow increasingly collaborative, crowdsourcing offers an appealing frame for looking at the interaction of users with online systems and each other. However, it is a broad term that fails to emphasize the use of crowds in subtler system augmentation.

This paper introduces incidental crowdsourcing ({IC):} an approach to user-provided item description that adopts crowdsourcing as a frame for thinking about augmentative features of system design. {IC} is intended to frame discussion around peripheral and non-critical system design choices.

A provisional definition of incidental crowdsourcing will be defined in this paper, and then refined based on examples seen in practice. {IC} will be examined from both the user and system ends, positioned within existing work, and considered in the context of its benefits and drawbacks. This approach allows us to explore the robustness and feasibility of {IC}, looking at the implications inherent to accepting the provisional definition.

The consequences of considering system design on a scale between {IC} and non-{IC} design choices remain to be seen. Toward this goal, the second part of this paper shows a study comparing the participation habits of users in two online systems — one that is representative of {IC} properties and one that is not. This study finds differences in user engagement between the two systems.},
	eventtitle = {Digital Humanities 2013},
	author = {Organisciak, Peter},
	date = {2013-07-17},
	keywords = {{citedDissProp}}
}

@online{chen_improving_2013,
	title = {Improving Twitter search with real-time human computation},
	url = {https://blog.twitter.com/2013/improving-twitter-search-real-time-human-computation},
	abstract = {One of the magical things about Twitter is that it opens a window to the world in real-time. An event happens, and seconds later, people share it across the planet. Consider, for example, what happ......},
	titleaddon = {Twitter Engineering Blog},
	author = {Chen, Edwin and Jain, Alpa},
	urldate = {2013-12-09},
	date = {2013},
	keywords = {{citedDissProp}},
	file = {Snapshot:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\NWJQ7G5V\improving-twitter-search-real-time-human-computation.html:text/html}
}

@inproceedings{efron_hashtag_2010,
	location = {New York, {NY}, {USA}},
	title = {Hashtag Retrieval in a Microblogging Environment},
	isbn = {978-1-4503-0153-4},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1835449.1835616},
	doi = {10.1145/1835449.1835616},
	series = {{SIGIR} '10},
	abstract = {Microblog services let users broadcast brief textual messages to people who "follow" their activity. Often these posts contain terms called hashtags, markers of a post's meaning, audience, etc. This poster treats the following problem: given a user's stated topical interest, retrieve useful hashtags from microblog posts. Our premise is that a user interested in topic x might like to find hashtags that are often applied to posts about x. This poster proposes a language modeling approach to hashtag retrieval. The main contribution is a novel method of relevance feedback based on hashtags. The approach is tested on a corpus of data harvested from twitter.com.},
	pages = {787–788},
	booktitle = {Proceedings of the 33rd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Efron, Miles},
	urldate = {2013-12-08},
	date = {2010},
	keywords = {{citedDissProp}, hashtag, microblog, relevance feedback, twitter},
	file = {p787-efron.pdf:C:\Users\organisciak\Dropbox\papers\dissertation\p787-efron.pdf:application/pdf}
}

@article{efron_information_2011,
	title = {Information search and retrieval in microblogs},
	volume = {62},
	rights = {© 2011 {ASIS\&T}},
	issn = {1532-2890},
	url = {http://onlinelibrary.wiley.com.proxy2.library.illinois.edu/doi/10.1002/asi.21512/abstract},
	doi = {10.1002/asi.21512},
	abstract = {Modern information retrieval ({IR)} has come to terms with numerous new media in efforts to help people find information in increasingly diverse settings. Among these new media are so-called microblogs. A microblog is a stream of text that is written by an author over time. It comprises many very brief updates that are presented to the microblog's readers in reverse-chronological order. Today, the service called Twitter is the most popular microblogging platform. Although microblogging is increasingly popular, methods for organizing and providing access to microblog data are still new. This review offers an introduction to the problems that face researchers and developers of {IR} systems in microblog settings. After an overview of microblogs and the behavior surrounding them, the review describes established problems in microblog retrieval, such as entity search and sentiment analysis, and modeling abstractions, such as authority and quality. The review also treats user-created metadata that often appear in microblogs. Because the problem of microblog search is so new, the review concludes with a discussion of particularly pressing research issues yet to be studied in the field.},
	pages = {996–1008},
	number = {6},
	journaltitle = {Journal of the American Society for Information Science and Technology},
	author = {Efron, Miles},
	urldate = {2013-12-08},
	date = {2011},
	langid = {english},
	keywords = {{citedDissProp}},
	file = {efron-microblog-jasist.pdf:C:\Users\organisciak\Dropbox\papers\dissertation\efron-microblog-jasist.pdf:application/pdf;Snapshot:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\XX7W7A5Q\full.html:text/html}
}

@article{eickhoff_increasing_2012,
	title = {Increasing cheat robustness of crowdsourcing tasks},
	issn = {1386-4564, 1573-7659},
	url = {http://www.springerlink.com/content/70807017421n1462/},
	doi = {10.1007/s10791-011-9181-9},
	abstract = {Crowdsourcing successfully strives to become a widely used means of collecting large-scale scientific corpora. Many research fields, including Information Retrieval, rely on this novel way of data acquisition. However, it seems to be undermined by a significant share of workers that are primarily interested in producing quick generic answers rather than correct ones in order to optimise their time-efficiency and, in turn, earn more money. Recently, we have seen numerous sophisticated schemes of identifying such workers. Those, however, often require additional resources or introduce artificial limitations to the task. In this work, we take a different approach by investigating means of a priori making crowdsourced tasks more resistant against cheaters.},
	journaltitle = {Information Retrieval},
	author = {Eickhoff, Carsten and Vries, Arjen P.},
	urldate = {2012-02-19},
	date = {2012-02-14},
	keywords = {{citedDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	file = {Eickhoff and Vries - 2012 - Increasing cheat robustness of crowdsourcing tasks.html:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\PM8NP24R\Eickhoff and Vries - 2012 - Increasing cheat robustness of crowdsourcing tasks.html:text/html;eickhoff-cheaters.pdf:C:\Users\organisciak\Dropbox\papers\eickhoff-cheaters.pdf:application/pdf}
}

@article{organisciak_evaluating_2012,
	title = {Evaluating rater quality and rating difficulty in online annotation activities},
	volume = {49},
	rights = {Copyright © 2012 by American Society for Information Science and Technology},
	issn = {1550-8390},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/meet.14504901166/abstract},
	doi = {10.1002/meet.14504901166},
	abstract = {Gathering annotations from non-expert online raters is an attractive method for quickly completing large-scale annotation tasks, but the increased possibility of unreliable annotators and diminished work quality remains a cause for concern. In the context of information retrieval, where human-encoded relevance judgments underlie the evaluation of new systems and methods, the ability to quickly and reliably collect trustworthy annotations allows for quicker development and iteration of {research.In} the context of paid online workers, this study evaluates indicators of non-expert performance along three lines: temporality, experience, and agreement. It is found that user performance is a key indicator for future performance. Additionally, the time spent by raters familiarizing themselves with a new set of tasks is important for rater quality, as is long-term familiarity with a topic being {rated.These} findings may inform large-scale digital collections' use of non-expert raters for performing more purposive and affordable online annotation activities.},
	pages = {1–10},
	number = {1},
	journaltitle = {Proceedings of the American Society for Information Science and Technology},
	author = {Organisciak, Peter and Efron, Miles and Fenlon, Katrina and Senseney, Megan},
	urldate = {2013-11-23},
	date = {2012},
	langid = {english},
	keywords = {{citedDissProp}},
	file = {Snapshot:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\KMHITP8J\abstract;jsessionid=D2B4003B525D768D514C414B7C85E315.html:text/html}
}

@article{law_human_2011,
	title = {Human Computation},
	volume = {5},
	issn = {1939-4608, 1939-4616},
	url = {http://www.morganclaypool.com.proxy2.library.illinois.edu/doi/abs/10.2200/S00371ED1V01Y201107AIM013},
	doi = {10.2200/S00371ED1V01Y201107AIM013},
	pages = {1-121},
	number = {3},
	journaltitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Law, Edith and Ahn, Luis von},
	urldate = {2013-09-18},
	date = {2011-06-30},
	keywords = {{citedDissProp}},
	file = {Morgan & Claypool Publishers - Synthesis Lectures on Artificial Intelligence and Machine Learning - 5(3):1 - Abstract:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\CCKE2ZTG\S00371ED1V01Y201107AIM013.html:text/html;Morgan & Claypool Publishers - Synthesis Lectures on Artificial Intelligence and Machine Learning - 5(3):1 - Abstract:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\BTJCHEXW\S00371ED1V01Y201107AIM013.html:text/html}
}

@article{galton_vox_1907,
	title = {Vox populi},
	volume = {75},
	url = {http://adsabs.harvard.edu/abs/1907Natur..75..450G},
	pages = {450–451},
	journaltitle = {Nature},
	author = {Galton, Francis},
	urldate = {2013-10-22},
	date = {1907},
	keywords = {{citedDissProp}},
	file = {Snapshot:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\AJHAXVIM\1907Natur..75..html:text/html}
}

@inproceedings{springer_for_2008,
	title = {For the common good: The Library of Congress Flickr pilot project},
	url = {http://www.loc.gov/rr/print/flickr_report_final.pdf},
	shorttitle = {For the common good},
	author = {Springer, Michelle and Dulabahn, Beth and Michel, Phil and Natanson, Barbara and Reser, David W. and Ellison, Nicole B. and Zinkham, Helena and Woodward, David},
	urldate = {2013-08-26},
	date = {2008},
	keywords = {{citedDissProp}},
	file = {[PDF] from loc.gov:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\ESPKMZX7\Prints et al. - 2008 - For the common good The Library of Congress Flick.pdf:application/pdf}
}

@article{causer_transcription_2012,
	title = {Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham},
	volume = {27},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/27/2/119},
	doi = {10.1093/llc/fqs004},
	shorttitle = {Transcription maximized; expense minimized?},
	abstract = {This article discusses the crowdsourced manuscript transcription project Transcribe Bentham, and how it will impact upon long-established editorial practices at the Bentham Project, University College London, which is producing the new and authoritative edition of The Collected Works of Jeremy Bentham. We site Transcribe Bentham in the burgeoning field of scholarly crowdsourcing projects, and, by detailing our experiences of running and administering the project, attempt to assess the potential benefits of engaging the public in humanities research. The article examines the conceptualization and development of Transcribe Bentham, and how editorial practices at the Bentham Project may change as a result. We account for the design of the bespoke transcription tool which is at the project's heart, and which allows volunteers to transcribe the material and encode it in {TEI-compliant} {XML.} We attempt to answer five key questions: is crowdsourcing the transcription of complex manuscripts cost-effective? Is crowdsourcing exploitative? Are volunteer-produced transcripts of sufficient quality for editorial use and uploading to a digital repository, and what quality controls are required? Does crowdsourcing ensure sustainability and widen access to this priceless material? And finally, should the success of a project like Transcribe Bentham be measured solely according to cost-effectiveness or the volume of work produced, or do considerations of public engagement and access outweigh such concerns?},
	pages = {119-137},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Causer, Tim and Tonra, Justin and Wallace, Valerie},
	urldate = {2013-08-26},
	date = {2012-06-01},
	langid = {english},
	keywords = {{citedDissProp}},
	file = {Snapshot:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\SI2TC3DD\119.html:text/html}
}

@book{raymond_cathedral_1999,
	title = {The Cathedral and the Bazaar},
	abstract = {I anatomize a successful open-source project, fetchmail, that was run as a deliberate test of the surprising theories about software engineering suggested by the history of Linux. I discuss these theories in terms of two fundamentally different development styles, the ``cathedral'' model of most of the commercial world versus the ``bazaar'' model of the Linux world. I show that these models derive from opposing assumptions about the nature of the software-debugging task. I then make a sustained argument from the Linux experience for the proposition that {``Given} enough eyeballs, all bugs are shallow'', suggest productive analogies with other self-correcting systems of selfish agents, and conclude with some exploration of the implications of this insight for the future of software.},
	pagetotal = {241},
	publisher = {{O'Reilly} Media},
	author = {Raymond, Eric S.},
	date = {1999},
	keywords = {{citedDissProp}}
}

@book{benkler_wealth_2006,
	location = {New Haven},
	title = {Wealth of Networks},
	url = {http://cyber.law.harvard.edu/wealth_of_networks/Download_PDFs_of_the_book},
	publisher = {Yale University Press},
	author = {Benkler, Yochai},
	urldate = {2008-10-20},
	date = {2006},
	keywords = {{citedDissProp}},
	file = {Benkler_Wealth_Of_Networks.pdf:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\EATAADGC\Benkler_Wealth_Of_Networks.pdf:application/pdf}
}

@article{mason_financial_2010,
	title = {Financial incentives and the "performance of crowds"},
	volume = {11},
	issn = {1931-0145},
	url = {http://doi.acm.org/10.1145/1809400.1809422},
	doi = {10.1145/1809400.1809422},
	abstract = {The relationship between financial incentives and performance, long of interest to social scientists, has gained new relevance with the advent of web-based "crowd-sourcing" models of production. Here we investigate the effect of compensation on performance in the context of two experiments, conducted on Amazon's Mechanical Turk ({AMT).} We find that increased financial incentives increase the quantity, but not the quality, of work performed by participants, where the difference appears to be due to an "anchoring" effect: workers who were paid more also perceived the value of their work to be greater, and thus were no more motivated than workers paid less. In contrast with compensation levels, we find the details of the compensation scheme do matter--specifically, a "quota" system results in better work for less pay than an equivalent "piece rate" system. Although counterintuitive, these findings are consistent with previous laboratory studies, and may have real-world analogs as well.},
	pages = {100–108},
	number = {2},
	journaltitle = {{SIGKDD} Explor. Newsl.},
	author = {Mason, Winter and Watts, Duncan J.},
	urldate = {2012-06-13},
	date = {2010-05},
	keywords = {candidates, {citedDissProp}, crowdsourcing, crowd-sourcing, extrinsic motivation, hcir, humans in {IR}, incentives, Intrinsic motivation, mechanical turk, peer production, performance},
	file = {ee-15-p77-mason.pdf:C:\Users\organisciak\Dropbox\school\PhD5-Field Exam\PDFs\hcir-collabIR-and-crowdsourcing\ee-15-p77-mason.pdf:application/pdf}
}

@inproceedings{quinn_human_2011,
	location = {New York, {NY}, {USA}},
	title = {Human computation: a survey and taxonomy of a growing field},
	isbn = {978-1-4503-0228-9},
	url = {http://doi.acm.org/10.1145/1978942.1979148},
	doi = {10.1145/1978942.1979148},
	series = {{CHI} '11},
	shorttitle = {Human computation},
	abstract = {The rapid growth of human computation within research and industry has produced many novel ideas aimed at organizing web users to do great things. However, the growth is not adequately supported by a framework with which to understand each new system in the context of the old. We classify human computation systems to help identify parallels between different systems and reveal "holes" in the existing work as opportunities for new research. Since human computation is often confused with "crowdsourcing" and other terms, we explore the position of human computation with respect to these related topics.},
	pages = {1403–1412},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Quinn, Alexander J. and Bederson, Benjamin B.},
	urldate = {2013-05-24},
	date = {2011},
	keywords = {{citedDissProp}, crowdsourcing, data mining, human computation, literature review, social computing, survey, Taxonomy},
	file = {ACM Full Text PDF:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\IBCWFJFV\Quinn and Bederson - 2011 - Human computation a survey and taxonomy of a grow.pdf:application/pdf}
}

@inproceedings{geiger_managing_2011,
	title = {Managing the crowd: towards a taxonomy of crowdsourcing processes},
	url = {http://schader.bwl.uni-mannheim.de/fileadmin/files/schader/files/publikationen/Geiger_et_al._-_2011_-_Managing_the_Crowd_Towards_a_Taxonomy_of_Crowdsourcing_Processes.pdf},
	shorttitle = {Managing the crowd},
	pages = {1–15},
	booktitle = {Proceedings of the seventeenth Americas conference on information systems, Detroit, Michigan},
	author = {Geiger, David and Seedorf, Stefan and Schulze, Thimo and Nickerson, Robert and Schader, Martin},
	urldate = {2013-05-24},
	date = {2011},
	keywords = {{citedDissProp}},
	file = {[PDF] from uni-mannheim.de:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\FFK7USVE\Geiger et al. - 2011 - Managing the crowd towards a taxonomy of crowdsou.pdf:application/pdf}
}

@online{wales_insist_2006,
	title = {Insist on Sources},
	url = {http://lists.wikimedia.org/pipermail/wikien-l/2006-July/050773.html},
	titleaddon = {{WikiEN-l}},
	author = {Wales, Jimmy},
	date = {2006-07-19},
	keywords = {{citedDissProp}}
}

@inproceedings{ahn_labeling_2004,
	location = {Vienna, Austria},
	title = {Labeling images with a computer game},
	isbn = {1-58113-702-8},
	url = {http://dl.acm.org/citation.cfm?id=985733},
	abstract = {We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.},
	pages = {319-326},
	booktitle = {Proceedings of the {SIGCHI} conference on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Ahn, Luis von and Dabbish, Laura},
	urldate = {2008-11-03},
	date = {2004},
	keywords = {{citedDissProp}, distributed knowledge acquisition, hcir, humans in {IR}, {ICcited}, image labeling, online games, rate5, World Wide Web},
	file = {ACM Snapshot:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\4GEWMUBZ\citation.html:text/html;p319-vonahn.pdf:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\WTSEN8V8\p319-vonahn.pdf:application/pdf}
}

@inproceedings{spiteri_social_2011,
	title = {Social discovery tools: Cataloguing meets user convenience},
	volume = {3},
	url = {http://journals.lib.washington.edu/index.php/nasko/article/view/12790},
	abstract = {This paper examines how library users access, use, and interact with two social discovery systems used in two Canadian public library systems. How do public library users interact with social discovery systems? How does usage between the two social discovery systems compare? Daily transaction logs of the social discovery systems used by the two libraries were compiled from May-August, 2010. Fifty sets of bibliographic records were compared to evaluate user-contributed content. Results indicate that features that allow for user-generated content are underused in both systems. Future research will thus focus on clients' motivations for engaging with the social features of social discovery systems, and their perceptions of, and satisfaction with, the benefits of these features.},
	booktitle = {Proceedings from North American Symposium on Knowledge Organization},
	author = {Spiteri, Louise F.},
	date = {2011},
	keywords = {bibliocommons, {citedDissProp}, {ICcited}, libraries, opac, sorted}
}

@article{kraut_encouraging_2012,
	title = {Encouraging contribution to online communities},
	journaltitle = {Designing From Theory: Using the Social Sciences as the Basis for Building Online Communities},
	author = {Kraut, {R.E.} and Resnick, P.},
	date = {2012},
	keywords = {{citedDissProp}, crowdsourcing}
}

@book{surowiecki_wisdom_2004,
	title = {The Wisdom of Crowds},
	publisher = {Doubleday},
	author = {Surowiecki, James},
	date = {2004},
	keywords = {{citedDissProp}, crowdsourcing, {ICcited}, read, sorted}
}

@thesis{organisciak_why_2010,
	location = {Edmonton, Alberta},
	title = {Why Bother? Examining the motivations of users in large-scale crowd-powered online initiatives},
	url = {http://hdl.handle.net/10048/1370},
	abstract = {This study examines the motivations of participants in networked, large-scale content production and research – a paradigm of distributed work magnified by the Internet. This has come to be called crowdsourcing. The approach taken in examining the crowdsourcing paradigm is of retrospection, with a study focused on observed examples and existing theories. Thirteen cases of existing crowdsourcing sites were selected for study, from a larger sample of 300. These cases were coded by their site properties and analyzed, identifying possible motivational mechanisms. Subsequent interviews with eight medium to heavy Internet users further explored these features, with an emphasis on ranking relative importance of various motivators. This study concludes with a series of recommendations on motivating crowds in such projects, emphasizing among others the importance of topical interest, ease of participation, and appeals to the individuals’ knowledge. In addition to base motivators, a number of support, or secondary, motivators are outlined.},
	pagetotal = {167},
	institution = {University of Alberta},
	type = {Thesis},
	author = {Organisciak, Peter},
	date = {2010-08-31},
	keywords = {{citedDissProp}, {ICcited}, sorted}
}

@article{moyle_manuscript_2010,
	title = {Manuscript transcription by crowdsourcing: Transcribe Bentham},
	volume = {20},
	url = {http://liber.library.uu.nl/publish/issues/2010-3_4/index.html?000514},
	shorttitle = {Manuscript transcription by crowdsourcing},
	abstract = {Transcribe Bentham is testing the feasibility of outsourcing the work of manuscript transcription to members of the public.  {UCL} Library Services holds 60,000 folios of manuscripts of the philosopher and jurist Jeremy Bentham (1748-1832).  Transcribe Bentham will digitise 12,500 Bentham folios, and, through a wiki-based interface, allow volunteer transcribers to take temporary ownership of manuscript images and to create {TEI-encoded} transcription text for final approval by {UCL} experts.  Approved transcripts will be stored and preserved, with the manuscript images, in {UCL's} public Digital Collections repository.  

The project makes innovative use of traditional Library material. It will stimulate public engagement with {UCL's} scholarly archive collections and the challenges of palaeography and manuscript transcription; it will raise the profile of the work and thought of Jeremy Bentham; and it will create new digital resources for future use by  professional researchers.  Towards the end of the project, the transcription tool will be made available to other projects and services.},
	number = {3},
	journaltitle = {{LIBER} Quarterly},
	author = {Moyle, M. and Tonra, J. and Wallace, V.},
	urldate = {2012-02-01},
	date = {2010},
	keywords = {{citedDissProp}, crowdsourcing, digital humanities, {swiftCite}},
	file = {Full Text PDF:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\U44QXI89\Moyle et al. - 2010 - Manuscript transcription by crowdsourcing Transcr.pdf:application/pdf;Snapshot:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\TDIV9BMK\20474.html:text/html}
}

@inproceedings{snow_cheap_2008,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks},
	url = {http://dl.acm.org.proxy2.library.illinois.edu/citation.cfm?id=1613715.1613751},
	series = {{EMNLP} '08},
	shorttitle = {Cheap and fast—but is it good?},
	abstract = {Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.},
	pages = {254–263},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Snow, R. and {O'Connor}, B. and Jurafsky, D. and Ng, {A.Y.}},
	date = {2008},
	keywords = {annotation, {citedDissProp}, expertise, experts, {hcirCITE}, {hcirMidtermCITE}, mechanical turk, {mturkCITE}, tagging, turk},
	file = {Google Scholar Linked Page:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\BQTJ4D2F\login.html:text/html;p254-snow.pdf:C:\Users\organisciak\Dropbox\school\PHD4-Readings\HCIR Literature\p254-snow.pdf:application/pdf}
}

@book{kraut_evidence-based_????,
	location = {Cambridge, {MA}},
	title = {Evidence-based social design: Mining the social sciences to build successful online communities},
	url = {http://kraut.hciresearch.org/content/books},
	publisher = {{MIT} Press},
	author = {Kraut, {R.E.} and Resnick, P.},
	keywords = {{citedDissProp}}
}

@inproceedings{ipeirotis_quality_2010,
	location = {New York, {NY}, {USA}},
	title = {Quality management on Amazon Mechanical Turk},
	isbn = {978-1-4503-0222-7},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1837885.1837906},
	doi = {10.1145/1837885.1837906},
	series = {{HCOMP} '10},
	pages = {64–67},
	booktitle = {Proceedings of the {ACM} {SIGKDD} Workshop on Human Computation},
	publisher = {{ACM}},
	author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
	urldate = {2012-02-19},
	date = {2010},
	keywords = {{citedDissProp}}
}

@inproceedings{raykar_supervised_2009,
	location = {New York, {NY}, {USA}},
	title = {Supervised learning from multiple experts: whom to trust when everyone lies a bit},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1553374.1553488},
	doi = {10.1145/1553374.1553488},
	series = {{ICML} '09},
	shorttitle = {Supervised learning from multiple experts},
	pages = {889–896},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Raykar, Vikas C. and Yu, Shipeng and Zhao, Linda H. and Jerebko, Anna and Florin, Charles and Valadez, Gerardo Hermosillo and Bogoni, Luca and Moy, Linda},
	urldate = {2012-02-19},
	date = {2009},
	keywords = {{citedDissProp}}
}

@inproceedings{wallace_who_2011,
	title = {Who should label what? Instance allocation in multiple expert active learning},
	shorttitle = {Who should label what?},
	booktitle = {Proceedings of the {SIAM} International Conference on Data Mining ({SDM)}},
	author = {Wallace, B. and Small, K. and Brodley, C. and Trikalinos, T.},
	date = {2011},
	keywords = {{citedDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	file = {Google Scholar Linked Page:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\R4E4ACD6\Wallace et al. - 2011 - Who should label what Instance allocation in mult.pdf:application/pdf;wallace.pdf:C:\Users\organisciak\Dropbox\school\PHD4-Readings\HCIR Literature\wallace.pdf:application/pdf}
}

@inproceedings{sheng_get_2008,
	location = {New York, {NY}, {USA}},
	title = {Get another label? improving data quality and data mining using multiple, noisy labelers},
	isbn = {978-1-60558-193-4},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1401890.1401965},
	doi = {10.1145/1401890.1401965},
	series = {{KDD} '08},
	shorttitle = {Get another label?},
	pages = {614–622},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {{ACM}},
	author = {Sheng, Victor S. and Provost, Foster and Ipeirotis, Panagiotis G.},
	urldate = {2012-02-19},
	date = {2008},
	keywords = {{citedDissProp}, data preprocessing, data selection, {hcirCITE}},
	file = {sheng.pdf:C:\Users\organisciak\Dropbox\school\PHD4-Readings\HCIR Literature\sheng.pdf:application/pdf}
}

@inproceedings{welinder_online_2010,
	title = {Online crowdsourcing: Rating annotators and obtaining cost-effective labels},
	isbn = {978-1-4244-7029-7},
	doi = {10.1109/CVPRW.2010.5543189},
	shorttitle = {Online crowdsourcing},
	abstract = {Labeling large datasets has become faster, cheaper, and easier with the advent of crowdsourcing services like Amazon Mechanical Turk. How can one trust the labels obtained from such services? We propose a model of the labeling process which includes label uncertainty, as well a multi-dimensional measure of the annotators' ability. From the model we derive an online algorithm that estimates the most likely value of the labels and the annotator abilities. It finds and prioritizes experts when requesting labels, and actively excludes unreliable annotators. Based on labels already obtained, it dynamically chooses which images will be labeled next, and how many labels to request in order to achieve a desired level of confidence. Our algorithm is general and can handle binary, multi-valued, and continuous annotations (e.g. bounding boxes). Experiments on a dataset containing more than 50,000 labels show that our algorithm reduces the number of labels required, and thus the total cost of labeling, by a large factor while keeping error rates low on a variety of datasets.},
	eventtitle = {2010 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW)}},
	pages = {25-32},
	booktitle = {2010 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW)}},
	publisher = {{IEEE}},
	author = {Welinder, P. and Perona, P.},
	date = {2010-06-13},
	keywords = {Adaptation model, amazon mechanical turk, {citedDissProp}, Computer vision, cost effective label, costing, Costs, dataset labeling service, Error analysis, {hcirCITE}, {hcirMidtermCITE}, image classification, label uncertainty, Labeling, multidimensional annotator ability measurement, multivalued continuous annotation, Noise figure, online crowdsourcing, Outsourcing, rating annotator, Web services},
	file = {welinder.pdf:C:\Users\organisciak\Dropbox\school\PHD4-Readings\HCIR Literature\welinder.pdf:application/pdf}
}

@article{whitehill_whose_2009,
	title = {Whose vote should count more: Optimal integration of labels from labelers of unknown expertise},
	volume = {22},
	shorttitle = {Whose vote should count more},
	pages = {2035–2043},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Whitehill, J. and Ruvolo, P. and Wu, T. and Bergsma, J. and Movellan, J.},
	date = {2009},
	keywords = {{citedDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	file = {Google Scholar Linked Page:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\PV9EWG5K\Whitehill et al. - 2009 - Whose vote should count more Optimal integration .pdf:application/pdf;Whitehill1-OptimalLabeling.pdf:C:\Users\organisciak\Dropbox\school\PHD4-Readings\HCIR Literature\Whitehill1-OptimalLabeling.pdf:application/pdf}
}

@article{von_ahn_games_2006,
	title = {Games with a purpose},
	volume = {39},
	issn = {0018-9162},
	url = {http://scholar.google.ca.login.ezproxy.library.ualberta.ca/scholar?hl=en&lr=&cluster=7220788619130524050},
	abstract = {Through online games, people can collectively solve large-scale computational problems.},
	pages = {96-98},
	number = {6},
	journaltitle = {Computer},
	author = {von Ahn, L.},
	urldate = {2009-04-17},
	date = {2006-06},
	keywords = {{citedDissProp}},
	file = {ieee-gwap.pdf:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\9UGX3936\ieee-gwap.pdf:application/pdf}
}

@inproceedings{novotney_cheap_2010,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Cheap, fast and good enough: Automatic speech recognition with non-expert transcription},
	shorttitle = {Cheap, fast and good enough},
	eventtitle = {{HLT} '10},
	pages = {207-215},
	booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	author = {Novotney, S. and Callison-Burch, C.},
	date = {2010},
	keywords = {annotation, {citedDissProp}, expertise, experts, {hcirCITE}, {hcirMidtermCITE}, mechanical turk, {mturkCITE}, turk},
	file = {Google Scholar Linked Page:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\4P7694FR\login.html:text/html;p207-novotney (cirss-dcc-lap3's conflicted copy 2011-05-06).pdf:C:\Users\organisciak\Dropbox\school\LIS590tx\Lit Review\p207-novotney (cirss-dcc-lap3's conflicted copy 2011-05-06).pdf:application/pdf}
}

@book{howe_crowdsourcing:_2008,
	edition = {1},
	title = {Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business},
	isbn = {0307396207},
	shorttitle = {Crowdsourcing},
	pagetotal = {320},
	publisher = {Crown Business},
	author = {Howe, Jeff},
	date = {2008-08-26},
	keywords = {{citedDissProp}}
}

@article{howe_rise_2006,
	title = {The rise of crowdsourcing},
	volume = {14},
	number = {6},
	journaltitle = {Wired Magazine},
	author = {Howe, J.},
	date = {2006},
	keywords = {{citedDissProp}},
	file = {wired-jun06-theriseofcrowdsourcing.pdf:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\7C4PDU9A\wired-jun06-theriseofcrowdsourcing.pdf:application/pdf}
}

@article{lakhani_how_2003,
	title = {How open source software works: "free" user-to-user assistance},
	volume = {32},
	url = {http://www.sciencedirect.com/science/article/B6V77-479TM54-1/2/5672b73de696a2d8a1d68e6d5747a2cb},
	doi = {10.1016/S0048-7333(02)00095-1},
	shorttitle = {How open source software works},
	abstract = {Research into free and open source software development projects has so far largely focused on how the major tasks of software development are organized and motivated. But a complete project requires the execution of "mundane but necessary" tasks as well. In this paper, we explore how the mundane but necessary task of field support is organized in the case of Apache web server software, and why some project participants are motivated to provide this service gratis to others. We find that the Apache field support system functions effectively. We also find that, when we partition the help system into its component tasks, 98\% of the effort expended by information providers in fact returns direct learning benefits to those providers. This finding considerably reduces the puzzle of why information providers are willing to perform this task "for free." Implications are discussed.},
	pages = {923-943},
	number = {6},
	journaltitle = {Research Policy},
	author = {Lakhani, Karim R. and Hippel, Eric von},
	urldate = {2008-10-20},
	date = {2003-06},
	keywords = {{citedDissProp}, Open source software, User innovation, User support, Virtual community},
	file = {ScienceDirect Snapshot:C:\Users\organisciak\AppData\Roaming\Zotero\Zotero\Profiles\873jkxkn.default\zotero\storage\PPCWABCT\science.html:text/html}
}