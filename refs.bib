
@article{von_ahn_games_2006,
	title = {Games with a purpose},
	volume = {39},
	issn = {0018-9162},
	url = {http://scholar.google.ca.login.ezproxy.library.ualberta.ca/scholar?hl=en&lr=&cluster=7220788619130524050},
	abstract = {Through online games, people can collectively solve large-scale computational problems.},
	number = {6},
	urldate = {2009-04-17},
	journal = {Computer},
	author = {von Ahn, L.},
	month = jun,
	year = {2006},
	pages = {96--98},
	file = {ieee-gwap.pdf:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/9UGX3936/ieee-gwap.pdf:application/pdf}
},

@book{benkler_wealth_2006,
	address = {New Haven},
	title = {Wealth of Networks},
	urldate = {2008-10-20},
	publisher = {Yale University Press},
	author = {Benkler, Yochai},
	year = {2006},
	file = {Benkler_Wealth_Of_Networks.pdf:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/EATAADGC/Benkler_Wealth_Of_Networks.pdf:application/pdf}
},

@book{surowiecki_wisdom_2004,
	title = {The Wisdom of Crowds},
	publisher = {Doubleday},
	author = {Surowiecki, James},
	year = {2004},
	keywords = {crowdsourcing, {ICcited}, read, sorted}
},

@article{lakhani_how_2003,
	title = {How open source software works: "free" user-to-user assistance},
	volume = {32},
	shorttitle = {How open source software works},
	url = {http://www.sciencedirect.com/science/article/B6V77-479TM54-1/2/5672b73de696a2d8a1d68e6d5747a2cb},
	doi = {10.1016/S0048-7333(02)00095-1},
	abstract = {Research into free and open source software development projects has so far largely focused on how the major tasks of software development are organized and motivated. But a complete project requires the execution of "mundane but necessary" tasks as well. In this paper, we explore how the mundane but necessary task of field support is organized in the case of Apache web server software, and why some project participants are motivated to provide this service gratis to others. We find that the Apache field support system functions effectively. We also find that, when we partition the help system into its component tasks, 98\% of the effort expended by information providers in fact returns direct learning benefits to those providers. This finding considerably reduces the puzzle of why information providers are willing to perform this task "for free." Implications are discussed.},
	number = {6},
	urldate = {2008-10-20},
	journal = {Research Policy},
	author = {Lakhani, Karim R. and Hippel, Eric von},
	month = jun,
	year = {2003},
	keywords = {Open source software, User innovation, User support, Virtual community},
	pages = {923--943},
	file = {ScienceDirect Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/PPCWABCT/science.html:text/html}
},

@phdthesis{organisciak_why_2010,
	address = {Edmonton, Alberta},
	type = {Thesis},
	title = {Why Bother? Examining the motivations of users in large-scale crowd-powered online initiatives},
	url = {http://hdl.handle.net/10048/1370},
	abstract = {This study examines the motivations of participants in networked, large-scale content production and research – a paradigm of distributed work magnified by the Internet. This has come to be called crowdsourcing. The approach taken in examining the crowdsourcing paradigm is of retrospection, with a study focused on observed examples and existing theories. Thirteen cases of existing crowdsourcing sites were selected for study, from a larger sample of 300. These cases were coded by their site properties and analyzed, identifying possible motivational mechanisms. Subsequent interviews with eight medium to heavy Internet users further explored these features, with an emphasis on ranking relative importance of various motivators. This study concludes with a series of recommendations on motivating crowds in such projects, emphasizing among others the importance of topical interest, ease of participation, and appeals to the individuals’ knowledge. In addition to base motivators, a number of support, or secondary, motivators are outlined.},
	school = {University of Alberta},
	author = {Organisciak, Peter},
	month = aug,
	year = {2010},
	keywords = {{ICcited}, sorted}
},

@article{moyle_manuscript_2010,
	title = {Manuscript transcription by crowdsourcing: Transcribe Bentham},
	volume = {20},
	shorttitle = {Manuscript transcription by crowdsourcing},
	url = {http://liber.library.uu.nl/publish/issues/2010-3_4/index.html?000514},
	abstract = {Transcribe Bentham is testing the feasibility of outsourcing the work of manuscript transcription to members of the public.  {UCL} Library Services holds 60,000 folios of manuscripts of the philosopher and jurist Jeremy Bentham (1748-1832).  Transcribe Bentham will digitise 12,500 Bentham folios, and, through a wiki-based interface, allow volunteer transcribers to take temporary ownership of manuscript images and to create {TEI-encoded} transcription text for final approval by {UCL} experts.  Approved transcripts will be stored and preserved, with the manuscript images, in {UCL's} public Digital Collections repository.  

The project makes innovative use of traditional Library material. It will stimulate public engagement with {UCL's} scholarly archive collections and the challenges of palaeography and manuscript transcription; it will raise the profile of the work and thought of Jeremy Bentham; and it will create new digital resources for future use by  professional researchers.  Towards the end of the project, the transcription tool will be made available to other projects and services.},
	number = {3-4},
	urldate = {2012-02-01},
	journal = {{LIBER} Quarterly},
	author = {Moyle, M. and Tonra, J. and Wallace, V.},
	year = {2010},
	keywords = {crowdsourcing, digital humanities, {swiftCite}},
	file = {Full Text PDF:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/U44QXI89/Moyle et al. - 2010 - Manuscript transcription by crowdsourcing Transcr.pdf:application/pdf;Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/TDIV9BMK/20474.html:text/html}
},

@misc{_lists_????,
	title = {Lists},
	url = {http://help.bibliocommons.com/en-ca/045faq/060faq_lists},
	urldate = {2011-10-11},
	journal = {Bibliocommons},
	keywords = {{ICcited}, sorted}
},

@inproceedings{spiteri_social_2011,
	title = {Social discovery tools: Cataloguing meets user convenience},
	volume = {3},
	url = {http://journals.lib.washington.edu/index.php/nasko/article/view/12790},
	abstract = {This paper examines how library users access, use, and interact with two social discovery systems used in two Canadian public library systems. How do public library users interact with social discovery systems? How does usage between the two social discovery systems compare? Daily transaction logs of the social discovery systems used by the two libraries were compiled from May-August, 2010. Fifty sets of bibliographic records were compared to evaluate user-contributed content. Results indicate that features that allow for user-generated content are underused in both systems. Future research will thus focus on clients' motivations for engaging with the social features of social discovery systems, and their perceptions of, and satisfaction with, the benefits of these features.},
	booktitle = {Proceedings from North American Symposium on Knowledge Organization},
	author = {Spiteri, Louise F.},
	year = {2011},
	keywords = {bibliocommons, {ICcited}, libraries, opac, sorted}
},

@techreport{holley_many_2009,
	title = {Many Hands Make Light Work: Public Collaborative {OCR} Text Correction in Australian Historic Newspapers},
	url = {http://www-prod.nla.gov.au/openpublish/index.php/nlasp/article/viewArticle/1406},
	abstract = {The {ANDP} team had from the outset in January 2007 decided to make a considerable investment in software development in order to be able to quality assure digital outputs to ensure they met minimum standards and to future proof the files in case they could be improved further in the future.

Contributors had all expressed concern that the digital outputs (image quality, {OCR} text) may not be good enough to enable adequate full text retrieval or to meet user expectations. The {ANDP} team had regularly brainstormed and reviewed ideas to improve the quality of outputs and had implemented a number of ideas to achieve this3. At this stage the team were assuming that quality of data entirely relied on the Library’s digitisation technique. It had not yet occurred to the team or the contributors that the public may play a role in improving and enhancing the quality of the data.},
	institution = {National Library of Australia},
	author = {Holley, Rose},
	year = {2009},
	keywords = {{ICcited}, sorted}
},

@book{kraut_evidence-based_????,
	address = {Cambridge, {MA}},
	title = {Evidence-based social design: Mining the social sciences to build successful online communities},
	url = {http://kraut.hciresearch.org/content/books},
	publisher = {{MIT} Press},
	author = {Kraut, {R.E.} and Resnick, P.}
},

@article{mason_financial_2010,
	title = {Financial incentives and the "performance of crowds"},
	volume = {11},
	issn = {1931-0145},
	url = {http://doi.acm.org/10.1145/1809400.1809422},
	doi = {10.1145/1809400.1809422},
	abstract = {The relationship between financial incentives and performance, long of interest to social scientists, has gained new relevance with the advent of web-based "crowd-sourcing" models of production. Here we investigate the effect of compensation on performance in the context of two experiments, conducted on Amazon's Mechanical Turk ({AMT).} We find that increased financial incentives increase the quantity, but not the quality, of work performed by participants, where the difference appears to be due to an "anchoring" effect: workers who were paid more also perceived the value of their work to be greater, and thus were no more motivated than workers paid less. In contrast with compensation levels, we find the details of the compensation scheme do matter--specifically, a "quota" system results in better work for less pay than an equivalent "piece rate" system. Although counterintuitive, these findings are consistent with previous laboratory studies, and may have real-world analogs as well.},
	number = {2},
	urldate = {2012-06-13},
	journal = {{SIGKDD} Explor. Newsl.},
	author = {Mason, Winter and Watts, Duncan J.},
	month = may,
	year = {2010},
	keywords = {candidates, crowd-sourcing, crowdsourcing, extrinsic motivation, hcir, humans in {IR}, incentives, Intrinsic motivation, mechanical turk, peer production, performance},
	pages = {100–108},
	file = {ee-15-p77-mason.pdf:/Users/dccuser/Dropbox/School/PhD5-Field Exam/PDFs/hcir-collabIR-and-crowdsourcing/ee-15-p77-mason.pdf:application/pdf}
},

@article{law_human_2011,
	title = {Human Computation},
	volume = {5},
	issn = {1939-4608, 1939-4616},
	url = {http://www.morganclaypool.com/doi/abs/10.2200/S00371ED1V01Y201107AIM013?journalCode=aim},
	doi = {10.2200/S00371ED1V01Y201107AIM013},
	number = {3},
	urldate = {2012-11-05},
	journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Law, Edith and Ahn, Luis von},
	month = jun,
	year = {2011},
	pages = {1--121},
	file = {Morgan & Claypool Publishers - Synthesis Lectures on Artificial Intelligence and Machine Learning - 5(3):1 - Abstract:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/CCKE2ZTG/S00371ED1V01Y201107AIM013.html:text/html}
},

@book{raymond_cathedral_1999,
	title = {The Cathedral and the Bazaar},
	abstract = {I anatomize a successful open-source project, fetchmail, that was run as a deliberate test of the surprising theories about software engineering suggested by the history of Linux. I discuss these theories in terms of two fundamentally different development styles, the ``cathedral'' model of most of the commercial world versus the ``bazaar'' model of the Linux world. I show that these models derive from opposing assumptions about the nature of the software-debugging task. I then make a sustained argument from the Linux experience for the proposition that {``Given} enough eyeballs, all bugs are shallow'', suggest productive analogies with other self-correcting systems of selfish agents, and conclude with some exploration of the implications of this insight for the future of software.},
	publisher = {{O'Reilly} Media},
	author = {Raymond, Eric S.},
	year = {1999}
},

@article{galton_vox_1907,
	title = {Vox populi},
	volume = {75},
	url = {http://adsabs.harvard.edu/abs/1907Natur..75..450G},
	urldate = {2013-10-22},
	journal = {Nature},
	author = {Galton, Francis},
	year = {1907},
	pages = {450–451},
	file = {Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/AJHAXVIM/1907Natur..75..html:text/html}
},

@article{causer_transcription_2012,
	title = {Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham},
	volume = {27},
	issn = {0268-1145, 1477-4615},
	shorttitle = {Transcription maximized; expense minimized?},
	url = {http://llc.oxfordjournals.org/content/27/2/119},
	doi = {10.1093/llc/fqs004},
	abstract = {This article discusses the crowdsourced manuscript transcription project Transcribe Bentham, and how it will impact upon long-established editorial practices at the Bentham Project, University College London, which is producing the new and authoritative edition of The Collected Works of Jeremy Bentham. We site Transcribe Bentham in the burgeoning field of scholarly crowdsourcing projects, and, by detailing our experiences of running and administering the project, attempt to assess the potential benefits of engaging the public in humanities research. The article examines the conceptualization and development of Transcribe Bentham, and how editorial practices at the Bentham Project may change as a result. We account for the design of the bespoke transcription tool which is at the project's heart, and which allows volunteers to transcribe the material and encode it in {TEI-compliant} {XML.} We attempt to answer five key questions: is crowdsourcing the transcription of complex manuscripts cost-effective? Is crowdsourcing exploitative? Are volunteer-produced transcripts of sufficient quality for editorial use and uploading to a digital repository, and what quality controls are required? Does crowdsourcing ensure sustainability and widen access to this priceless material? And finally, should the success of a project like Transcribe Bentham be measured solely according to cost-effectiveness or the volume of work produced, or do considerations of public engagement and access outweigh such concerns?},
	language = {en},
	number = {2},
	urldate = {2013-08-26},
	journal = {Literary and Linguistic Computing},
	author = {Causer, Tim and Tonra, Justin and Wallace, Valerie},
	month = jun,
	year = {2012},
	pages = {119--137},
	file = {Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/SI2TC3DD/119.html:text/html}
},

@misc{_what_????,
	title = {What is {reCAPTCHA?}},
	url = {http://www.google.com/recaptcha/learnmore},
	urldate = {2013-08-26},
	journal = {{reCAPTCHA}}
},

@inproceedings{springer_for_2008,
	title = {For the common good: The Library of Congress Flickr pilot project},
	shorttitle = {For the common good},
	url = {http://www.loc.gov/rr/print/flickr_report_final.pdf},
	urldate = {2013-08-26},
	author = {Springer, Michelle and Dulabahn, Beth and Michel, Phil and Natanson, Barbara and Reser, David W. and Ellison, Nicole B. and Zinkham, Helena and Woodward, David},
	year = {2008},
	file = {[PDF] from loc.gov:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/ESPKMZX7/Prints et al. - 2008 - For the common good The Library of Congress Flick.pdf:application/pdf}
}
