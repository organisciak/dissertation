
@inreference{_bias_????,
	title = {bias, adj., n., and adv.},
	url = {http://www.oed.com.proxy2.library.illinois.edu/view/Entry/18564},
	booktitle = {{OED} Online},
	publisher = {Oxford University Press},
	urldate = {2014},
	langid = {british},
	keywords = {{citeDissProp}},
	file = {OED snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/WV62CVIG/bias, adj., n., and adv..html:text/html}
}

@inproceedings{snow_cheap_2008,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks},
	url = {http://dl.acm.org.proxy2.library.illinois.edu/citation.cfm?id=1613715.1613751},
	series = {{EMNLP} '08},
	shorttitle = {Cheap and fast—but is it good?},
	abstract = {Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.},
	pages = {254–263},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Snow, R. and {O'Connor}, B. and Jurafsky, D. and Ng, {A.Y.}},
	date = {2008},
	keywords = {annotation, {citeDissProp}, expertise, experts, {hcirCITE}, {hcirMidtermCITE}, mechanical turk, {mturkCITE}, tagging, turk},
	file = {Google Scholar Linked Page:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/BQTJ4D2F/login.html:text/html;p254-snow.pdf:/Users/dccuser/Dropbox/school/PHD4-Readings/HCIR Literature/p254-snow.pdf:application/pdf}
}

@inproceedings{novotney_cheap_2010,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Cheap, fast and good enough: Automatic speech recognition with non-expert transcription},
	shorttitle = {Cheap, fast and good enough},
	eventtitle = {{HLT} '10},
	pages = {207-215},
	booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	author = {Novotney, S. and Callison-Burch, C.},
	date = {2010},
	keywords = {annotation, {citeDissProp}, expertise, experts, {hcirCITE}, {hcirMidtermCITE}, mechanical turk, {mturkCITE}, turk},
	file = {Google Scholar Linked Page:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/4P7694FR/login.html:text/html;p207-novotney (cirss-dcc-lap3's conflicted copy 2011-05-06).pdf:/Users/dccuser/Dropbox/school/LIS590tx/Lit Review/p207-novotney (cirss-dcc-lap3's conflicted copy 2011-05-06).pdf:application/pdf}
}

@article{alonso_crowdsourcing_2008,
	title = {Crowdsourcing for relevance evaluation},
	volume = {42},
	issn = {0163-5840},
	url = {http://doi.acm.org/10.1145/1480506.1480508},
	doi = {10.1145/1480506.1480508},
	abstract = {Relevance evaluation is an essential part of the development and maintenance of information retrieval systems. Yet traditional evaluation approaches have several limitations; in particular, conducting new editorial evaluations of a search system can be very expensive. We describe a new approach to evaluation called {TERC}, based on the crowdsourcing paradigm, in which many online users, drawn from a large community, each performs a small evaluation task.},
	pages = {9–15},
	number = {2},
	journaltitle = {{SIGIR} Forum},
	author = {Alonso, Omar and Rose, Daniel E. and Stewart, Benjamin},
	urldate = {2012-09-05},
	date = {2008-11},
	keywords = {candidates, {citeDissProp}, crowdsourcing, humans in {IR}, Miles suggestions},
	file = {alonso-crowdsourcing.pdf:/Users/dccuser/Dropbox/school/PhD5-Field Exam/PDFs/hcir-collabIR-and-crowdsourcing/alonso-crowdsourcing.pdf:application/pdf}
}

@inproceedings{schenk_crowdsourcing:_2009,
	title = {Crowdsourcing: What can be Outsourced to the Crowd, and Why?},
	url = {http://raptor1.bizlab.mtsu.edu/s-drive/DMORRELL/Mgmt%204990/Crowdsourcing/Schenk%20and%20Guittard.pdf},
	shorttitle = {Crowdsourcing},
	booktitle = {Workshop on Open Source Innovation, Strasbourg, France},
	author = {Schenk, Eric and Guittard, Claude},
	urldate = {2014},
	date = {2009},
	keywords = {{citeDissProp}},
	file = {[PDF] from mtsu.edu:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/WQ3GB2XB/Schenk and Guittard - 2009 - Crowdsourcing What can be Outsourced to the Crowd.pdf:application/pdf}
}

@book{howe_crowdsourcing:_2008,
	edition = {1},
	title = {Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business},
	isbn = {0307396207},
	shorttitle = {Crowdsourcing},
	pagetotal = {320},
	publisher = {Crown Business},
	author = {Howe, Jeff},
	date = {2008-08-26},
	keywords = {{citeDissProp}}
}

@inproceedings{law_defining_2011,
	location = {Vancouver, {BC}, Canada},
	title = {Defining (Human) Computation},
	url = {http://crowdresearch.org/chi2011-workshop/papers/law.pdf},
	abstract = {Human computation is a term that has been used synonymously with other related concepts, including
"crowdsourcing," "social computing," and "collective intelligence." Defining more precisely what human computation means will help to distinguish its research focus from other subfields, and isolate a set of fundamental research questions to pursue.
In this position paper, we propose a definition of human computation that is grounded on familiar
computer science concepts, such as computation and algorithm.

Based on our proposed definition, we then outline the three main aspects of a human computation system and the key research questions associated with each aspect."},
	eventtitle = {{CHI} 2011 Workshop on Crowdsourcing and Human Computation},
	author = {Law, Edith},
	date = {2011-05-08},
	keywords = {{citeDissProp}}
}

@incollection{alonso_design_2011,
	title = {Design and Implementation of Relevance Assessments Using Crowdsourcing},
	rights = {©2011 Springer Berlin Heidelberg},
	isbn = {978-3-642-20160-8, 978-3-642-20161-5},
	url = {http://link.springer.com.proxy2.library.illinois.edu/chapter/10.1007/978-3-642-20161-5_16},
	series = {Lecture Notes in Computer Science},
	abstract = {In the last years crowdsourcing has emerged as a viable platform for conducting relevance assessments. The main reason behind this trend is that makes possible to conduct experiments extremely fast, with good results and at low cost. However, like in any experiment, there are several details that would make an experiment work or fail. To gather useful results, user interface guidelines, inter-agreement metrics, and justification analysis are important aspects of a successful crowdsourcing experiment. In this work we explore the design and execution of relevance judgments using Amazon Mechanical Turk as crowdsourcing platform, introducing a methodology for crowdsourcing relevance assessments and the results of a series of experiments using {TREC} 8 with a fixed budget. Our findings indicate that workers are as good as {TREC} experts, even providing detailed feedback for certain query-document pairs. We also explore the importance of document design and presentation when performing relevance assessment tasks. Finally, we show our methodology at work with several examples that are interesting in their own.},
	pages = {153-164},
	number = {6611},
	booktitle = {Advances in Information Retrieval},
	publisher = {Springer Berlin Heidelberg},
	author = {Alonso, Omar and Baeza-Yates, Ricardo},
	editor = {Clough, Paul and Foley, Colum and Gurrin, Cathal and Jones, Gareth J. F. and Kraaij, Wessel and Lee, Hyowon and Mudoch, Vanessa},
	urldate = {2014},
	date = {2011},
	keywords = {Artificial Intelligence (incl. Robotics), {citeDissProp}, Data Mining and Knowledge Discovery, Database Management, design, Information Storage and Retrieval, Information Systems Applications ({incl.Internet)}, Multimedia Information Systems},
	file = {Full Text PDF:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/VJBHDGZP/Alonso and Baeza-Yates - 2011 - Design and Implementation of Relevance Assessments.pdf:application/pdf;Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/A6MZIU6B/978-3-642-20161-5_16.html:text/html}
}

@article{organisciak_evaluating_2012,
	title = {Evaluating rater quality and rating difficulty in online annotation activities},
	volume = {49},
	rights = {Copyright © 2012 by American Society for Information Science and Technology},
	issn = {1550-8390},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/meet.14504901166/abstract},
	doi = {10.1002/meet.14504901166},
	abstract = {Gathering annotations from non-expert online raters is an attractive method for quickly completing large-scale annotation tasks, but the increased possibility of unreliable annotators and diminished work quality remains a cause for concern. In the context of information retrieval, where human-encoded relevance judgments underlie the evaluation of new systems and methods, the ability to quickly and reliably collect trustworthy annotations allows for quicker development and iteration of {research.In} the context of paid online workers, this study evaluates indicators of non-expert performance along three lines: temporality, experience, and agreement. It is found that user performance is a key indicator for future performance. Additionally, the time spent by raters familiarizing themselves with a new set of tasks is important for rater quality, as is long-term familiarity with a topic being {rated.These} findings may inform large-scale digital collections' use of non-expert raters for performing more purposive and affordable online annotation activities.},
	pages = {1–10},
	number = {1},
	journaltitle = {Proceedings of the American Society for Information Science and Technology},
	author = {Organisciak, Peter and Efron, Miles and Fenlon, Katrina and Senseney, Megan},
	urldate = {2013-11-23},
	date = {2012},
	langid = {english},
	keywords = {{citeDissProp}},
	file = {Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/KMHITP8J/abstract;jsessionid=D2B4003B525D768D514C414B7C85E315.html:text/html}
}

@book{kraut_evidence-based_????,
	location = {Cambridge, {MA}},
	title = {Evidence-based social design: Mining the social sciences to build successful online communities},
	url = {http://kraut.hciresearch.org/content/books},
	publisher = {{MIT} Press},
	author = {Kraut, {R.E.} and Resnick, P.},
	keywords = {{citeDissProp}}
}

@article{mason_financial_2010,
	title = {Financial incentives and the "performance of crowds"},
	volume = {11},
	issn = {1931-0145},
	url = {http://doi.acm.org/10.1145/1809400.1809422},
	doi = {10.1145/1809400.1809422},
	abstract = {The relationship between financial incentives and performance, long of interest to social scientists, has gained new relevance with the advent of web-based "crowd-sourcing" models of production. Here we investigate the effect of compensation on performance in the context of two experiments, conducted on Amazon's Mechanical Turk ({AMT).} We find that increased financial incentives increase the quantity, but not the quality, of work performed by participants, where the difference appears to be due to an "anchoring" effect: workers who were paid more also perceived the value of their work to be greater, and thus were no more motivated than workers paid less. In contrast with compensation levels, we find the details of the compensation scheme do matter--specifically, a "quota" system results in better work for less pay than an equivalent "piece rate" system. Although counterintuitive, these findings are consistent with previous laboratory studies, and may have real-world analogs as well.},
	pages = {100–108},
	number = {2},
	journaltitle = {{SIGKDD} Explor. Newsl.},
	author = {Mason, Winter and Watts, Duncan J.},
	urldate = {2012-06-13},
	date = {2010-05},
	keywords = {candidates, {citeDissProp}, crowd-sourcing, crowdsourcing, extrinsic motivation, hcir, humans in {IR}, incentives, Intrinsic motivation, mechanical turk, peer production, performance},
	file = {ee-15-p77-mason.pdf:/Users/dccuser/Dropbox/school/PhD5-Field Exam/PDFs/hcir-collabIR-and-crowdsourcing/ee-15-p77-mason.pdf:application/pdf}
}

@inproceedings{springer_for_2008,
	title = {For the common good: The Library of Congress Flickr pilot project},
	url = {http://www.loc.gov/rr/print/flickr_report_final.pdf},
	shorttitle = {For the common good},
	author = {Springer, Michelle and Dulabahn, Beth and Michel, Phil and Natanson, Barbara and Reser, David W. and Ellison, Nicole B. and Zinkham, Helena and Woodward, David},
	urldate = {2013-08-26},
	date = {2008},
	keywords = {{citeDissProp}},
	file = {[PDF] from loc.gov:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/ESPKMZX7/Prints et al. - 2008 - For the common good The Library of Congress Flick.pdf:application/pdf}
}

@article{von_ahn_games_2006,
	title = {Games with a purpose},
	volume = {39},
	issn = {0018-9162},
	url = {http://scholar.google.ca.login.ezproxy.library.ualberta.ca/scholar?hl=en&lr=&cluster=7220788619130524050},
	abstract = {Through online games, people can collectively solve large-scale computational problems.},
	pages = {96-98},
	number = {6},
	journaltitle = {Computer},
	author = {von Ahn, L.},
	urldate = {2009-04-17},
	date = {2006-06},
	keywords = {{citeDissProp}},
	file = {ieee-gwap.pdf:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/9UGX3936/ieee-gwap.pdf:application/pdf}
}

@inproceedings{shiells_generating_2010,
	location = {New York, {NY}, {USA}},
	title = {Generating Document Summaries from User Annotations},
	isbn = {978-1-4503-0372-9},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1871962.1871978},
	doi = {10.1145/1871962.1871978},
	series = {{ESAIR} '10},
	abstract = {In this paper we analyze tweets that share the same link as another form of social annotation. By extracting all tweets that contain the same link over a period of time, we can generate a summary of what the crowd is writing about that particular link.},
	pages = {25–26},
	booktitle = {Proceedings of the Third Workshop on Exploiting Semantic Annotations in Information Retrieval},
	publisher = {{ACM}},
	author = {Shiells, Karen and Alonso, Omar and Lee, Ho John},
	urldate = {2013-12-08},
	date = {2010},
	keywords = {{citeDissProp}, crowdsourcing, social search, summarization, twitter},
	file = {p25-shiells.pdf:/Users/dccuser/Dropbox/papers/dissertation/p25-shiells.pdf:application/pdf}
}

@inproceedings{sheng_get_2008,
	location = {New York, {NY}, {USA}},
	title = {Get another label? improving data quality and data mining using multiple, noisy labelers},
	isbn = {978-1-60558-193-4},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1401890.1401965},
	doi = {10.1145/1401890.1401965},
	series = {{KDD} '08},
	shorttitle = {Get another label?},
	pages = {614–622},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {{ACM}},
	author = {Sheng, Victor S. and Provost, Foster and Ipeirotis, Panagiotis G.},
	urldate = {2012-02-19},
	date = {2008},
	keywords = {{citeDissProp}, data preprocessing, data selection, {hcirCITE}},
	file = {sheng.pdf:/Users/dccuser/Dropbox/school/PHD4-Readings/HCIR Literature/sheng.pdf:application/pdf}
}

@inproceedings{efron_hashtag_2010,
	location = {New York, {NY}, {USA}},
	title = {Hashtag Retrieval in a Microblogging Environment},
	isbn = {978-1-4503-0153-4},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1835449.1835616},
	doi = {10.1145/1835449.1835616},
	series = {{SIGIR} '10},
	abstract = {Microblog services let users broadcast brief textual messages to people who "follow" their activity. Often these posts contain terms called hashtags, markers of a post's meaning, audience, etc. This poster treats the following problem: given a user's stated topical interest, retrieve useful hashtags from microblog posts. Our premise is that a user interested in topic x might like to find hashtags that are often applied to posts about x. This poster proposes a language modeling approach to hashtag retrieval. The main contribution is a novel method of relevance feedback based on hashtags. The approach is tested on a corpus of data harvested from twitter.com.},
	pages = {787–788},
	booktitle = {Proceedings of the 33rd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Efron, Miles},
	urldate = {2013-12-08},
	date = {2010},
	keywords = {{citeDissProp}, hashtag, microblog, relevance feedback, twitter},
	file = {p787-efron.pdf:/Users/dccuser/Dropbox/papers/dissertation/p787-efron.pdf:application/pdf}
}

@article{lakhani_how_2003,
	title = {How open source software works: "free" user-to-user assistance},
	volume = {32},
	url = {http://www.sciencedirect.com/science/article/B6V77-479TM54-1/2/5672b73de696a2d8a1d68e6d5747a2cb},
	doi = {10.1016/S0048-7333(02)00095-1},
	shorttitle = {How open source software works},
	abstract = {Research into free and open source software development projects has so far largely focused on how the major tasks of software development are organized and motivated. But a complete project requires the execution of "mundane but necessary" tasks as well. In this paper, we explore how the mundane but necessary task of field support is organized in the case of Apache web server software, and why some project participants are motivated to provide this service gratis to others. We find that the Apache field support system functions effectively. We also find that, when we partition the help system into its component tasks, 98\% of the effort expended by information providers in fact returns direct learning benefits to those providers. This finding considerably reduces the puzzle of why information providers are willing to perform this task "for free." Implications are discussed.},
	pages = {923-943},
	number = {6},
	journaltitle = {Research Policy},
	author = {Lakhani, Karim R. and Hippel, Eric von},
	urldate = {2008-10-20},
	date = {2003-06},
	keywords = {{citeDissProp}, Open source software, User innovation, User support, Virtual community},
	file = {ScienceDirect Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/PPCWABCT/science.html:text/html}
}

@article{law_human_2011,
	title = {Human Computation},
	volume = {5},
	issn = {1939-4608, 1939-4616},
	url = {http://www.morganclaypool.com.proxy2.library.illinois.edu/doi/abs/10.2200/S00371ED1V01Y201107AIM013},
	doi = {10.2200/S00371ED1V01Y201107AIM013},
	pages = {1-121},
	number = {3},
	journaltitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Law, Edith and Ahn, Luis von},
	urldate = {2013-09-18},
	date = {2011-06-30},
	keywords = {{citeDissProp}},
	file = {Morgan & Claypool Publishers - Synthesis Lectures on Artificial Intelligence and Machine Learning - 5(3):1 - Abstract:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/CCKE2ZTG/S00371ED1V01Y201107AIM013.html:text/html;Morgan & Claypool Publishers - Synthesis Lectures on Artificial Intelligence and Machine Learning - 5(3):1 - Abstract:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/BTJCHEXW/S00371ED1V01Y201107AIM013.html:text/html}
}

@inproceedings{quinn_human_2011,
	location = {New York, {NY}, {USA}},
	title = {Human Computation: A Survey and Taxonomy of a Growing Field},
	isbn = {978-1-4503-0228-9},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1978942.1979148},
	doi = {10.1145/1978942.1979148},
	series = {{CHI} '11},
	shorttitle = {Human Computation},
	abstract = {The rapid growth of human computation within research and industry has produced many novel ideas aimed at organizing web users to do great things. However, the growth is not adequately supported by a framework with which to understand each new system in the context of the old. We classify human computation systems to help identify parallels between different systems and reveal "holes" in the existing work as opportunities for new research. Since human computation is often confused with "crowdsourcing" and other terms, we explore the position of human computation with respect to these related topics.},
	pages = {1403–1412},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Quinn, Alexander J. and Bederson, Benjamin B.},
	urldate = {2014},
	date = {2011},
	keywords = {{citeDissProp}, crowdsourcing, data mining, human computation, literature review, social computing, survey, Taxonomy},
	file = {ACM Full Text PDF:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/4QDUSEH2/Quinn and Bederson - 2011 - Human Computation A Survey and Taxonomy of a Grow.pdf:application/pdf}
}

@online{chen_improving_2013,
	title = {Improving Twitter search with real-time human computation},
	url = {https://blog.twitter.com/2013/improving-twitter-search-real-time-human-computation},
	abstract = {One of the magical things about Twitter is that it opens a window to the world in real-time. An event happens, and seconds later, people share it across the planet. Consider, for example, what happ......},
	titleaddon = {Twitter Engineering Blog},
	author = {Chen, Edwin and Jain, Alpa},
	urldate = {2013-12-09},
	date = {2013},
	keywords = {{citeDissProp}},
	file = {Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/NWJQ7G5V/improving-twitter-search-real-time-human-computation.html:text/html}
}

@inproceedings{organisciak_incidental_2013,
	location = {Lincoln, Nebraska},
	title = {Incidental Crowdsourcing: Crowdsourcing in the Periphery},
	url = {http://dh2013.unl.edu/abstracts/ab-273.html},
	abstract = {As the customs of the Internet grow increasingly collaborative, crowdsourcing offers an appealing frame for looking at the interaction of users with online systems and each other. However, it is a broad term that fails to emphasize the use of crowds in subtler system augmentation.

This paper introduces incidental crowdsourcing ({IC):} an approach to user-provided item description that adopts crowdsourcing as a frame for thinking about augmentative features of system design. {IC} is intended to frame discussion around peripheral and non-critical system design choices.

A provisional definition of incidental crowdsourcing will be defined in this paper, and then refined based on examples seen in practice. {IC} will be examined from both the user and system ends, positioned within existing work, and considered in the context of its benefits and drawbacks. This approach allows us to explore the robustness and feasibility of {IC}, looking at the implications inherent to accepting the provisional definition.

The consequences of considering system design on a scale between {IC} and non-{IC} design choices remain to be seen. Toward this goal, the second part of this paper shows a study comparing the participation habits of users in two online systems — one that is representative of {IC} properties and one that is not. This study finds differences in user engagement between the two systems.},
	eventtitle = {Digital Humanities 2013},
	author = {Organisciak, Peter},
	date = {2013-07-17},
	keywords = {{citeDissProp}}
}

@article{eickhoff_increasing_2012,
	title = {Increasing cheat robustness of crowdsourcing tasks},
	issn = {1386-4564, 1573-7659},
	url = {http://www.springerlink.com/content/70807017421n1462/},
	doi = {10.1007/s10791-011-9181-9},
	abstract = {Crowdsourcing successfully strives to become a widely used means of collecting large-scale scientific corpora. Many research fields, including Information Retrieval, rely on this novel way of data acquisition. However, it seems to be undermined by a significant share of workers that are primarily interested in producing quick generic answers rather than correct ones in order to optimise their time-efficiency and, in turn, earn more money. Recently, we have seen numerous sophisticated schemes of identifying such workers. Those, however, often require additional resources or introduce artificial limitations to the task. In this work, we take a different approach by investigating means of a priori making crowdsourced tasks more resistant against cheaters.},
	journaltitle = {Information Retrieval},
	author = {Eickhoff, Carsten and Vries, Arjen P.},
	urldate = {2012-02-19},
	date = {2012-02-14},
	keywords = {{citeDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	file = {Eickhoff and Vries - 2012 - Increasing cheat robustness of crowdsourcing tasks.html:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/PM8NP24R/Eickhoff and Vries - 2012 - Increasing cheat robustness of crowdsourcing tasks.html:text/html;eickhoff-cheaters.pdf:/Users/dccuser/Dropbox/papers/eickhoff-cheaters.pdf:application/pdf}
}

@article{efron_information_2011,
	title = {Information search and retrieval in microblogs},
	volume = {62},
	rights = {© 2011 {ASIS\&T}},
	issn = {1532-2890},
	url = {http://onlinelibrary.wiley.com.proxy2.library.illinois.edu/doi/10.1002/asi.21512/abstract},
	doi = {10.1002/asi.21512},
	abstract = {Modern information retrieval ({IR)} has come to terms with numerous new media in efforts to help people find information in increasingly diverse settings. Among these new media are so-called microblogs. A microblog is a stream of text that is written by an author over time. It comprises many very brief updates that are presented to the microblog's readers in reverse-chronological order. Today, the service called Twitter is the most popular microblogging platform. Although microblogging is increasingly popular, methods for organizing and providing access to microblog data are still new. This review offers an introduction to the problems that face researchers and developers of {IR} systems in microblog settings. After an overview of microblogs and the behavior surrounding them, the review describes established problems in microblog retrieval, such as entity search and sentiment analysis, and modeling abstractions, such as authority and quality. The review also treats user-created metadata that often appear in microblogs. Because the problem of microblog search is so new, the review concludes with a discussion of particularly pressing research issues yet to be studied in the field.},
	pages = {996–1008},
	number = {6},
	journaltitle = {Journal of the American Society for Information Science and Technology},
	author = {Efron, Miles},
	urldate = {2013-12-08},
	date = {2011},
	langid = {english},
	keywords = {{citeDissProp}},
	file = {efron-microblog-jasist.pdf:/Users/dccuser/Dropbox/papers/dissertation/efron-microblog-jasist.pdf:application/pdf;Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/XX7W7A5Q/full.html:text/html}
}

@online{wales_insist_2006,
	title = {Insist on Sources},
	url = {http://lists.wikimedia.org/pipermail/wikien-l/2006-July/050773.html},
	titleaddon = {{WikiEN-l}},
	author = {Wales, Jimmy},
	date = {2006-07-19},
	keywords = {{citeDissProp}}
}

@inproceedings{ahn_labeling_2004,
	location = {Vienna, Austria},
	title = {Labeling images with a computer game},
	isbn = {1-58113-702-8},
	url = {http://dl.acm.org/citation.cfm?id=985733},
	abstract = {We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.},
	pages = {319-326},
	booktitle = {Proceedings of the {SIGCHI} conference on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Ahn, Luis von and Dabbish, Laura},
	urldate = {2008-11-03},
	date = {2004},
	keywords = {{citeDissProp}, distributed knowledge acquisition, hcir, humans in {IR}, {ICcited}, image labeling, online games, rate5, World Wide Web},
	file = {ACM Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/4GEWMUBZ/citation.html:text/html;p319-vonahn.pdf:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/WTSEN8V8/p319-vonahn.pdf:application/pdf}
}

@article{hofmann_latent_2004,
	title = {Latent semantic models for collaborative filtering},
	volume = {22},
	issn = {1046-8188},
	url = {http://doi.acm.org/10.1145/963770.963774},
	doi = {10.1145/963770.963774},
	abstract = {Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.},
	pages = {89–115},
	number = {1},
	journaltitle = {{ACM} Trans. Inf. Syst.},
	author = {Hofmann, Thomas},
	urldate = {2013-09-18},
	date = {2004},
	keywords = {{citeDissProp}}
}

@online{_lists_????,
	title = {Lists},
	url = {http://help.bibliocommons.com/en-ca/045faq/060faq_lists},
	titleaddon = {Bibliocommons},
	urldate = {2011-10-11},
	keywords = {{citeDissProp}, {ICcited}, sorted}
}

@inproceedings{geiger_managing_2011,
	title = {Managing the Crowd: Towards a Taxonomy of Crowdsourcing Processes.},
	url = {http://schader.bwl.uni-mannheim.de/fileadmin/files/schader/files/publikationen/Geiger_et_al._-_2011_-_Managing_the_Crowd_Towards_a_Taxonomy_of_Crowdsourcing_Processes.pdf},
	shorttitle = {Managing the Crowd},
	booktitle = {{AMCIS}},
	author = {Geiger, David and Seedorf, Stefan and Schulze, Thimo and Nickerson, Robert C. and Schader, Martin},
	urldate = {2014},
	date = {2011},
	keywords = {{citeDissProp}},
	file = {[PDF] from uni-mannheim.de:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/KE7N3EAR/Geiger et al. - 2011 - Managing the Crowd Towards a Taxonomy of Crowdsou.pdf:application/pdf}
}

@article{moyle_manuscript_2010,
	title = {Manuscript transcription by crowdsourcing: Transcribe Bentham},
	volume = {20},
	url = {http://liber.library.uu.nl/publish/issues/2010-3_4/index.html?000514},
	shorttitle = {Manuscript transcription by crowdsourcing},
	abstract = {Transcribe Bentham is testing the feasibility of outsourcing the work of manuscript transcription to members of the public.  {UCL} Library Services holds 60,000 folios of manuscripts of the philosopher and jurist Jeremy Bentham (1748-1832).  Transcribe Bentham will digitise 12,500 Bentham folios, and, through a wiki-based interface, allow volunteer transcribers to take temporary ownership of manuscript images and to create {TEI-encoded} transcription text for final approval by {UCL} experts.  Approved transcripts will be stored and preserved, with the manuscript images, in {UCL's} public Digital Collections repository.  

The project makes innovative use of traditional Library material. It will stimulate public engagement with {UCL's} scholarly archive collections and the challenges of palaeography and manuscript transcription; it will raise the profile of the work and thought of Jeremy Bentham; and it will create new digital resources for future use by  professional researchers.  Towards the end of the project, the transcription tool will be made available to other projects and services.},
	number = {3},
	journaltitle = {{LIBER} Quarterly},
	author = {Moyle, M. and Tonra, J. and Wallace, V.},
	urldate = {2012-02-01},
	date = {2010},
	keywords = {{citeDissProp}, crowdsourcing, digital humanities, {swiftCite}},
	file = {Full Text PDF:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/U44QXI89/Moyle et al. - 2010 - Manuscript transcription by crowdsourcing Transcr.pdf:application/pdf;Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/TDIV9BMK/20474.html:text/html}
}

@report{holley_many_2009,
	title = {Many Hands Make Light Work: Public Collaborative {OCR} Text Correction in Australian Historic Newspapers},
	url = {http://www-prod.nla.gov.au/openpublish/index.php/nlasp/article/viewArticle/1406},
	abstract = {The {ANDP} team had from the outset in January 2007 decided to make a considerable investment in software development in order to be able to quality assure digital outputs to ensure they met minimum standards and to future proof the files in case they could be improved further in the future.

Contributors had all expressed concern that the digital outputs (image quality, {OCR} text) may not be good enough to enable adequate full text retrieval or to meet user expectations. The {ANDP} team had regularly brainstormed and reviewed ideas to improve the quality of outputs and had implemented a number of ideas to achieve this3. At this stage the team were assuming that quality of data entirely relied on the Library’s digitisation technique. It had not yet occurred to the team or the contributors that the public may play a role in improving and enhancing the quality of the data.},
	institution = {National Library of Australia},
	author = {Holley, Rose},
	date = {2009},
	keywords = {{citeDissProp}, {ICcited}, sorted}
}

@inproceedings{welinder_online_2010,
	title = {Online crowdsourcing: Rating annotators and obtaining cost-effective labels},
	isbn = {978-1-4244-7029-7},
	doi = {10.1109/CVPRW.2010.5543189},
	shorttitle = {Online crowdsourcing},
	abstract = {Labeling large datasets has become faster, cheaper, and easier with the advent of crowdsourcing services like Amazon Mechanical Turk. How can one trust the labels obtained from such services? We propose a model of the labeling process which includes label uncertainty, as well a multi-dimensional measure of the annotators' ability. From the model we derive an online algorithm that estimates the most likely value of the labels and the annotator abilities. It finds and prioritizes experts when requesting labels, and actively excludes unreliable annotators. Based on labels already obtained, it dynamically chooses which images will be labeled next, and how many labels to request in order to achieve a desired level of confidence. Our algorithm is general and can handle binary, multi-valued, and continuous annotations (e.g. bounding boxes). Experiments on a dataset containing more than 50,000 labels show that our algorithm reduces the number of labels required, and thus the total cost of labeling, by a large factor while keeping error rates low on a variety of datasets.},
	eventtitle = {2010 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW)}},
	pages = {25-32},
	booktitle = {2010 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW)}},
	publisher = {{IEEE}},
	author = {Welinder, P. and Perona, P.},
	date = {2010-06-13},
	keywords = {Adaptation model, amazon mechanical turk, {citeDissProp}, Computer vision, cost effective label, costing, Costs, dataset labeling service, Error analysis, {hcirCITE}, {hcirMidtermCITE}, image classification, label uncertainty, Labeling, multidimensional annotator ability measurement, multivalued continuous annotation, Noise figure, online crowdsourcing, Outsourcing, rating annotator, Web services},
	file = {welinder.pdf:/Users/dccuser/Dropbox/school/PHD4-Readings/HCIR Literature/welinder.pdf:application/pdf}
}

@inproceedings{organisciak_personalized_2013,
	location = {Palm Spring, {CA}},
	title = {Personalized Human Computation},
	eventtitle = {{HCOMP} 2013},
	author = {Organisciak, Peter and Teevan, Jaime and Dumais, Susan and Miller, Robert C. and Kalai, Adam Tauman},
	date = {2013},
	keywords = {{citeDissProp}}
}

@inproceedings{ipeirotis_quality_2010,
	location = {New York, {NY}, {USA}},
	title = {Quality management on Amazon Mechanical Turk},
	isbn = {978-1-4503-0222-7},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1837885.1837906},
	doi = {10.1145/1837885.1837906},
	series = {{HCOMP} '10},
	pages = {64–67},
	booktitle = {Proceedings of the {ACM} {SIGKDD} Workshop on Human Computation},
	publisher = {{ACM}},
	author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
	urldate = {2012-02-19},
	date = {2010},
	keywords = {{citeDissProp}}
}

@inproceedings{eickhoff_quality_2012,
	location = {New York, {NY}, {USA}},
	title = {Quality Through Flow and Immersion: Gamifying Crowdsourced Relevance Assessments},
	isbn = {978-1-4503-1472-5},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/2348283.2348400},
	doi = {10.1145/2348283.2348400},
	series = {{SIGIR} '12},
	shorttitle = {Quality Through Flow and Immersion},
	abstract = {Crowdsourcing is a market of steadily-growing importance upon which both academia and industry increasingly rely. However, this market appears to be inherently infested with a significant share of malicious workers who try to maximise their profits through cheating or sloppiness. This serves to undermine the very merits crowdsourcing has come to represent. Based on previous experience as well as psychological insights, we propose the use of a game in order to attract and retain a larger share of reliable workers to frequently-requested crowdsourcing tasks such as relevance assessments and clustering. In a large-scale comparative study conducted using recent {TREC} data, we investigate the performance of traditional {HIT} designs and a game-based alternative that is able to achieve high quality at significantly lower pay rates, facing fewer malicious submissions.},
	pages = {871–880},
	booktitle = {Proceedings of the 35th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Eickhoff, Carsten and Harris, Christopher G. and de Vries, Arjen P. and Srinivasan, Padmini},
	urldate = {2014},
	date = {2012},
	keywords = {{citeDissProp}, clustering, crowdsourcing, gamification, relevance assessments, serious games},
	file = {ACM Full Text PDF:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/IUDVJ9QI/Eickhoff et al. - 2012 - Quality Through Flow and Immersion Gamifying Crow.pdf:application/pdf}
}

@inproceedings{spiteri_social_2011,
	title = {Social discovery tools: Cataloguing meets user convenience},
	volume = {3},
	url = {http://journals.lib.washington.edu/index.php/nasko/article/view/12790},
	abstract = {This paper examines how library users access, use, and interact with two social discovery systems used in two Canadian public library systems. How do public library users interact with social discovery systems? How does usage between the two social discovery systems compare? Daily transaction logs of the social discovery systems used by the two libraries were compiled from May-August, 2010. Fifty sets of bibliographic records were compared to evaluate user-contributed content. Results indicate that features that allow for user-generated content are underused in both systems. Future research will thus focus on clients' motivations for engaging with the social features of social discovery systems, and their perceptions of, and satisfaction with, the benefits of these features.},
	booktitle = {Proceedings from North American Symposium on Knowledge Organization},
	author = {Spiteri, Louise F.},
	date = {2011},
	keywords = {bibliocommons, {citeDissProp}, {ICcited}, libraries, opac, sorted}
}

@inproceedings{raykar_supervised_2009,
	location = {New York, {NY}, {USA}},
	title = {Supervised learning from multiple experts: whom to trust when everyone lies a bit},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1553374.1553488},
	doi = {10.1145/1553374.1553488},
	series = {{ICML} '09},
	shorttitle = {Supervised learning from multiple experts},
	pages = {889–896},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Raykar, Vikas C. and Yu, Shipeng and Zhao, Linda H. and Jerebko, Anna and Florin, Charles and Valadez, Gerardo Hermosillo and Bogoni, Luca and Moy, Linda},
	urldate = {2012-02-19},
	date = {2009},
	keywords = {{citeDissProp}}
}

@article{bell_bellkor_2008,
	title = {The {BellKor} 2008 Solution to the Netflix Prize},
	url = {ftp://140.118.199.9:2100/public9/2010_ALL/2010_TRU/2010Fall_Matlab-LABfiles/00_VECTOR-at-MATH/%E5%90%91%E9%87%8F%E7%9A%84%E5%88%86%E8%A7%A3/Public9/zTEMP/ZTemp2011/zMSIC-NSC101-DM/40--Dec27/Netflix-Articles-AWARD/(netflix)-ProgressPrize2008_BellKor.pdf},
	journaltitle = {Statistics Research Department at {AT\&T} Research},
	author = {Bell, Robert M. and Koren, Yehuda and Volinsky, Chris},
	urldate = {2014},
	date = {2008},
	keywords = {{citeDissProp}}
}

@book{raymond_cathedral_1999,
	title = {The Cathedral and the Bazaar},
	abstract = {I anatomize a successful open-source project, fetchmail, that was run as a deliberate test of the surprising theories about software engineering suggested by the history of Linux. I discuss these theories in terms of two fundamentally different development styles, the ``cathedral'' model of most of the commercial world versus the ``bazaar'' model of the Linux world. I show that these models derive from opposing assumptions about the nature of the software-debugging task. I then make a sustained argument from the Linux experience for the proposition that {``Given} enough eyeballs, all bugs are shallow'', suggest productive analogies with other self-correcting systems of selfish agents, and conclude with some exploration of the implications of this insight for the future of software.},
	pagetotal = {241},
	publisher = {{O'Reilly} Media},
	author = {Raymond, Eric S.},
	date = {1999},
	keywords = {{citeDissProp}}
}

@book{neuendorf_content_2002,
	location = {Thousand Oaks, {CA}, {USA}},
	title = {The Content Analysis Guidebook},
	pagetotal = {301},
	publisher = {Sage Publications},
	author = {Neuendorf, Kimberly A.},
	date = {2002},
	keywords = {{citeDissProp}}
}

@article{howe_rise_2006,
	title = {The rise of crowdsourcing},
	volume = {14},
	number = {6},
	journaltitle = {Wired Magazine},
	author = {Howe, J.},
	date = {2006},
	keywords = {{citeDissProp}},
	file = {wired-jun06-theriseofcrowdsourcing.pdf:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/7C4PDU9A/wired-jun06-theriseofcrowdsourcing.pdf:application/pdf}
}

@book{surowiecki_wisdom_2004,
	title = {The Wisdom of Crowds},
	publisher = {Doubleday},
	author = {Surowiecki, James},
	date = {2004},
	keywords = {{citeDissProp}, crowdsourcing, {ICcited}, read, sorted}
}

@article{causer_transcription_2012,
	title = {Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham},
	volume = {27},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/27/2/119},
	doi = {10.1093/llc/fqs004},
	shorttitle = {Transcription maximized; expense minimized?},
	abstract = {This article discusses the crowdsourced manuscript transcription project Transcribe Bentham, and how it will impact upon long-established editorial practices at the Bentham Project, University College London, which is producing the new and authoritative edition of The Collected Works of Jeremy Bentham. We site Transcribe Bentham in the burgeoning field of scholarly crowdsourcing projects, and, by detailing our experiences of running and administering the project, attempt to assess the potential benefits of engaging the public in humanities research. The article examines the conceptualization and development of Transcribe Bentham, and how editorial practices at the Bentham Project may change as a result. We account for the design of the bespoke transcription tool which is at the project's heart, and which allows volunteers to transcribe the material and encode it in {TEI-compliant} {XML.} We attempt to answer five key questions: is crowdsourcing the transcription of complex manuscripts cost-effective? Is crowdsourcing exploitative? Are volunteer-produced transcripts of sufficient quality for editorial use and uploading to a digital repository, and what quality controls are required? Does crowdsourcing ensure sustainability and widen access to this priceless material? And finally, should the success of a project like Transcribe Bentham be measured solely according to cost-effectiveness or the volume of work produced, or do considerations of public engagement and access outweigh such concerns?},
	pages = {119-137},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Causer, Tim and Tonra, Justin and Wallace, Valerie},
	urldate = {2013-08-26},
	date = {2012-06-01},
	langid = {english},
	keywords = {{citeDissProp}},
	file = {Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/SI2TC3DD/119.html:text/html}
}

@article{galton_vox_1907,
	title = {Vox populi},
	volume = {75},
	url = {http://adsabs.harvard.edu/abs/1907Natur..75..450G},
	pages = {450–451},
	journaltitle = {Nature},
	author = {Galton, Francis},
	urldate = {2013-10-22},
	date = {1907},
	keywords = {{citeDissProp}},
	file = {Snapshot:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/AJHAXVIM/1907Natur..75..html:text/html}
}

@book{benkler_wealth_2006,
	location = {New Haven},
	title = {Wealth of Networks},
	url = {http://cyber.law.harvard.edu/wealth_of_networks/Download_PDFs_of_the_book},
	publisher = {Yale University Press},
	author = {Benkler, Yochai},
	urldate = {2008-10-20},
	date = {2006},
	keywords = {{citeDissProp}},
	file = {Benkler_Wealth_Of_Networks.pdf:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/EATAADGC/Benkler_Wealth_Of_Networks.pdf:application/pdf}
}

@online{_what_????,
	title = {What is {reCAPTCHA?}},
	url = {http://recaptcha.net/learnmore.html},
	titleaddon = {Recaptcha},
	urldate = {2008-09-27},
	keywords = {{citeDissProp}},
	file = {What is reCAPTCHA?:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/M3CIH96B/learnmore.html:text/html}
}

@inproceedings{wallace_who_2011,
	title = {Who should label what? Instance allocation in multiple expert active learning},
	shorttitle = {Who should label what?},
	booktitle = {Proceedings of the {SIAM} International Conference on Data Mining ({SDM)}},
	author = {Wallace, B. and Small, K. and Brodley, C. and Trikalinos, T.},
	date = {2011},
	keywords = {{citeDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	file = {Google Scholar Linked Page:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/R4E4ACD6/Wallace et al. - 2011 - Who should label what Instance allocation in mult.pdf:application/pdf;wallace.pdf:/Users/dccuser/Dropbox/school/PHD4-Readings/HCIR Literature/wallace.pdf:application/pdf}
}

@article{whitehill_whose_2009,
	title = {Whose vote should count more: Optimal integration of labels from labelers of unknown expertise},
	volume = {22},
	shorttitle = {Whose vote should count more},
	pages = {2035–2043},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Whitehill, J. and Ruvolo, P. and Wu, T. and Bergsma, J. and Movellan, J.},
	date = {2009},
	keywords = {{citeDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	file = {Google Scholar Linked Page:/Users/dccuser/Library/Application Support/Zotero/Profiles/5ej8homi.default/zotero/storage/PV9EWG5K/Whitehill et al. - 2009 - Whose vote should count more Optimal integration .pdf:application/pdf;Whitehill1-OptimalLabeling.pdf:/Users/dccuser/Dropbox/school/PHD4-Readings/HCIR Literature/Whitehill1-OptimalLabeling.pdf:application/pdf}
}

@thesis{organisciak_why_2010,
	location = {Edmonton, Alberta},
	title = {Why Bother? Examining the motivations of users in large-scale crowd-powered online initiatives},
	url = {http://hdl.handle.net/10048/1370},
	abstract = {This study examines the motivations of participants in networked, large-scale content production and research – a paradigm of distributed work magnified by the Internet. This has come to be called crowdsourcing. The approach taken in examining the crowdsourcing paradigm is of retrospection, with a study focused on observed examples and existing theories. Thirteen cases of existing crowdsourcing sites were selected for study, from a larger sample of 300. These cases were coded by their site properties and analyzed, identifying possible motivational mechanisms. Subsequent interviews with eight medium to heavy Internet users further explored these features, with an emphasis on ranking relative importance of various motivators. This study concludes with a series of recommendations on motivating crowds in such projects, emphasizing among others the importance of topical interest, ease of participation, and appeals to the individuals’ knowledge. In addition to base motivators, a number of support, or secondary, motivators are outlined.},
	pagetotal = {167},
	institution = {University of Alberta},
	type = {Thesis},
	author = {Organisciak, Peter},
	date = {2010-08-31},
	keywords = {{citeDissProp}, {ICcited}, sorted}
}