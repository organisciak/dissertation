
@inbook{lavrakas_intercoder_2008,
	location = {Thousand Oaks, {CA}, {USA}},
	title = {Intercoder Reliability},
	isbn = {978-1-4129-1808-4 978-1-4129-6394-7},
	url = {http://knowledge.sagepub.com/view/survey/n228.xml},
	booktitle = {Encyclopedia of Survey Research Methods},
	publisher = {{SAGE} Publications, Inc.},
	author = {Cho, Young Ik},
	bookauthor = {Lavrakas, Paul},
	urldate = {2015-08-03},
	date = {2008},
	keywords = {{citeDiss}}
}

@article{kittur_crowdsourcing_2008,
	title = {Crowdsourcing for usability: Using micro-task markets for rapid, remote, and low-cost user measurements},
	url = {http://www.ipv6.parc.com/content/attachments/crowdsourcing_user_studies_6280_parc.pdf},
	shorttitle = {Crowdsourcing for usability},
	journaltitle = {Proc. {CHI} 2008},
	author = {Kittur, Aniket and Chi, E. and Suh, Bongwon},
	urldate = {2015-08-02},
	date = {2008},
	keywords = {{citeDiss}}
}

@inproceedings{ambati_towards_2011,
	title = {Towards Task Recommendation in Micro-Task Markets.},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.210.5588&rep=rep1&type=pdf},
	pages = {1--4},
	booktitle = {Human computation},
	publisher = {Citeseer},
	author = {Ambati, Vamshi and Vogel, Stephan and Carbonell, Jaime G.},
	urldate = {2015-08-02},
	date = {2011},
	keywords = {{citeDiss}},
	file = {Ambati et al_2011_Towards Task Recommendation in Micro-Task Markets.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\Citeseer2011\\Ambati et al_2011_Towards Task Recommendation in Micro-Task Markets.pdf:application/pdf}
}

@misc{_guidelines_2014,
	title = {Guidelines for Academic Requesters},
	url = {http://wiki.wearedynamo.org/index.php/Guidelines_for_Academic_Requesters},
	publisher = {Dynamo Wiki},
	urldate = {2015-07-23},
	date = {2014},
	keywords = {{citeDiss}}
}

@inproceedings{sen_quest_2007,
	location = {New York, {NY}, {USA}},
	title = {The Quest for Quality Tags},
	isbn = {978-1-59593-845-9},
	url = {http://doi.acm.org/10.1145/1316624.1316678},
	doi = {10.1145/1316624.1316678},
	series = {{GROUP} '07},
	abstract = {Many online communities use tags - community selected words or phrases - to help people find what they desire. The quality of tags varies widely, from tags that capture akey dimension of an entity to those that are profane, useless, or unintelligible. Tagging systems must often select a subset of available tags to display to users due to limited screen space. Because users often spread tags they have seen, selecting good tags not only improves an individual's view of tags, it also encourages them to create better tags in the future. We explore implicit (behavioral) and explicit (rating) mechanisms for determining tag quality. Based on 102,056 tag ratings and survey responses collected from 1,039 users over 100 days, we offer simple suggestions to designers of online communities to improve the quality of tags seen by their users.},
	pages = {361--370},
	booktitle = {Proceedings of the 2007 International {ACM} Conference on Supporting Group Work},
	publisher = {{ACM}},
	author = {Sen, Shilad and Harper, F. Maxwell and {LaPitz}, Adam and Riedl, John},
	urldate = {2015-06-07},
	date = {2007},
	keywords = {{citeDiss}, moderation, tagging, user interfaces},
	file = {Sen et al_2007_The Quest for Quality Tags.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\ACM2007\\Sen et al_2007_The Quest for Quality Tags.pdf:application/pdf}
}

@inproceedings{mitra_comparing_2015,
	title = {Comparing Person-and Process-centric Strategies for Obtaining Quality Data on Amazon Mechanical Turk},
	url = {http://comp.social.gatech.edu/papers/chi15.crowd.mitra.pdf},
	pages = {1345--1354},
	booktitle = {Proceedings of the 33rd Annual {ACM} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Mitra, Tanushree and Hutto, C. J. and Gilbert, Eric},
	urldate = {2015-05-31},
	date = {2015},
	keywords = {{citeDiss}},
	file = {Mitra et al_2015_Comparing Person-and Process-centric Strategies for Obtaining Quality Data on.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\ACM2015\\Mitra et al_2015_Comparing Person-and Process-centric Strategies for Obtaining Quality Data on.pdf:application/pdf}
}

@article{hofmann_latent_2004,
	title = {Latent Semantic Models for Collaborative Filtering},
	volume = {22},
	issn = {1046-8188},
	url = {http://doi.acm.org/10.1145/963770.963774},
	doi = {10.1145/963770.963774},
	abstract = {Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.},
	pages = {89--115},
	number = {1},
	journaltitle = {{ACM}  Transactions on Information Systems},
	author = {Hofmann, Thomas},
	urldate = {2015-05-07},
	date = {2004-01},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Hofmann - 2004 - Latent Semantic Models for Collaborative Filtering.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\Hofmann - 2004 - Latent Semantic Models for Collaborative Filtering.pdf:application/pdf}
}

@inproceedings{organisciak_improving_2015,
	location = {Knoxville, {TN}},
	title = {Improving Consistency of Crowdsourced Multimedia Similarity for Evaluation},
	series = {{JCDL} '15},
	eventtitle = {Joint Conference on Digital Libraries 2015},
	publisher = {{ACM}},
	author = {Organisciak, Peter and Downie, J. Stephen},
	date = {2015-06},
	keywords = {{citeDiss}}
}

@inproceedings{lee_survey_2004,
	title = {Survey Of Music Information Needs, Uses, And Seeking Behaviours: Preliminary Findings.},
	volume = {2004},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.8649&rep=rep1&type=pdf},
	shorttitle = {Survey Of Music Information Needs, Uses, And Seeking Behaviours},
	pages = {5th},
	booktitle = {{ISMIR}},
	publisher = {Citeseer},
	author = {Lee, Jin Ha and Downie, J. Stephen},
	urldate = {2015-02-02},
	date = {2004},
	keywords = {{citeDiss}}
}

@article{polk_rating_2002,
	title = {Rating the similarity of simple perceptual stimuli: asymmetries induced by manipulating exposure frequency},
	volume = {82},
	issn = {0010-0277},
	shorttitle = {Rating the similarity of simple perceptual stimuli},
	abstract = {When judging the similarity of two stimuli, people's ratings often differ depending on the order in which the comparison is presented (A vs. B or B vs. A). Such directional asymmetries have typically been demonstrated using complex concepts that have a large number of semantic features and a standard explanation is that different sets of features are emphasized depending on the direction of the comparison. In this study, we show that directional asymmetries in the similarity of simple perceptual stimuli can be predictably manipulated merely by presenting each member of a pair with different frequency. Participants rated the similarity of color patches before and after performing an irrelevant training task in which a subset of colors was presented ten times more frequently than others. The similarity ratings after training were significantly more asymmetric than the ratings before training. We discuss the implications of these findings for models of similarity judgment and propose a computationally explicit explanation based on asymmetries in representational stability.},
	pages = {B75--88},
	number = {3},
	journaltitle = {Cognition},
	shortjournal = {Cognition},
	author = {Polk, Thad A. and Behensky, Charles and Gonzalez, Richard and Smith, Edward E.},
	date = {2002-01},
	pmid = {11747865},
	keywords = {Adult, attention, {citeDiss}, Color Perception, Concept Formation, Discrimination Learning, Female, Humans, Male, Mental Recall, Neural Networks (Computer)}
}

@inproceedings{hiatt_role_,
	title = {The Role of Familiarity, Priming and Perception in Similarity Judgments},
	url = {http://www.nrl.navy.mil/itd/aic/sites/www.nrl.navy.mil.itd.aic/files/pdfs/hiatt-trafton-cogsci-2013.pdf},
	booktitle = {Proceedings of the 35th Annual Meeting of the Cognitive Science Society},
	publisher = {Cognitive Science Society},
	author = {Hiatt, Laura M. and Trafton, J. Gregory},
	urldate = {2015-01-24},
	keywords = {{citeDiss}}
}

@article{tversky_features_1977,
	title = {Features of similarity},
	volume = {84},
	rights = {(c) 2012 {APA}, all rights reserved},
	issn = {1939-1471(Electronic);0033-295X(Print)},
	doi = {10.1037/0033-295X.84.4.327},
	abstract = {Questions the metric and dimensional assumptions that underlie the geometric representation of similarity on both theoretical and empirical grounds. A new set-theoretical approach to similarity is developed in which objects are represented as collections of features and similarity is described as a feature-matching process. Specifically, a set of qualitative assumptions is shown to imply the contrast model, which expresses the similarity between objects as a linear combination of the measures of their common and distinctive features. Several predictions of the contrast model are tested in studies of similarity with both semantic and perceptual stimuli. The model is used to uncover, analyze, and explain a variety of empirical phenomena such as the role of common and distinctive features, the relations between judgments of similarity and difference, the presence of asymmetric similarities, and the effects of context on judgments of similarity. The contrast model generalizes standard representations of similarity data in terms of clusters and trees. It is also used to analyze the relations of prototypicality and family resemblance. (39 ref)},
	pages = {327--352},
	number = {4},
	journaltitle = {Psychological Review},
	author = {Tversky, Amos},
	date = {1977},
	keywords = {*Cognitive Processes, *Pattern Discrimination, *Stimulus Discrimination, {citeDiss}, Stimulus Similarity}
}

@article{marsden_interrogating_2012,
	title = {Interrogating Melodic Similarity: A Definitive Phenomenon or the Product of Interpretation?},
	volume = {41},
	issn = {0929-8215},
	url = {http://dx.doi.org/10.1080/09298215.2012.740051},
	doi = {10.1080/09298215.2012.740051},
	shorttitle = {Interrogating Melodic Similarity},
	abstract = {The nature of melodic similarity is interrogated through a survey of the different means by which the phenomenon has been studied, examination of methods for measuring melodic similarity, a Monte Carlo analysis of data from the experiment which formed the basis for the ‘ground truth’ used in the {MIREX} 2005 contest on melodic similarity, and examples of interest in the music of Mozart. Melodic similarity has been studied by a number of means, sometimes quite contrasting, which lead to important differences in the light of the finding that similarity is dependent on context. Models of melodic similarity based on reduction show that the existence of multiple possible reductions forms a natural basis for similarity to depend on interpretation. Examination of the {MIREX} 2005 data shows wide variations in subjects' judgements of melodic similarity and some evidence that the perceived similarity between two melodies can be influenced by the presence of a third melody. Examples from Mozart suggest that he deliberately exploited the possibilities inherent in recognizing similarity through different interpretations. It is therefore proposed that similarity be thought of not as a distinct and definite function of two melodies but as something created in the minds of those who hear the melodies.},
	pages = {323--335},
	number = {4},
	journaltitle = {Journal of New Music Research},
	author = {Marsden, Alan},
	urldate = {2015-01-23},
	date = {2012-12-01},
	keywords = {{citeDiss}},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\33XZXZJG\\09298215.2012.html:text/html}
}

@article{katter_influence_1968,
	title = {The influence of scale form on relevance judgments},
	volume = {4},
	issn = {0020-0271},
	url = {http://www.sciencedirect.com/science/article/pii/0020027168900028},
	doi = {10.1016/0020-0271(68)90002-8},
	abstract = {This paper reports the results of two studies. The first compared ranking and category rating procedures for measuring relevance of documents to information requirement statements. The comparison measure was number of reversals; the condition where document-requirement pair A is measured as more relevant than pair B by one procedure and as less relevant than pair B by the other. As compared to category rating, ranking produced three times the expected number of reversals. The results are explained in terms of a “cascaded distortion process” that can affect any procedure which arbitrarily restricts distribution shape.

The second study compared the stimulus range and anchoring sensitivities of a nine-point category scale and a magnitude-ratio scale procedure. Results from the category scale were more consistent and more as predicted. Magnitude ratio results were distorted by unrepresentative scale moduli selected by about one-sixth of the judges, a condition which may be correctable. Suggestions for improved anchoring procedures are discussed in light of the findings for the anchoring treatment.},
	pages = {1--11},
	number = {1},
	journaltitle = {Information Storage and Retrieval},
	shortjournal = {Information Storage and Retrieval},
	author = {Katter, R. V.},
	urldate = {2014-11-21},
	date = {1968-03},
	keywords = {{citeDiss}},
	file = {ScienceDirect Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\QXMCQ3RA\\0020027168900028.html:text/html}
}

@inproceedings{efron_building_2011,
	location = {New Orleans, {USA}},
	title = {Building Topic Models in a Federated Digital Library Through Selective Document Exclusion},
	series = {{ASIS}\&T '11},
	eventtitle = {{ASIS}\&T Annual Meeting},
	booktitle = {Proceedings of the American Society for Information Science and Technology},
	author = {Efron, Miles and Organisciak, Peter and Fenlon, Katrina},
	date = {2011-10},
	keywords = {{citeDiss}, {hcirCITE}},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\7R82INVG\\full.html:text/html}
}

@inproceedings{efron_improving_2012,
	location = {New York, {NY}, {USA}},
	title = {Improving Retrieval of Short Texts Through Document Expansion},
	isbn = {978-1-4503-1472-5},
	url = {http://doi.acm.org/10.1145/2348283.2348405},
	doi = {10.1145/2348283.2348405},
	series = {{SIGIR} '12},
	abstract = {Collections containing a large number of short documents are becoming increasingly common. As these collections grow in number and size, providing effective retrieval of brief texts presents a significant research problem. We propose a novel approach to improving information retrieval ({IR}) for short texts based on aggressive document expansion. Starting from the hypothesis that short documents tend to be about a single topic, we submit documents as pseudo-queries and analyze the results to learn about the documents themselves. Document expansion helps in this context because short documents yield little in the way of term frequency information. However, as we show, the proposed technique helps us model not only lexical properties, but also temporal properties of documents. We present experimental results using a corpus of microblog (Twitter) data and a corpus of metadata records from a federated digital library. With respect to established baselines, results of these experiments show that applying our proposed document expansion method yields significant improvements in effectiveness. Specifically, our method improves the lexical representation of documents and the ability to let time influence retrieval.},
	pages = {911--920},
	booktitle = {Proceedings of the 35th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Efron, Miles and Organisciak, Peter and Fenlon, Katrina},
	urldate = {2014-11-11},
	date = {2012},
	keywords = {{citeDiss}, document expansion, dublin core, {hcirCITE}, {INFORMATION} retrieval, language models, microblogs, temporal {IR}, twitter}
}

@inproceedings{little_turkit_2009,
	location = {New York, {NY}, {USA}},
	title = {{TurKit}: Tools for Iterative Tasks on Mechanical Turk},
	isbn = {978-1-60558-672-4},
	url = {http://doi.acm.org/10.1145/1600150.1600159},
	doi = {10.1145/1600150.1600159},
	series = {{HCOMP} '09},
	shorttitle = {{TurKit}},
	abstract = {Mechanical Turk ({MTurk}) is an increasingly popular web service for paying people small rewards to do human computation tasks. Current uses of {MTurk} typically post independent parallel tasks. We are exploring an alternative iterative paradigm, in which workers build on or evaluate each other's work. We describe {TurKit}, a new toolkit for deploying iterative tasks to {MTurk}, with a familiar imperative programming paradigm that effectively uses {MTurk} workers as subroutines.},
	pages = {29--30},
	booktitle = {Proceedings of the {ACM} {SIGKDD} Workshop on Human Computation},
	publisher = {{ACM}},
	author = {Little, Greg and Chilton, Lydia B. and Goldman, Max and Miller, Robert C.},
	urldate = {2014-10-13},
	date = {2009},
	keywords = {{citeDiss}, {citeDissProp}, human computation, mechanical turk, toolkit}
}

@book{simon_participatory_2010,
	title = {The participatory museum},
	url = {http://books.google.com/books?hl=en&lr=&id=qun060HUcOcC&oi=fnd&pg=PR1&dq=participatory+museum&ots=EdEfFdpVEc&sig=tyCA-iZQRPqQcvFOpw2XW8UwDqo},
	publisher = {Museum 2.0},
	author = {Simon, Nina},
	urldate = {2014-09-05},
	date = {2010},
	keywords = {{citeDiss}},
	file = {[HTML] from google.com:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\HE9HVAW9\\books.html:text/html}
}

@inproceedings{wiggins_goals_2012,
	title = {Goals and Tasks: Two Typologies of Citizen Science Projects},
	doi = {10.1109/HICSS.2012.295},
	shorttitle = {Goals and Tasks},
	abstract = {Citizen science is a form of research collaboration involving members of the public in scientific research projects to address real-world problems. Often organized as a virtual collaboration, these projects are a type of open movement, with collective goals addressed through open participation in research tasks. We conducted a survey of citizen science projects to elicit multiple aspects of project design and operation. We then clustered projects based on the tasks performed by participants and on the project's stated goals. The clustering results group projects that show similarities along other dimensions, suggesting useful divisions of the projects.},
	eventtitle = {2012 45th Hawaii International Conference on System Science ({HICSS})},
	pages = {3426--3435},
	booktitle = {2012 45th Hawaii International Conference on System Science ({HICSS})},
	author = {Wiggins, A and Crowston, Kevin},
	date = {2012-01},
	keywords = {Approximation methods, {citeDiss}, Citizen science, citizen science project, Collaboration, Communities, Educational institutions, Electronic mail, groupware, Monitoring, open movement, Production, project design, project operation, real-world problem, research collaboration, scientific information systems, scientific research project, virtual collaboration},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\NJQZCP74\\Wiggins and Crowston - 2012 - Goals and Tasks Two Typologies of Citizen Science.html:text/html}
}

@article{von_ahn_recaptcha_2008,
	title = {recaptcha: Human-based character recognition via web security measures},
	volume = {321},
	url = {http://www.sciencemag.org/content/321/5895/1465.short},
	shorttitle = {recaptcha},
	pages = {1465--1468},
	number = {5895},
	journaltitle = {Science},
	author = {Von Ahn, Luis and Maurer, Benjamin and {McMillen}, Colin and Abraham, David and Blum, Manuel},
	urldate = {2014-08-25},
	date = {2008},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {[HTML] from sciencemag.org:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\NEKH8S8Q\\1465.html:text/html}
}

@article{khatib_algorithm_2011,
	title = {Algorithm discovery by protein folding game players},
	volume = {108},
	url = {http://www.pnas.org/content/108/47/18949.short},
	pages = {18949--18953},
	number = {47},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Khatib, Firas and Cooper, Seth and Tyka, Michael D. and Xu, Kefan and Makedon, Ilya and Popović, Zoran and Baker, David and Players, Foldit},
	urldate = {2014-08-25},
	date = {2011},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {[HTML] from pnas.org:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\EBN8D2SM\\18949.html:text/html;Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\U7W2U2FU\\18949.html:text/html}
}

@book{hotho_information_2006,
	title = {Information retrieval in folksonomies: Search and ranking},
	url = {http://link.springer.com/chapter/10.1007/11762256_31},
	shorttitle = {Information retrieval in folksonomies},
	publisher = {Springer},
	author = {Hotho, Andreas and Jäschke, Robert and Schmitz, Christoph and Stumme, Gerd},
	urldate = {2014-08-25},
	date = {2006},
	keywords = {{citeDiss}, {citeDissProp}, folksonomy, Information Retrieval},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\2JCEKW2A\\11762256_31.html:text/html}
}

@inproceedings{zhou_exploring_2008,
	location = {New York, {NY}, {USA}},
	title = {Exploring Social Annotations for Information Retrieval},
	isbn = {978-1-60558-085-2},
	url = {http://doi.acm.org/10.1145/1367497.1367594},
	doi = {10.1145/1367497.1367594},
	series = {{WWW} '08},
	abstract = {Social annotation has gained increasing popularity in many Web-based applications, leading to an emerging research area in text analysis and information retrieval. This paper is concerned with developing probabilistic models and computational algorithms for social annotations. We propose a unified framework to combine the modeling of social annotations with the language modeling-based methods for information retrieval. The proposed approach consists of two steps: (1) discovering topics in the contents and annotations of documents while categorizing the users by domains; and (2) enhancing document and query language models by incorporating user domain interests as well as topical background models. In particular, we propose a new general generative model for social annotations, which is then simplified to a computationally tractable hierarchical Bayesian network. Then we apply smoothing techniques in a risk minimization framework to incorporate the topical information to language models. Experiments are carried out on a real-world annotation data set sampled from del.icio.us. Our results demonstrate significant improvements over the traditional approaches.},
	pages = {715--724},
	booktitle = {Proceedings of the 17th International Conference on World Wide Web},
	publisher = {{ACM}},
	author = {Zhou, Ding and Bian, Jiang and Zheng, Shuyi and Zha, Hongyuan and Giles, C. Lee},
	urldate = {2014-08-25},
	date = {2008},
	keywords = {{citeDiss}, {citeDissProp}, folksonomy, {INFORMATION} retrieval, language modeling, social annotations},
	file = {Zhou-et-al_2008_Exploring Social Annotations for Information Retrieval.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\ACM2008\\Zhou-et-al_2008_Exploring Social Annotations for Information Retrieval.pdf:application/pdf}
}

@inproceedings{bischoff_can_2008,
	location = {New York, {NY}, {USA}},
	title = {Can All Tags Be Used for Search?},
	isbn = {978-1-59593-991-3},
	url = {http://doi.acm.org/10.1145/1458082.1458112},
	doi = {10.1145/1458082.1458112},
	series = {{CIKM} '08},
	abstract = {Collaborative tagging has become an increasingly popular means for sharing and organizing Web resources, leading to a huge amount of user generated metadata. These tags represent quite a few different aspects of the resources they describe and it is not obvious whether and how these tags or subsets of them can be used for search. This paper is the first to present an in-depth study of tagging behavior for very different kinds of resources and systems - Web pages (Del.icio.us), music (Last.fm), and images (Flickr) - and compares the results with anchor text characteristics. We analyze and classify sample tags from these systems, to get an insight into what kinds of tags are used for different resources, and provide statistics on tag distributions in all three tagging environments. Since even relevant tags may not add new information to the search procedure, we also check overlap of tags with content, with metadata assigned by experts and from other sources. We discuss the potential of different kinds of tags for improving search, comparing them with user queries posted to search engines as well as through a user survey. The results are promising and provide more insight into both the use of different kinds of tags for improving search and possible extensions of tagging systems to support the creation of potentially search-relevant tags.},
	pages = {193--202},
	booktitle = {Proceedings of the 17th {ACM} Conference on Information and Knowledge Management},
	publisher = {{ACM}},
	author = {Bischoff, Kerstin and Firan, Claudiu S. and Nejdl, Wolfgang and Paiu, Raluca},
	urldate = {2014-08-25},
	date = {2008},
	keywords = {{citeDiss}, {citeDissProp}, collaborative tagging, query classification, tag classification, tagging system analysis and comparison, tag search},
	file = {Bischoff-et-al_2008_Can All Tags Be Used for Search.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\ACM2008\\Bischoff-et-al_2008_Can All Tags Be Used for Search.pdf:application/pdf}
}

@inproceedings{heymann_can_2008,
	location = {New York, {NY}, {USA}},
	title = {Can Social Bookmarking Improve Web Search?},
	isbn = {978-1-59593-927-2},
	url = {http://doi.acm.org/10.1145/1341531.1341558},
	doi = {10.1145/1341531.1341558},
	series = {{WSDM} '08},
	abstract = {Social bookmarking is a recent phenomenon which has the potential to give us a great deal of data about pages on the web. One major question is whether that data can be used to augment systems like web search. To answer this question, over the past year we have gathered what we believe to be the largest dataset from a social bookmarking site yet analyzed by academic researchers. Our dataset represents about forty million bookmarks from the social bookmarking site del.icio.us. We contribute a characterization of posts to del.icio. us: how many bookmarks exist (about 115 million), how fast is it growing, and how active are the {URLs} being posted about (quite active). We also contribute a characterization of tags used by bookmarkers. We found that certain tags tend to gravitate towards certain domains, and vice versa. We also found that tags occur in over 50 percent of the pages that they annotate, and in only 20 percent of cases do they not occur in the page text, backlink page text, or forward link page text of the pages they annotate. We conclude that social bookmarking can provide search data not currently provided by other sources, though it may currently lack the size and distribution of tags necessary to make a significant impact},
	pages = {195--206},
	booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
	publisher = {{ACM}},
	author = {Heymann, Paul and Koutrika, Georgia and Garcia-Molina, Hector},
	urldate = {2014-08-25},
	date = {2008},
	keywords = {{citeDiss}, {citeDissProp}, collaborative tagging, social bookmarking, web search}
}

@inproceedings{bao_optimizing_2007,
	location = {New York, {NY}, {USA}},
	title = {Optimizing Web Search Using Social Annotations},
	isbn = {978-1-59593-654-7},
	url = {http://doi.acm.org/10.1145/1242572.1242640},
	doi = {10.1145/1242572.1242640},
	series = {{WWW} '07},
	abstract = {This paper explores the use of social annotations to improve websearch. Nowadays, many services, e.g. del.icio.us, have been developed for web users to organize and share their favorite webpages on line by using social annotations. We observe that the social annotations can benefit web search in two aspects: 1) the annotations are usually good summaries of corresponding webpages; 2) the count of annotations indicates the popularity of webpages. Two novel algorithms are proposed to incorporate the above information into page ranking: 1) {SocialSimRank} ({SSR})calculates the similarity between social annotations and webqueries; 2) {SocialPageRank} ({SPR}) captures the popularity of webpages. Preliminary experimental results show that {SSR} can find the latent semantic association between queries and annotations, while {SPR} successfully measures the quality (popularity) of a webpage from the web users' perspective. We further evaluate the proposed methods empirically with 50 manually constructed queries and 3000 auto-generated queries on a dataset crawledfrom delicious. Experiments show that both {SSR} and {SPRbenefit} web search significantly.},
	pages = {501--510},
	booktitle = {Proceedings of the 16th International Conference on World Wide Web},
	publisher = {{ACM}},
	author = {Bao, Shenghua and Xue, Guirong and Wu, Xiaoyuan and Yu, Yong and Fei, Ben and Su, Zhong},
	urldate = {2014-08-25},
	date = {2007},
	keywords = {{citeDiss}, {citeDissProp}, evaluation, social annotation, social page rank, social similarity, web search}
}

@inproceedings{mccreadie_crowdsourcing_2011,
	title = {Crowdsourcing blog track top news judgments at {TREC}},
	url = {http://ir.ischool.utexas.edu/csdm2011/csdm2011_proceedings.pdf#page=23},
	pages = {23--26},
	booktitle = {Proceedings of the workshop on crowdsourcing for search and data mining ({CSDM}) at the fourth {ACM} international conference on web search and data mining ({WSDM})},
	author = {{McCreadie}, Richard and Macdonald, Craig and Ounis, Iadh},
	urldate = {2014-08-25},
	date = {2011},
	keywords = {{citeDiss}, {citeDissProp}, {TREC} crowd}
}

@inproceedings{mccreadie_university_2011,
	title = {University of Glasgow at {TREC} 2011: Experiments with Terrier in Crowdsourcing, Microblog, and Web Tracks.},
	url = {http://homepages.dcc.ufmg.br/~rodrygo/wp-content/papercite-data/pdf/mccreadie2011trec.pdf},
	shorttitle = {University of Glasgow at {TREC} 2011},
	booktitle = {{TREC}},
	author = {{McCreadie}, Richard and Macdonald, Craig and Santos, Rodrygo {LT} and Ounis, Iadh},
	urldate = {2014-08-25},
	date = {2011},
	keywords = {{citeDiss}, {citeDissProp}, {TREC} crowd}
}

@report{smucker_overview_2012,
	title = {Overview of the trec 2012 crowdsourcing track},
	url = {http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA578659},
	institution = {{DTIC} Document},
	author = {Smucker, Mark D. and Kazai, Gabriella and Lease, Matthew},
	urldate = {2014-08-25},
	date = {2012},
	keywords = {{citeDiss}, {citeDissProp}, {TREC} crowd},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\ISWTCKJB\\oai.html:text/html}
}

@article{rouse_preliminary_2010,
	title = {A Preliminary Taxonomy of Crowdsourcing},
	url = {http://aisel.aisnet.org/acis2010/76},
	journaltitle = {{ACIS} 2010 Proceedings},
	author = {Rouse, Anne},
	date = {2010-01-01},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14, crowdsourcing, Taxonomy}
}

@article{fung_larry_,
	title = {Larry Lessig’s super {PAC} to end super {PACs} raised \$2.5 million in just 2 days. Here’s what comes next.},
	issn = {0190-8286},
	url = {http://www.washingtonpost.com/blogs/the-switch/wp/2014/07/07/larry-lessigs-super-pac-to-end-super-pacs-raised-2-5-million-in-2-days/},
	abstract = {The average donation was around \$140, and other numbers.},
	journaltitle = {The Washington Post},
	author = {Fung, Brian},
	urldate = {2014-08-21},
	langid = {american},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Washington Post Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\6K2WEWH8\\larry-lessigs-super-pac-to-end-super-pacs-raised-2-5-million-in-2-days.html:text/html}
}

@article{cortese_crowdfunding_2013,
	title = {Crowdfunding for Small Business Is Still an Unclear Path},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2013/01/06/business/crowdfunding-for-small-business-is-still-an-unclear-path.html},
	abstract = {For new businesses, the regulatory path to crowdfunding — Internet-based financing that involves ordinary investors — is still far from clear.},
	journaltitle = {The New York Times},
	author = {Cortese, Amy},
	urldate = {2014-08-21},
	date = {2013-01-05},
	keywords = {Banking and Financial Institutions, {CircleUp} Network Inc, {citeDiss}, {citeDissProp}, Crowdfunding (Internet), Entrepreneurship, Regulation and Deregulation of Industry, Securities and Exchange Commission, Small Business, {SoMoLend} Holdings {LLC}},
	file = {New York Times Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\AU9UVDGM\\crowdfunding-for-small-business-is-still-an-unclear-path.html:text/html}
}

@article{cortese_proposal_2011,
	title = {A Proposal to Allow Small Private Companies to Get Investors Online},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2011/09/26/opinion/a-proposal-to-allow-small-private-companies-to-get-investors-online.html},
	abstract = {A proposal on crowdfunding would make it legal for ordinary investors to put some money (but not enough to bankrupt them) into small, private companies online.},
	journaltitle = {The New York Times},
	author = {Cortese, Amy},
	urldate = {2014-08-21},
	date = {2011-09-25},
	keywords = {{citeDiss}, {citeDissProp}, Computers and the Internet, Entrepreneurship, Finances, Kickstarter, Kiva.org, {McHenry}, Patrick T, Obama, Barack, Regulation and Deregulation of Industry, Securities and Exchange Commission, Small Business},
	file = {New York Times Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\P9MH2MMJ\\a-proposal-to-allow-small-private-companies-to-get-investors-online.html:text/html}
}

@inproceedings{komarov_crowdsourcing_2013,
	location = {New York, {NY}, {USA}},
	title = {Crowdsourcing Performance Evaluations of User Interfaces},
	isbn = {978-1-4503-1899-0},
	url = {http://doi.acm.org/10.1145/2470654.2470684},
	doi = {10.1145/2470654.2470684},
	series = {{CHI} '13},
	abstract = {Online labor markets, such as Amazon's Mechanical Turk ({MTurk}), provide an attractive platform for conducting human subjects experiments because the relative ease of recruitment, low cost, and a diverse pool of potential participants enable larger-scale experimentation and faster experimental revision cycle compared to lab-based settings. However, because the experimenter gives up the direct control over the participants' environments and behavior, concerns about the quality of the data collected in online settings are pervasive. In this paper, we investigate the feasibility of conducting online performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via {MTurk}. We implemented three performance experiments to re-evaluate three previously well-studied user interface designs. We conducted each experiment both in lab and online with participants recruited via {MTurk}. The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings: All statistically significant differences detected in lab were also present on {MTurk} and the effect sizes were similar. In addition, there were no significant differences between the two settings in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments. These results suggest that {MTurk} may be a productive setting for conducting performance evaluations of user interfaces providing a complementary approach to existing methodologies.},
	pages = {207--216},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Komarov, Steven and Reinecke, Katharina and Gajos, Krzysztof Z.},
	urldate = {2014-08-17},
	date = {2013},
	keywords = {{citeDiss}, {citeDissProp}, crowdsourcing, mechanical turk, user interface evaluation}
}

@inproceedings{yilmaz_simple_2008,
	location = {New York, {NY}, {USA}},
	title = {A Simple and Efficient Sampling Method for Estimating {AP} and {NDCG}},
	isbn = {978-1-60558-164-4},
	url = {http://doi.acm.org/10.1145/1390334.1390437},
	doi = {10.1145/1390334.1390437},
	series = {{SIGIR} '08},
	abstract = {We consider the problem of large scale retrieval evaluation. Recently two methods based on random sampling were proposed as a solution to the extensive effort required to judge tens of thousands of documents. While the first method proposed by Aslam et al. [1] is quite accurate and efficient, it is overly complex, making it difficult to be used by the community, and while the second method proposed by Yilmaz et al., {infAP} [14], is relatively simple, it is less efficient than the former since it employs uniform random sampling from the set of complete judgments. Further, none of these methods provide confidence intervals on the estimated values. The contribution of this paper is threefold: (1) we derive confidence intervals for {infAP}, (2) we extend {infAP} to incorporate nonrandom relevance judgments by employing stratified random sampling, hence combining the efficiency of stratification with the simplicity of random sampling, (3) we describe how this approach can be utilized to estimate {nDCG} from incomplete judgments. We validate the proposed methods using {TREC} data and demonstrate that these new methods can be used to incorporate nonrandom samples, as were available in {TREC} Terabyte track '06.},
	pages = {603--610},
	booktitle = {Proceedings of the 31st Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Yilmaz, Emine and Kanoulas, Evangelos and Aslam, Javed A.},
	urldate = {2014-08-16},
	date = {2008},
	keywords = {average precision, {citeDiss}, {citeDissProp}, evaluation, incomplete judgments, {infAP}, {nDCG}, sampling}
}

@article{maslow_theory_1943,
	title = {A theory of human motivation},
	volume = {50},
	rights = {(c) 2012 {APA}, all rights reserved},
	issn = {1939-1471(Electronic);0033-295X(Print)},
	doi = {10.1037/h0054346},
	abstract = {After listing the propositions that must be considered as basic, the author formulates a theory of human motivation in line with these propositions and with the known facts derived from observation and experiment. There are 5 sets of goals (basic needs) which are related to each other and are arranged in a hierarchy of prepotency. When the most prepotent goal is realized, the next higher need emerges. "Thus man is a perpetually wanting animal." Thwarting, actual or imminent, of these basic needs provides a psychological threat that leads to psychopathy.},
	pages = {370--396},
	number = {4},
	journaltitle = {Psychological Review},
	author = {Maslow, A.H.},
	date = {1943},
	keywords = {*Motivation, Antisocial Personality Disorder, {citeDiss}, {citeDissProp}, {citeiConf}14, Human Development}
}

@article{alderfer_empirical_1969,
	title = {An empirical test of a new theory of human needs},
	volume = {4},
	url = {http://www.sciencedirect.com/science/article/pii/003050736990004X},
	pages = {142--175},
	number = {2},
	journaltitle = {Organizational behavior and human performance},
	author = {Alderfer, Clayton P.},
	urldate = {2014-08-16},
	date = {1969},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14},
	file = {[HTML] from sciencedirect.com:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\2WFJHRP3\\003050736990004X.html:text/html}
}

@article{sanger_fate_2009,
	title = {The Fate of Expertise after Wikipedia},
	volume = {6},
	doi = {10.3366/E1742360008000543},
	pages = {52--73},
	number = {1},
	journaltitle = {Episteme},
	author = {Sanger, Lawrence M.},
	date = {2009},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14},
	file = {Cambridge Journals Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\T8JHP7G4\\displayAbstract.html:text/html}
}

@article{angwin_volunteers_2009,
	title = {Volunteers log off as Wikipedia ages},
	volume = {23},
	url = {https://secure.strategyone.net/mtr/Canon/2009/2009_11/228771_11252009/BIOE3434479_1_H.pdf},
	journaltitle = {Wall Street Journal},
	author = {Angwin, Julia and Fowler, Geoffrey A.},
	urldate = {2014-08-15},
	date = {2009},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14}
}

@online{_about_,
	title = {About Pinterest},
	url = {http://about.pinterest.com/en},
	titleaddon = {Pinterest},
	urldate = {2014-08-15},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\CJEDEUWD\\en.html:text/html}
}

@article{thompson_if_2008,
	title = {If You Liked This, You’re Sure to Love That},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2008/11/23/magazine/23Netflix-t.html},
	abstract = {Basement hackers and amateur mathematicians are competing to improve the program that Netflix uses to recommend {DVDs} — and to win \$1 million in the process.},
	journaltitle = {The New York Times},
	author = {Thompson, Clive},
	urldate = {2014-08-13},
	date = {2008-11-23},
	keywords = {{citeDiss}, {citeDissProp}, Computers and the Internet, {DVD} (Digital Versatile Disc), Motion Pictures, Netflix Incorporated, Ratings and Rating Systems, Retail Stores and Trade},
	file = {New York Times Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\3IBHT4N3\\23Netflix-t.html:text/html}
}

@inreference{_wikipedia:size_2014,
	title = {Wikipedia:Size of Wikipedia},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {http://en.wikipedia.org/w/index.php?title=Wikipedia:Size_of_Wikipedia&oldid=615924147},
	shorttitle = {Wikipedia},
	abstract = {This Wikipedia:Statistics page measures the size of the English-language edition of Wikipedia; mostly page and article count. There are currently 4,579,708 articles in the English Wikipedia.},
	booktitle = {Wikipedia, the free encyclopedia},
	urldate = {2014-08-13},
	date = {2014-08-13},
	langid = {english},
	note = {Page Version {ID}: 615924147},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\7RGJ2NUT\\index.html:text/html}
}

@inproceedings{agichtein_improving_2006,
	location = {New York, {NY}, {USA}},
	title = {Improving Web Search Ranking by Incorporating User Behavior Information},
	isbn = {1-59593-369-7},
	url = {http://doi.acm.org/10.1145/1148170.1148177},
	doi = {10.1145/1148170.1148177},
	series = {{SIGIR} '06},
	abstract = {We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting. We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features. We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine. We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31\% relative to the original performance.},
	pages = {19--26},
	booktitle = {Proceedings of the 29th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Agichtein, Eugene and Brill, Eric and Dumais, Susan},
	urldate = {2014-08-11},
	date = {2006},
	keywords = {{citeDiss}, {citeDissProp}, implicit relevance feedback, web search, web search ranking}
}

@online{taylor_friendfeed_2007,
	title = {{FriendFeed} Blog: I like it, I like it},
	url = {http://blog.friendfeed.com/2007/10/i-like-it-i-like-it.html},
	shorttitle = {{FriendFeed} Blog},
	titleaddon = {friendblog},
	author = {Taylor, Bret},
	urldate = {2014-08-09},
	date = {2007-10-30},
	keywords = {{citeDiss}, {citeDissProp}, friendfeed, ratings, rating scales},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\HMFGS4ZS\\i-like-it-i-like-it.html:text/html}
}

@book{abt_serious_1987,
	title = {Serious Games},
	isbn = {978-0-8191-6148-2},
	abstract = {The author explores the ways in which games can be used to instruct and inform as well as provide pleasure. He uses innovative approaches to problem solving through individualized game techniques. Topics include: improving education with games; educational games for the physical and social sciences; games for the learning disadvantaged; games for occupational choice and training; games for planning and problem solving in government and industry; and the future of serious games. This book was originally published in 1970 by Viking Press.},
	pagetotal = {200},
	publisher = {University Press of America},
	author = {Abt, Clark C.},
	date = {1987-01-01},
	langid = {english},
	keywords = {{citeDiss}, {citeDissProp}, Education / Experimental Methods, gamification, Psychology / Psychotherapy / Counseling, serious games}
}

@book{michael_serious_2005,
	title = {Serious Games: Games That Educate, Train, and Inform},
	isbn = {1-59200-622-1},
	shorttitle = {Serious Games},
	abstract = {Coverage includes- David "{RM}" Michael has been a successful independent software developer for over 10 years, working in a variety of industries, including video games. He is the owner of {DavidRM} Software (www.davidrm.com) and co-owner of Samu Games (www.samugames.com). Michael is the author of The Indie Game Development Survival Guide, and his articles about game design, development, and the game development industry have appeared on {GameDev}.net (www.gamedev.net) and in the book Game Design Perspectives. His blog about independent games, serious games, and independent software is Joe Indie (www.joeindie.com). Sande Chen has been active in the gaming industry for over five years. She has written for mainstream and industry publications, including Secrets of the Game Business, and was a speaker at the 2005 Game Developers Conference. Her past game credits include Independent Games Festival winner Terminus, Scooby-Doo, and {JamDat} Scrabble. Chen holds dual degrees in economics and in writing and humanistic studies from the Massachusetts Institute of Technology, an M.Sc. in economics from the London School of Economics, and an M.F.A. in cinema-television from the University of Southern California. In 1996, she was nominated for a Grammy in music video direction. She currently works as a freelance writer/game designer. Covers techniques to make entertainment-oriented games richer and provide a deeper experience. The focus on serious games continues to grow--from coverage in the media to conferences and buzz within the game development community. Provides an overview of the major markets for serious games, including current examples and future anticipation.},
	publisher = {Muska \& Lipman/Premier-Trade},
	author = {Michael, David R. and Chen, Sandra L.},
	date = {2005},
	keywords = {{citeDiss}, {citeDissProp}, gamification, serious games}
}

@book{ritterfeld_serious_2010,
	title = {Serious Games: Mechanisms and Effects},
	isbn = {978-1-135-84891-0},
	shorttitle = {Serious Games},
	pagetotal = {553},
	publisher = {Routledge},
	author = {Ritterfeld, Ute and Cody, Michael and Vorderer, Peter},
	date = {2010-04-26},
	langid = {english},
	keywords = {{citeDiss}, {citeDissProp}, Education / Computers \& Technology, Games / Video \& Electronic, gamification, Language Arts \& Disciplines / Communication Studies, serious games, Social Science / Media Studies}
}

@book{twain_adventures_1920,
	title = {The Adventures of Tom Sawyer},
	abstract = {The book that introduced the world to the iconic American characters of Tom Sawyer and Huckleberry Finn, this 1876 novel by Mark Twain follows the mischievous exploits of the two young boys, who find themselves in situations both humorous and dangerous. Never short of ways to stir up trouble in his hometown on the Mississippi River, Tom uses his wits to get both in and out of tight spots, often with Huck at his side. Featuring moments of significant social commentary, these interconnected tales essentially served as a dry run for Twain's notably weightier sequel, Adventures of Huckleberry Finn.},
	pagetotal = {330},
	publisher = {Harper \& brothers},
	author = {Twain, Mark},
	date = {1920},
	langid = {english},
	keywords = {{citeDiss}, {citeDissProp}}
}

@misc{_requester_2011,
	title = {Requester Best Practices},
	url = {http://mturkpublic.s3.amazonaws.com/docs/MTURK_BP.pdf},
	abstract = {Amazon Mechanical Turk is a marketplace for work where businesses (aka Requesters) publish tasks (aka 
{HITS}), and human providers (aka Workers) complete them. Amazon Mechanical Turk gives businesses 
immediate access to a diverse, global, on-demand, scalable workforce and gives Workers a selection of 
thousands of tasks to complete whenever and wherever it's convenient. 
There are many ways to structure your work in Mechanical Turk. This guide helps you optimize your 
approach to using Mechanical Turk to get the most accurate results at the best price with the turnaround 
time your business needs. Use this guide as you plan, design, and test your Amazon Mechanical Turk 
{HITs}},
	publisher = {Amazon Web Services {LLC}},
	urldate = {2014-08-08},
	date = {2011-06},
	keywords = {{citeDiss}, {citeDissProp}}
}

@inproceedings{ponte_language_1998,
	location = {New York, {NY}, {USA}},
	title = {A Language Modeling Approach to Information Retrieval},
	isbn = {1-58113-015-5},
	url = {http://doi.acm.org/10.1145/290941.291008},
	doi = {10.1145/290941.291008},
	series = {{SIGIR} '98},
	pages = {275--281},
	booktitle = {Proceedings of the 21st Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Ponte, Jay M. and Croft, W. Bruce},
	urldate = {2014-07-24},
	date = {1998},
	keywords = {candidates, {citeDiss}, {citeDissProp}}
}

@inproceedings{song_general_1999,
	location = {New York, {NY}, {USA}},
	title = {A General Language Model for Information Retrieval},
	isbn = {1-58113-146-1},
	url = {http://doi.acm.org/10.1145/319950.320022},
	doi = {10.1145/319950.320022},
	series = {{CIKM} '99},
	abstract = {Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and {TREC}4 data sets showed that the performance of our model is comparable to that of {INQUERY} and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance.},
	pages = {316--321},
	booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
	publisher = {{ACM}},
	author = {Song, Fei and Croft, W. Bruce},
	urldate = {2014-07-24},
	date = {1999},
	keywords = {{citeDiss}, {citeDissProp}, curve-fitting functions, good-turing estimate, model combinations, statistical language modeling}
}

@inproceedings{dong_time_2010,
	location = {New York, {NY}, {USA}},
	title = {Time is of the Essence: Improving Recency Ranking Using Twitter Data},
	isbn = {978-1-60558-799-8},
	url = {http://doi.acm.org/10.1145/1772690.1772725},
	doi = {10.1145/1772690.1772725},
	series = {{WWW} '10},
	shorttitle = {Time is of the Essence},
	abstract = {Realtime web search refers to the retrieval of very fresh content which is in high demand. An effective portal web search engine must support a variety of search needs, including realtime web search. However, supporting realtime web search introduces two challenges not encountered in non-realtime web search: quickly crawling relevant content and ranking documents with impoverished link and click information. In this paper, we advocate the use of realtime micro-blogging data for addressing both of these problems. We propose a method to use the micro-blogging data stream to detect fresh {URLs}. We also use micro-blogging data to compute novel and effective features for ranking fresh {URLs}. We demonstrate these methods improve effective of the portal web search engine for realtime web search.},
	pages = {331--340},
	booktitle = {Proceedings of the 19th International Conference on World Wide Web},
	publisher = {{ACM}},
	author = {Dong, Anlei and Zhang, Ruiqiang and Kolari, Pranam and Bai, Jing and Diaz, Fernando and Chang, Yi and Zheng, Zhaohui and Zha, Hongyuan},
	urldate = {2014-07-21},
	date = {2010},
	keywords = {{citeDiss}, {citeDissProp}, recency modeling, recency ranking, twitter}
}

@inproceedings{zhai_model-based_2001,
	location = {New York, {NY}, {USA}},
	title = {Model-based Feedback in the Language Modeling Approach to Information Retrieval},
	isbn = {1-58113-436-3},
	url = {http://doi.acm.org/10.1145/502585.502654},
	doi = {10.1145/502585.502654},
	series = {{CIKM} '01},
	abstract = {The language modeling approach to retrieval has been shown to perform well empirically. One advantage of this new approach is its statistical foundations. However, feedback, as one important component in a retrieval system, has only been dealt with heuristically in this new retrieval approach: the original query is usually literally expanded by adding additional terms to it. Such expansion-based feedback creates an inconsistent interpretation of the original and the expanded query. In this paper, we present a more principled approach to feedback in the language modeling approach. Specifically, we treat feedback as updating the query language model based on the extra evidence carried by the feedback documents. Such a model-based feedback strategy easily fits into an extension of the language modeling approach. We propose and evaluate two different approaches to updating a query language model based on feedback documents, one based on a generative probabilistic model of feedback documents and one based on minimization of the {KL}-divergence over feedback documents. Experiment results show that both approaches are effective and outperform the Rocchio feedback approach.},
	pages = {403--410},
	booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
	publisher = {{ACM}},
	author = {Zhai, Chengxiang and Lafferty, John},
	urldate = {2014-07-21},
	date = {2001},
	keywords = {{citeDiss}, {citeDissProp}}
}

@article{von_hippel_democratizing_2006,
	title = {Democratizing Innovation},
	url = {http://econpapers.repec.org/bookchap/mtptitles/0262720477.htm},
	author = {von Hippel, Eric},
	urldate = {2014-06-12},
	date = {2006},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\6N5NBVDI\\0262720477.html:text/html}
}

@report{von_hippel_sources_1988,
	location = {Rochester, {NY}},
	title = {The Sources of Innovation},
	url = {http://papers.ssrn.com.proxy2.library.illinois.edu/abstract=1496218},
	abstract = {Presents a series of studies showing that the sources of innovation vary greatly; possible sources include innovation users, suppliers of innovation-related components, and product manufacturers. These types of roles are known as functional areas. Specific areas of innovation are marked by having innovators predominantly in one specific functional area. Using empirical data from industrial histories, the authors show that this innovation-function relationship has held in scientific instrument, semiconductor and printed circuit board assembly process innovations. Users are predominantly the innovators in these fields. Also identifies a few industries where manufacturers are typically the innovators and a few others where suppliers tend to be.        Analysis of the economic rents of innovation expected by potential innovators can often, if not always, by itself predict the functional source of innovation. Innovating firms will do so only when these rents prove attractive. Two factors suggest that this will tend to limit exploitation of the innovation to a functional area. First, it is difficult for innovators to adopt new functional relationships to their innovations. Second, innovators face a poor ability to capture innovation rents by licensing their innovation-related knowledge to others. This hypothesis and its implications are tested against the empirical datasets used initially. The role of informal R\&D know-how trading is also discussed and analyzed using the Prisoner's Dilemma. Guidance is given to innovation managers and policymakers.  ({CAR})},
	number = {{ID} 1496218},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {von Hippel, Eric},
	urldate = {2014-06-11},
	date = {1988},
	keywords = {{citeDiss}, {citeDissProp}, Economic rents, Industrial research, Information behavior, Information exchange, Innovation management, Innovation policies, Innovation process, Know-how, Licensing strategies, Manufacturing firms, R\&D, Suppliers, User needs},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\TZQ5Q426\\login.html:text/html}
}

@misc{norvig_english_,
	title = {English Letter Frequency Counts: Mayzner Revisited or {ETAOIN} {SRHLDCU}},
	url = {http://norvig.com/mayzner.html},
	shorttitle = {English Letter Frequency Counts},
	author = {Norvig, Peter},
	urldate = {2014-05-20},
	keywords = {{citeDiss}, {citeDissProp}}
}

@article{downie_music_2006,
	title = {The Music Information Retrieval Evaluation {eXchange} ({MIREX})},
	volume = {12},
	pages = {795--825},
	number = {12},
	journaltitle = {D-Lib Magazine},
	author = {Downie, J. Stephen},
	date = {2006},
	keywords = {{citeDiss}}
}

@article{koren_bellkor_2009,
	title = {The bellkor solution to the netflix grand prize},
	url = {http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BellKor.pdf},
	journaltitle = {Netflix prize documentation},
	author = {Koren, Yehuda},
	urldate = {2014-05-05},
	date = {2009},
	keywords = {{citeDiss}},
	file = {[PDF] from osu.edu:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\8W56GMFB\\Koren - 2009 - The bellkor solution to the netflix grand prize.pdf:application/pdf}
}

@inproceedings{gruzd_evalutron_2007,
	location = {New York, {NY}, {USA}},
	title = {Evalutron 6000: Collecting Music Relevance Judgments},
	isbn = {978-1-59593-644-8},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1255175.1255307},
	doi = {10.1145/1255175.1255307},
	series = {{JCDL} '07},
	shorttitle = {Evalutron 6000},
	pages = {507--507},
	booktitle = {Proceedings of the 7th {ACM}/{IEEE}-{CS} Joint Conference on Digital Libraries},
	publisher = {{ACM}},
	author = {Gruzd, Anatoliy A. and Downie, J. Stephen and Jones, M. Cameron and Lee, Jin Ha},
	urldate = {2014-05-03},
	date = {2007},
	keywords = {{citeDiss}, {MIREX}, music digital libraries, music information retrieval, music similarity}
}

@inproceedings{urbano_audio_2011,
	title = {Audio Music Similarity and Retrieval: Evaluation Power and Stability.},
	url = {http://julian-urbano.info/wp-content/uploads/032-audio-music-similarity-retrieval-evaluation-power-stability.pdf},
	shorttitle = {Audio Music Similarity and Retrieval},
	pages = {597--602},
	booktitle = {{ISMIR}},
	author = {Urbano, Julián and Martín, Diego and Marrero, Mónica and Morato, Jorge},
	urldate = {2014-05-02},
	date = {2011},
	keywords = {{citeDiss}},
	file = {[PDF] from julian-urbano.info:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\JR6CEFGB\\Urbano et al. - 2011 - Audio Music Similarity and Retrieval Evaluation P.pdf:application/pdf}
}

@inproceedings{lee_crowdsourcing_2010,
	title = {Crowdsourcing Music Similarity Judgments using Mechanical Turk.},
	pages = {183--188},
	booktitle = {{ISMIR}},
	author = {Lee, Jin Ha},
	date = {2010},
	keywords = {{citeDiss}},
	file = {[PDF] from ismir.net:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\8ZB68KB4\\Lee - 2010 - Crowdsourcing Music Similarity Judgments using Mec.pdf:application/pdf}
}

@online{howe_birth_2006,
	title = {Birth of a Meme},
	url = {http://www.crowdsourcing.com/cs/2006/05/birth_of_a_meme.html},
	titleaddon = {Crowdsourcing},
	author = {Howe, Jeff},
	urldate = {2014-04-26},
	date = {2006-05-27},
	keywords = {{citeDiss}, {citeDissProp}}
}

@article{lamere_social_2008,
	title = {Social tagging and music information retrieval},
	volume = {37},
	url = {http://www.tandfonline.com.proxy2.library.illinois.edu/doi/abs/10.1080/09298210802479284},
	pages = {101--114},
	number = {2},
	journaltitle = {Journal of New Music Research},
	author = {Lamere, Paul},
	urldate = {2014-04-04},
	date = {2008},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {[PDF] from vigliensoni.com:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\UX77S4RS\\Lamere - 2008 - Social tagging and music information retrieval.pdf:application/pdf;Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\39XEFUJW\\09298210802479284.html:text/html}
}

@article{harris_applying_2012,
	title = {Applying human computation mechanisms to information retrieval},
	volume = {49},
	issn = {1550-8390},
	url = {http://onlinelibrary.wiley.com.proxy2.library.illinois.edu/doi/10.1002/meet.14504901050/abstract},
	doi = {10.1002/meet.14504901050},
	abstract = {Crowdsourcing and Games with a Purpose ({GWAP}) have each received considerable attention in recent years. These two human computation mechanisms assist with tasks that cannot be solved by computers alone. Despite this increased attention, much of this transformation has been limited to a few aspects of Information Retrieval ({IR}). In this paper, we examine these two mechanisms' applicability to {IR}. Using an {IR} model, we apply criteria to determine the suitability of these crowdsourcing and {GWAP} mechanisms to each step of the model. Our analysis illustrates that these mechanisms can apply to several of these steps with good returns.},
	pages = {1--10},
	number = {1},
	journaltitle = {Proceedings of the American Society for Information Science and Technology},
	shortjournal = {Proc. Am. Soc. Info. Sci. Tech.},
	author = {Harris, Christopher G. and Srinivasan, Padmini},
	urldate = {2014-03-25},
	date = {2012-01-01},
	langid = {english},
	keywords = {{citeDiss}, {citeDissProp}, crowdsourcing, human computation, Information Retrieval},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\ZGWQ5BM2\\abstract.html:text/html}
}

@book{kraut_building_2011,
	location = {Cambridge, {MA}},
	title = {Building Successful Online Communities},
	publisher = {{MIT} Press},
	author = {Kraut, Robert E. and Resnick, Paul},
	date = {2011},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14, crowdsourcing}
}

@article{ryan_intrinsic_2000,
	title = {Intrinsic and Extrinsic Motivations: Classic Definitions and New Directions},
	volume = {25},
	issn = {0361-476X},
	url = {http://www.sciencedirect.com/science/article/pii/S0361476X99910202},
	doi = {10.1006/ceps.1999.1020},
	shorttitle = {Intrinsic and Extrinsic Motivations},
	abstract = {Intrinsic and extrinsic types of motivation have been widely studied, and the distinction between them has shed important light on both developmental and educational practices. In this review we revisit the classic definitions of intrinsic and extrinsic motivation in light of contemporary research and theory. Intrinsic motivation remains an important construct, reflecting the natural human propensity to learn and assimilate. However, extrinsic motivation is argued to vary considerably in its relative autonomy and thus can either reflect external control or true self-regulation. The relations of both classes of motives to basic human needs for autonomy, competence and relatedness are discussed.},
	pages = {54--67},
	number = {1},
	journaltitle = {Contemporary Educational Psychology},
	shortjournal = {Contemporary Educational Psychology},
	author = {Ryan, Richard M. and Deci, Edward L.},
	urldate = {2014-03-19},
	date = {2000-01},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14, extrinsic motivation, Intrinsic motivation, motivation},
	file = {ScienceDirect Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\QUNSM9WG\\S0361476X99910202.html:text/html}
}

@article{zwass_cocreation_2010,
	title = {Co-Creation: Toward a Taxonomy and an Integrated Research Perspective},
	volume = {15},
	doi = {10.2753/JEC1086-4415150101},
	pages = {11--48},
	number = {1},
	journaltitle = {International Journal of Electronic Commerce},
	author = {Zwass, Vladamir},
	date = {2010},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14, cocreation, crowdsourcing, Taxonomy},
	file = {Zwass_2010_Co-Creation.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\International Journal of Electronic Commerce2010\\Zwass_2010_Co-Creation.pdf:application/pdf}
}

@inproceedings{schenk_crowdsourcing_2009,
	title = {Crowdsourcing: What can be Outsourced to the Crowd, and Why?},
	url = {http://raptor1.bizlab.mtsu.edu/s-drive/DMORRELL/Mgmt%204990/Crowdsourcing/Schenk%20and%20Guittard.pdf},
	shorttitle = {Crowdsourcing},
	booktitle = {Workshop on Open Source Innovation, Strasbourg, France},
	author = {Schenk, Eric and Guittard, Claude},
	urldate = {2014-01-30},
	date = {2009},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14, crowdsourcing, Taxonomy},
	file = {Schenk_Guittard_2009_Crowdsourcing.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\2009\\Schenk_Guittard_2009_Crowdsourcing.pdf:application/pdf}
}

@incollection{vukovic_towards_2010,
	title = {Towards a Research Agenda for Enterprise Crowdsourcing},
	rights = {Springer Berlin Heidelberg},
	isbn = {978-3-642-16557-3 978-3-642-16558-0},
	url = {http://link.springer.com.proxy2.library.illinois.edu/chapter/10.1007/978-3-642-16558-0_36},
	series = {Lecture Notes in Computer Science},
	abstract = {Over the past few years the crowdsourcing paradigm has evolved from its humble beginnings as isolated purpose-built initiatives, such as Wikipedia and Elance and Mechanical Turk to a growth industry employing over 2 million knowledge workers, contributing over half a billion dollars to the digital economy. Web 2.0 provides the technological foundations upon which the crowdsourcing paradigm evolves and operates, enabling networked experts to work collaboratively to complete a specific task. Enterprise crowdsourcing poses interesting challenges for both academic and industrial research along the social, legal, and technological dimensions. In this paper we describe the challenges that researchers and practitioners face when thinking about various aspects of enterprise crowdsourcing. First, to establish technological foundations, what are the interaction models and protocols between the Enterprise and the crowd. Secondly, how is crowdsourcing going to face the challenges in quality assurance, enabling Enterprises to optimally leverage the scalable workforce. Thirdly, what are the novel (Web) applications enabled by Enterprise crowdsourcing.},
	pages = {425--434},
	number = {6415},
	booktitle = {Leveraging Applications of Formal Methods, Verification, and Validation},
	publisher = {Springer Berlin Heidelberg},
	author = {Vukovic, Maja and Bartolini, Claudio},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	urldate = {2014-01-30},
	date = {2010-01-01},
	keywords = {business process modeling, {citeDiss}, {citeDissProp}, {citeiConf}14, Computer Communication Networks, crowdsourcing, Data Mining and Knowledge Discovery, Information Systems Applications (incl.Internet), Logics and Meanings of Programs, Programming Languages, Compilers, Interpreters, Software Engineering},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\A4VHV7IM\\978-3-642-16558-0_36.html:text/html}
}

@inproceedings{eickhoff_quality_2012,
	location = {New York, {NY}, {USA}},
	title = {Quality Through Flow and Immersion: Gamifying Crowdsourced Relevance Assessments},
	isbn = {978-1-4503-1472-5},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/2348283.2348400},
	doi = {10.1145/2348283.2348400},
	series = {{SIGIR} '12},
	shorttitle = {Quality Through Flow and Immersion},
	abstract = {Crowdsourcing is a market of steadily-growing importance upon which both academia and industry increasingly rely. However, this market appears to be inherently infested with a significant share of malicious workers who try to maximise their profits through cheating or sloppiness. This serves to undermine the very merits crowdsourcing has come to represent. Based on previous experience as well as psychological insights, we propose the use of a game in order to attract and retain a larger share of reliable workers to frequently-requested crowdsourcing tasks such as relevance assessments and clustering. In a large-scale comparative study conducted using recent {TREC} data, we investigate the performance of traditional {HIT} designs and a game-based alternative that is able to achieve high quality at significantly lower pay rates, facing fewer malicious submissions.},
	pages = {871--880},
	booktitle = {Proceedings of the 35th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Eickhoff, Carsten and Harris, Christopher G. and de Vries, Arjen P. and Srinivasan, Padmini},
	urldate = {2014-01-27},
	date = {2012},
	keywords = {{citeDiss}, {citeDissProp}, clustering, crowdsourcing, gamification, relevance assessments, serious games}
}

@incollection{alonso_design_2011,
	title = {Design and Implementation of Relevance Assessments Using Crowdsourcing},
	rights = {Springer Berlin Heidelberg},
	isbn = {978-3-642-20160-8 978-3-642-20161-5},
	url = {http://link.springer.com.proxy2.library.illinois.edu/chapter/10.1007/978-3-642-20161-5_16},
	series = {Lecture Notes in Computer Science},
	abstract = {In the last years crowdsourcing has emerged as a viable platform for conducting relevance assessments. The main reason behind this trend is that makes possible to conduct experiments extremely fast, with good results and at low cost. However, like in any experiment, there are several details that would make an experiment work or fail. To gather useful results, user interface guidelines, inter-agreement metrics, and justification analysis are important aspects of a successful crowdsourcing experiment. In this work we explore the design and execution of relevance judgments using Amazon Mechanical Turk as crowdsourcing platform, introducing a methodology for crowdsourcing relevance assessments and the results of a series of experiments using {TREC} 8 with a fixed budget. Our findings indicate that workers are as good as {TREC} experts, even providing detailed feedback for certain query-document pairs. We also explore the importance of document design and presentation when performing relevance assessment tasks. Finally, we show our methodology at work with several examples that are interesting in their own.},
	pages = {153--164},
	number = {6611},
	booktitle = {Advances in Information Retrieval},
	publisher = {Springer Berlin Heidelberg},
	author = {Alonso, Omar and Baeza-Yates, Ricardo},
	editor = {Clough, Paul and Foley, Colum and Gurrin, Cathal and Jones, Gareth J. F. and Kraaij, Wessel and Lee, Hyowon and Mudoch, Vanessa},
	urldate = {2014-01-27},
	date = {2011-01-01},
	keywords = {Artificial Intelligence (incl. Robotics), {citeDiss}, {citeDissProp}, Database Management, Data Mining and Knowledge Discovery, design, Information Storage and Retrieval, Information Systems Applications (incl.Internet), Multimedia Information Systems},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\A6MZIU6B\\978-3-642-20161-5_16.html:text/html}
}

@article{bell_bellkor_2008,
	title = {The {BellKor} 2008 Solution to the Netflix Prize},
	url = {ftp://140.118.199.9:2100/public9/2010_ALL/2010_TRU/2010Fall_Matlab-LABfiles/00_VECTOR-at-MATH/%E5%90%91%E9%87%8F%E7%9A%84%E5%88%86%E8%A7%A3/Public9/zTEMP/ZTemp2011/zMSIC-NSC101-DM/40--Dec27/Netflix-Articles-AWARD/(netflix)-ProgressPrize2008_BellKor.pdf},
	journaltitle = {Statistics Research Department at {AT}\&T Research},
	author = {Bell, Robert M. and Koren, Yehuda and Volinsky, Chris},
	urldate = {2014-01-12},
	date = {2008},
	keywords = {{citeDiss}, {citeDissProp}}
}

@inproceedings{organisciak_personalized_2013,
	location = {Palm Spring, {CA}},
	title = {Personalized Human Computation},
	eventtitle = {{HCOMP} 2013},
	author = {Organisciak, Peter and Teevan, Jaime and Dumais, Susan and Miller, Robert C. and Kalai, Adam Tauman},
	date = {2013},
	keywords = {{citeDiss}, {citeDissProp}}
}

@inreference{_bias_,
	title = {bias, adj., n., and adv.},
	url = {http://www.oed.com.proxy2.library.illinois.edu/view/Entry/18564},
	booktitle = {{OED} Online},
	publisher = {Oxford University Press},
	urldate = {2014-01-09},
	langid = {british},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {OED snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\WV62CVIG\\bias, adj., n., and adv..html:text/html}
}

@book{neuendorf_content_2002,
	location = {Thousand Oaks, {CA}, {USA}},
	title = {The Content Analysis Guidebook},
	pagetotal = {301},
	publisher = {Sage Publications},
	author = {Neuendorf, Kimberly A.},
	date = {2002},
	keywords = {{citeDiss}, {citeDissProp}}
}

@inproceedings{trant_investigating_2006,
	title = {Investigating social tagging and folksonomy in art museums with steve. museum},
	url = {http://www.ra.ethz.ch/cdstore/www2006/www.rawsugar.com/www2006/4.pdf},
	booktitle = {Proceedings of the {WWW}’06 Collaborative Web Tagging Workshop},
	author = {Trant, Jennifer and Wyman, Bruce},
	urldate = {2013-12-16},
	date = {2006},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Trant_Wyman_2006_Investigating social tagging and folksonomy in art museums with steve.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\2006\\Trant_Wyman_2006_Investigating social tagging and folksonomy in art museums with steve.pdf:application/pdf}
}

@inproceedings{organisciak_incidental_2013,
	location = {Lincoln, Nebraska},
	title = {Incidental Crowdsourcing: Crowdsourcing in the Periphery},
	url = {http://dh2013.unl.edu/abstracts/ab-273.html},
	abstract = {As the customs of the Internet grow increasingly collaborative, crowdsourcing offers an appealing frame for looking at the interaction of users with online systems and each other. However, it is a broad term that fails to emphasize the use of crowds in subtler system augmentation.

This paper introduces incidental crowdsourcing ({IC}): an approach to user-provided item description that adopts crowdsourcing as a frame for thinking about augmentative features of system design. {IC} is intended to frame discussion around peripheral and non-critical system design choices.

A provisional definition of incidental crowdsourcing will be defined in this paper, and then refined based on examples seen in practice. {IC} will be examined from both the user and system ends, positioned within existing work, and considered in the context of its benefits and drawbacks. This approach allows us to explore the robustness and feasibility of {IC}, looking at the implications inherent to accepting the provisional definition.

The consequences of considering system design on a scale between {IC} and non-{IC} design choices remain to be seen. Toward this goal, the second part of this paper shows a study comparing the participation habits of users in two online systems — one that is representative of {IC} properties and one that is not. This study finds differences in user engagement between the two systems.},
	eventtitle = {Digital Humanities 2013},
	author = {Organisciak, Peter},
	date = {2013-07-17},
	keywords = {{citeDiss}, {citeDissProp}}
}

@online{chen_improving_2013,
	title = {Improving Twitter search with real-time human computation},
	url = {https://blog.twitter.com/2013/improving-twitter-search-real-time-human-computation},
	abstract = {One of the magical things about Twitter is that it opens a window to the world in real-time. An event happens, and seconds later, people share it across the planet. Consider, for example, what happ......},
	titleaddon = {Twitter Engineering Blog},
	author = {Chen, Edwin and Jain, Alpa},
	urldate = {2013-12-09},
	date = {2013-01-08},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\NWJQ7G5V\\improving-twitter-search-real-time-human-computation.html:text/html}
}

@inproceedings{efron_hashtag_2010,
	location = {New York, {NY}, {USA}},
	title = {Hashtag Retrieval in a Microblogging Environment},
	isbn = {978-1-4503-0153-4},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1835449.1835616},
	doi = {10.1145/1835449.1835616},
	series = {{SIGIR} '10},
	abstract = {Microblog services let users broadcast brief textual messages to people who "follow" their activity. Often these posts contain terms called hashtags, markers of a post's meaning, audience, etc. This poster treats the following problem: given a user's stated topical interest, retrieve useful hashtags from microblog posts. Our premise is that a user interested in topic x might like to find hashtags that are often applied to posts about x. This poster proposes a language modeling approach to hashtag retrieval. The main contribution is a novel method of relevance feedback based on hashtags. The approach is tested on a corpus of data harvested from twitter.com.},
	pages = {787--788},
	booktitle = {Proceedings of the 33rd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Efron, Miles},
	urldate = {2013-12-08},
	date = {2010},
	keywords = {{citeDiss}, {citeDissProp}, hashtag, microblog, relevance feedback, twitter}
}

@inproceedings{shiells_generating_2010,
	location = {New York, {NY}, {USA}},
	title = {Generating Document Summaries from User Annotations},
	isbn = {978-1-4503-0372-9},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1871962.1871978},
	doi = {10.1145/1871962.1871978},
	series = {{ESAIR} '10},
	abstract = {In this paper we analyze tweets that share the same link as another form of social annotation. By extracting all tweets that contain the same link over a period of time, we can generate a summary of what the crowd is writing about that particular link.},
	pages = {25--26},
	booktitle = {Proceedings of the Third Workshop on Exploiting Semantic Annotations in Information Retrieval},
	publisher = {{ACM}},
	author = {Shiells, Karen and Alonso, Omar and Lee, Ho John},
	urldate = {2013-12-08},
	date = {2010},
	keywords = {{citeDiss}, {citeDissProp}, crowdsourcing, social search, summarization, twitter}
}

@inproceedings{finin_annotating_2010,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Annotating Named Entities in Twitter Data with Crowdsourcing},
	url = {http://dl.acm.org/citation.cfm?id=1866696.1866709},
	series = {{CSLDAMT} '10},
	abstract = {We describe our experience using both Amazon Mechanical Turk ({MTurk}) and Crowd-Flower to collect simple named entity annotations for Twitter status updates. Unlike most genres that have traditionally been the focus of named entity experiments, Twitter is far more informal and abbreviated. The collected annotations and annotation techniques will provide a first step towards the full study of named entity recognition in domains like Facebook and Twitter. We also briefly describe how to use {MTurk} to collect judgements on the quality of "word clouds."},
	pages = {80--88},
	booktitle = {Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
	publisher = {Association for Computational Linguistics},
	author = {Finin, Tim and Murnane, Will and Karandikar, Anand and Keller, Nicholas and Martineau, Justin and Dredze, Mark},
	urldate = {2013-12-07},
	date = {2010},
	keywords = {{citeDiss}, {citeDissProp}, crowdsourcing, mechanical turk, paid crowdsourcing, twitter}
}

@inproceedings{organisciak_evaluating_2012,
	location = {Baltimore, {MD}},
	title = {Evaluating rater quality and rating difficulty in online annotation activities},
	volume = {49},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/meet.14504901166/abstract},
	doi = {10.1002/meet.14504901166},
	series = {{ASIS}\&T '12},
	abstract = {Gathering annotations from non-expert online raters is an attractive method for quickly completing large-scale annotation tasks, but the increased possibility of unreliable annotators and diminished work quality remains a cause for concern. In the context of information retrieval, where human-encoded relevance judgments underlie the evaluation of new systems and methods, the ability to quickly and reliably collect trustworthy annotations allows for quicker development and iteration of research.In the context of paid online workers, this study evaluates indicators of non-expert performance along three lines: temporality, experience, and agreement. It is found that user performance is a key indicator for future performance. Additionally, the time spent by raters familiarizing themselves with a new set of tasks is important for rater quality, as is long-term familiarity with a topic being rated.These findings may inform large-scale digital collections' use of non-expert raters for performing more purposive and affordable online annotation activities.},
	eventtitle = {{ASIS}\&T},
	pages = {1--10},
	booktitle = {Proceedings of the American Society for Information Science and Technology},
	author = {Organisciak, Peter and Efron, Miles and Fenlon, Katrina and Senseney, Megan},
	urldate = {2013-11-23},
	date = {2012},
	langid = {english},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\KMHITP8J\\abstract\;jsessionid=D2B4003B525D768D514C414B7C85E315.html:text/html}
}

@article{galton_vox_1907,
	title = {Vox populi},
	volume = {75},
	url = {http://adsabs.harvard.edu/abs/1907Natur..75..450G},
	pages = {450--451},
	journaltitle = {Nature},
	author = {Galton, Francis},
	urldate = {2013-10-22},
	date = {1907},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\AJHAXVIM\\1907Natur..75..html:text/html}
}

@article{law_human_2011,
	title = {Human Computation},
	volume = {5},
	issn = {1939-4608, 1939-4616},
	url = {http://www.morganclaypool.com.proxy2.library.illinois.edu/doi/abs/10.2200/S00371ED1V01Y201107AIM013},
	doi = {10.2200/S00371ED1V01Y201107AIM013},
	pages = {1--121},
	number = {3},
	journaltitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Law, Edith and Ahn, Luis von},
	urldate = {2013-09-18},
	date = {2011-06-30},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Morgan & Claypool Publishers - Synthesis Lectures on Artificial Intelligence and Machine Learning - 5(3)\:1 - Abstract:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\BTJCHEXW\\S00371ED1V01Y201107AIM013.html:text/html;Morgan & Claypool Publishers - Synthesis Lectures on Artificial Intelligence and Machine Learning - 5(3)\:1 - Abstract:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\CCKE2ZTG\\S00371ED1V01Y201107AIM013.html:text/html}
}

@inproceedings{springer_for_2008,
	title = {For the common good: The Library of Congress Flickr pilot project},
	url = {http://www.loc.gov/rr/print/flickr_report_final.pdf},
	shorttitle = {For the common good},
	author = {Springer, Michelle and Dulabahn, Beth and Michel, Phil and Natanson, Barbara and Reser, David W. and Ellison, Nicole B. and Zinkham, Helena and Woodward, David},
	urldate = {2013-08-26},
	date = {2008},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14},
	file = {[PDF] from loc.gov:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\ESPKMZX7\\Prints et al. - 2008 - For the common good The Library of Congress Flick.pdf:application/pdf;Springer et al_2008_For the common good.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\2008\\Springer et al_2008_For the common good.pdf:application/pdf}
}

@article{causer_transcription_2012,
	title = {Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham},
	volume = {27},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/27/2/119},
	doi = {10.1093/llc/fqs004},
	shorttitle = {Transcription maximized; expense minimized?},
	abstract = {This article discusses the crowdsourced manuscript transcription project Transcribe Bentham, and how it will impact upon long-established editorial practices at the Bentham Project, University College London, which is producing the new and authoritative edition of The Collected Works of Jeremy Bentham. We site Transcribe Bentham in the burgeoning field of scholarly crowdsourcing projects, and, by detailing our experiences of running and administering the project, attempt to assess the potential benefits of engaging the public in humanities research. The article examines the conceptualization and development of Transcribe Bentham, and how editorial practices at the Bentham Project may change as a result. We account for the design of the bespoke transcription tool which is at the project's heart, and which allows volunteers to transcribe the material and encode it in {TEI}-compliant {XML}. We attempt to answer five key questions: is crowdsourcing the transcription of complex manuscripts cost-effective? Is crowdsourcing exploitative? Are volunteer-produced transcripts of sufficient quality for editorial use and uploading to a digital repository, and what quality controls are required? Does crowdsourcing ensure sustainability and widen access to this priceless material? And finally, should the success of a project like Transcribe Bentham be measured solely according to cost-effectiveness or the volume of work produced, or do considerations of public engagement and access outweigh such concerns?},
	pages = {119--137},
	number = {2},
	journaltitle = {Literary and Linguistic Computing},
	shortjournal = {Lit Linguist Computing},
	author = {Causer, Tim and Tonra, Justin and Wallace, Valerie},
	urldate = {2013-08-26},
	date = {2012-06-01},
	langid = {english},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\SI2TC3DD\\119.html:text/html}
}

@book{raymond_cathedral_1999,
	title = {The Cathedral and the Bazaar},
	abstract = {I anatomize a successful open-source project, fetchmail, that was run as a deliberate test of the surprising theories about software engineering suggested by the history of Linux. I discuss these theories in terms of two fundamentally different development styles, the ``cathedral'' model of most of the commercial world versus the ``bazaar'' model of the Linux world. I show that these models derive from opposing assumptions about the nature of the software-debugging task. I then make a sustained argument from the Linux experience for the proposition that ``Given enough eyeballs, all bugs are shallow'', suggest productive analogies with other self-correcting systems of selfish agents, and conclude with some exploration of the implications of this insight for the future of software.},
	pagetotal = {241},
	publisher = {O'Reilly Media},
	author = {Raymond, Eric S.},
	date = {1999},
	keywords = {{citeDiss}, {citeDissProp}}
}

@article{efron_information_2011,
	title = {Information search and retrieval in microblogs},
	volume = {62},
	issn = {1532-2890},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/asi.21512/abstract},
	doi = {10.1002/asi.21512},
	abstract = {Modern information retrieval ({IR}) has come to terms with numerous new media in efforts to help people find information in increasingly diverse settings. Among these new media are so-called microblogs. A microblog is a stream of text that is written by an author over time. It comprises many very brief updates that are presented to the microblog's readers in reverse-chronological order. Today, the service called Twitter is the most popular microblogging platform. Although microblogging is increasingly popular, methods for organizing and providing access to microblog data are still new. This review offers an introduction to the problems that face researchers and developers of {IR} systems in microblog settings. After an overview of microblogs and the behavior surrounding them, the review describes established problems in microblog retrieval, such as entity search and sentiment analysis, and modeling abstractions, such as authority and quality. The review also treats user-created metadata that often appear in microblogs. Because the problem of microblog search is so new, the review concludes with a discussion of particularly pressing research issues yet to be studied in the field.},
	pages = {996--1008},
	number = {6},
	journaltitle = {Journal of the American Society for Information Science and Technology},
	author = {Efron, Miles},
	urldate = {2013-06-14},
	date = {2011},
	langid = {english},
	keywords = {{citeDiss}, {citeDissProp}}
}

@inproceedings{bigham_vizwiz_2010,
	location = {New York, {NY}, {USA}},
	title = {{VizWiz}: nearly real-time answers to visual questions},
	isbn = {978-1-4503-0271-5},
	url = {http://doi.acm.org/10.1145/1866029.1866080},
	doi = {10.1145/1866029.1866080},
	series = {{UIST} '10},
	shorttitle = {{VizWiz}},
	abstract = {The lack of access to visual information like text labels, icons, and colors can cause frustration and decrease independence for blind people. Current access technology uses automatic approaches to address some problems in this space, but the technology is error-prone, limited in scope, and quite expensive. In this paper, we introduce {VizWiz}, a talking application for mobile phones that offers a new alternative to answering visual questions in nearly real-time - asking multiple people on the web. To support answering questions quickly, we introduce a general approach for intelligently recruiting human workers in advance called {quikTurkit} so that workers are available when new questions arrive. A field deployment with 11 blind participants illustrates that blind people can effectively use {VizWiz} to cheaply answer questions in their everyday lives, highlighting issues that automatic approaches will need to address to be useful. Finally, we illustrate the potential of using {VizWiz} as part of the participatory design of advanced tools by using it to build and evaluate {VizWiz}::{LocateIt}, an interactive mobile tool that helps blind people solve general visual search problems.},
	pages = {333--342},
	booktitle = {Proceedings of the 23nd annual {ACM} symposium on User interface software and technology},
	publisher = {{ACM}},
	author = {Bigham, Jeffrey P. and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C. and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and Yeh, Tom},
	urldate = {2013-06-14},
	date = {2010},
	keywords = {blind users, {citeDiss}, non-visual interfaces, real-time human computation}
}

@inproceedings{geiger_managing_2011,
	title = {Managing the crowd: towards a taxonomy of crowdsourcing processes},
	url = {http://schader.bwl.uni-mannheim.de/fileadmin/files/schader/files/publikationen/Geiger_et_al._-_2011_-_Managing_the_Crowd_Towards_a_Taxonomy_of_Crowdsourcing_Processes.pdf},
	shorttitle = {Managing the crowd},
	pages = {1--15},
	booktitle = {Proceedings of the seventeenth Americas conference on information systems, Detroit, Michigan},
	author = {Geiger, David and Seedorf, Stefan and Schulze, Thimo and Nickerson, Robert and Schader, Martin},
	urldate = {2013-05-24},
	date = {2011},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14},
	file = {Geiger-et-al_2011_Managing the crowd.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\2011\\Geiger-et-al_2011_Managing the crowd.pdf:application/pdf}
}

@online{wales_insist_2006,
	title = {Insist on Sources},
	url = {http://lists.wikimedia.org/pipermail/wikien-l/2006-July/050773.html},
	titleaddon = {{WikiEN}-l},
	author = {Wales, Jimmy},
	date = {2006-07-19},
	keywords = {{citeDiss}, {citeDissProp}}
}

@report{page_pagerank_1999,
	title = {The {PageRank} Citation Ranking: Bringing Order to the Web.},
	url = {http://ilpubs.stanford.edu:8090/422/},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes {PageRank}, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare {PageRank} to an idealized random Web surfer. We show how to efficiently compute {PageRank} for large numbers of pages. And, we show how to apply {PageRank} to search and to user navigation.},
	number = {1999-66},
	institution = {Stanford {InfoLab}},
	type = {Technical Report},
	author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
	date = {1999-11},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14}
}

@inproceedings{wei_lda-based_2006,
	location = {New York, {NY}, {USA}},
	title = {{LDA}-based document models for ad-hoc retrieval},
	isbn = {1-59593-369-7},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1148170.1148204},
	doi = {10.1145/1148170.1148204},
	series = {{SIGIR} '06},
	abstract = {Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation ({LDA}), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use {LDA} to improve ad-hoc retrieval. We propose an {LDA}-based document model within the language modeling framework, and evaluate it on several {TREC} collections. Gibbs sampling is employed to conduct approximate inference in {LDA} and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.},
	pages = {178--185},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Wei, Xing and Croft, W. Bruce},
	urldate = {2013-02-11},
	date = {2006},
	keywords = {{citeDiss}, document model, {INFORMATION} retrieval, language model, latent dirichlet allocation ({LDA}), topic model},
	file = {ACM Full Text PDF:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\DCZNMFPD\\Wei and Croft - 2006 - LDA-based document models for ad-hoc retrieval.pdf:application/pdf}
}

@inproceedings{liu_cluster-based_2004,
	location = {New York, {NY}, {USA}},
	title = {Cluster-based retrieval using language models},
	isbn = {1-58113-881-4},
	url = {http://doi.acm.org/10.1145/1008992.1009026},
	doi = {10.1145/1008992.1009026},
	series = {{SIGIR} '04},
	abstract = {Previous research on cluster-based retrieval has been inconclusive as to whether it does bring improved retrieval effectiveness over document-based retrieval. Recent developments in the language modeling approach to {IR} have motivated us to re-examine this problem within this new retrieval framework. We propose two new models for cluster-based retrieval and evaluate them on several {TREC} collections. We show that cluster-based retrieval can perform consistently across collections of realistic size, and significant improvements over document-based retrieval can be obtained in a fully automatic manner and without relevance information provided by human.},
	pages = {186--193},
	booktitle = {Proceedings of the 27th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Liu, Xiaoyong and Croft, W. Bruce},
	urldate = {2013-02-04},
	date = {2004},
	keywords = {{citeDiss}, {citeDissProp}, cluster-based language model, cluster-based retrieval, cluster model, hierarchical clustering, {INFORMATION} retrieval, language model, query-specific clustering, smoothing, static clustering, topic model}
}

@article{holley_crowdsourcing_2010,
	title = {Crowdsourcing: How and Why Should Libraries Do It?},
	volume = {16},
	issn = {1082-9873},
	url = {http://www.dlib.org/dlib/march10/holley/03holley.html},
	doi = {10.1045/march2010-holley},
	shorttitle = {Crowdsourcing},
	number = {3},
	journaltitle = {D-Lib Magazine},
	author = {Holley, Rose},
	urldate = {2012-12-03},
	date = {2010-03},
	keywords = {{citeDiss}, {citeDissProp}}
}

@inproceedings{zhai_study_2001,
	location = {New York, {NY}, {USA}},
	title = {A study of smoothing methods for language models applied to Ad Hoc information retrieval},
	isbn = {1-58113-331-6},
	url = {http://doi.acm.org/10.1145/383952.384019},
	doi = {10.1145/383952.384019},
	series = {{SIGIR} '01},
	abstract = {Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model. A core problem in language model estimation is smoothing, which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. In this paper, we study the problem of language model smoothing and its influence on retrieval performance.  We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections.},
	pages = {334--342},
	booktitle = {Proceedings of the 24th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Zhai, Chengxiang and Lafferty, John},
	urldate = {2012-09-20},
	date = {2001},
	keywords = {basic concepts, {citeDiss}, {citeDissProp}}
}

@article{alonso_crowdsourcing_2008,
	title = {Crowdsourcing for relevance evaluation},
	volume = {42},
	issn = {0163-5840},
	url = {http://doi.acm.org/10.1145/1480506.1480508},
	doi = {10.1145/1480506.1480508},
	abstract = {Relevance evaluation is an essential part of the development and maintenance of information retrieval systems. Yet traditional evaluation approaches have several limitations; in particular, conducting new editorial evaluations of a search system can be very expensive. We describe a new approach to evaluation called {TERC}, based on the crowdsourcing paradigm, in which many online users, drawn from a large community, each performs a small evaluation task.},
	pages = {9--15},
	number = {2},
	journaltitle = {{SIGIR} Forum},
	author = {Alonso, Omar and Rose, Daniel E. and Stewart, Benjamin},
	urldate = {2012-09-05},
	date = {2008-11},
	keywords = {candidates, {citeDiss}, {citeDissProp}, crowdsourcing, humans in {IR}, Miles suggestions}
}

@article{mason_financial_2010,
	title = {Financial incentives and the "performance of crowds"},
	volume = {11},
	issn = {1931-0145},
	url = {http://doi.acm.org/10.1145/1809400.1809422},
	doi = {10.1145/1809400.1809422},
	abstract = {The relationship between financial incentives and performance, long of interest to social scientists, has gained new relevance with the advent of web-based "crowd-sourcing" models of production. Here we investigate the effect of compensation on performance in the context of two experiments, conducted on Amazon's Mechanical Turk ({AMT}). We find that increased financial incentives increase the quantity, but not the quality, of work performed by participants, where the difference appears to be due to an "anchoring" effect: workers who were paid more also perceived the value of their work to be greater, and thus were no more motivated than workers paid less. In contrast with compensation levels, we find the details of the compensation scheme do matter--specifically, a "quota" system results in better work for less pay than an equivalent "piece rate" system. Although counterintuitive, these findings are consistent with previous laboratory studies, and may have real-world analogs as well.},
	pages = {100--108},
	number = {2},
	journaltitle = {{SIGKDD} Explor. Newsl.},
	author = {Mason, Winter and Watts, Duncan J.},
	urldate = {2012-06-13},
	date = {2010-05},
	keywords = {candidates, {citeDiss}, {citeDissProp}, {citeiConf}14, crowd-sourcing, crowdsourcing, extrinsic motivation, hcir, humans in {IR}, incentives, Intrinsic motivation, mechanical turk, peer production, performance}
}

@report{urbano_university_2011,
	title = {The University Carlos {III} of Madrid  at {TREC} 2011 Crowdsourcing Track},
	url = {http://julian-urbano.info/wp-content/uploads/035-university-carlos-iii-madrid-trec-2011-crowdsourcing-track.pdf},
	author = {Urbano, Julián and Marrero, Mónica and Martín, Diego and Morato, Jorge and Robles, Karina and Lloréns, Juan},
	date = {2011},
	keywords = {{citeDiss}, {hcirCITE}}
}

@inproceedings{lease_overview_2011,
	title = {Overview of the {TREC} 2011 Crowdsourcing Track (Conference Notebook)},
	booktitle = {Text Retrieval Conference Notebook},
	author = {Lease, Matthew and Kazai, Gabriella},
	date = {2011},
	keywords = {{citeDiss}, {citeDissProp}, {hcirCITE}, {TREC} crowd}
}

@inproceedings{hsueh_data_2009,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Data quality from crowdsourcing: a study of annotation selection criteria},
	url = {http://dl.acm.org.proxy2.library.illinois.edu/citation.cfm?id=1564131.1564137},
	series = {{HLT} '09},
	shorttitle = {Data quality from crowdsourcing},
	abstract = {Annotation acquisition is an essential step in training supervised classifiers. However, manual annotation is often time-consuming and expensive. The possibility of recruiting annotators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates. In this paper, we consider the difficult problem of classifying sentiment in political blog snippets. Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined. Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty. Analysis confirm the utility of these criteria on improving data quality. We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency.},
	pages = {27--35},
	booktitle = {Proceedings of the {NAACL} {HLT} 2009 Workshop on Active Learning for Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Hsueh, Pei-Yun and Melville, Prem and Sindhwani, Vikas},
	urldate = {2012-04-23},
	date = {2009},
	keywords = {{citeDiss}, {hcirCITE}}
}

@inproceedings{donmez_probabilistic_2010,
	title = {A probabilistic framework to learn from multiple annotators with time-varying accuracy},
	abstract = {This paper addresses the challenging problem of learning from multiple annotators whose labeling accuracy (reliability) diﬀers and varies over time. We propose a framework based on Sequential Bayesian Estimation to learn the expected accuracy at each time step while simultaneously deciding which annotators to query for a label in an incremental learning framework. We develop a variant of the particle ﬁltering method that estimates the expected accuracy at every time step by sets of weighted samples and performs sequential Bayes updates. The estimated expected accuracies are then used to decide which annotators to be queried at the next time step. The empirical analysis shows that the proposed method is very eﬀective at predicting the true label using only moderate labeling eﬀorts, resulting in cleaner labels to train classiﬁers. The proposed method signiﬁcantly outperforms a repeated labeling baseline which queries all labelers per example and takes the majority vote to predict the true label. Moreover, our method is able to track the true accuracy of an annotator quite well in the absence of gold standard labels. These results demonstrate the strength of the proposed method in terms of estimating the time-varying reliability of multiple annotators and producing cleaner, better quality labels without extensive label queries.},
	pages = {826--837},
	booktitle = {{SIAM} International Conference on Data Mining ({SDM})},
	author = {Donmez, P. and Carbonell, J. and Schneider, J.},
	date = {2010},
	keywords = {{citeDiss}, {hcirCITE}, {hcirMidtermCITE}},
	file = {Google Scholar Linked Page:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\SZBMQBWD\\Donmez et al. - 2010 - A probabilistic framework to learn from multiple a.pdf:application/pdf}
}

@inproceedings{ipeirotis_quality_2010,
	location = {New York, {NY}, {USA}},
	title = {Quality management on Amazon Mechanical Turk},
	isbn = {978-1-4503-0222-7},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1837885.1837906},
	doi = {10.1145/1837885.1837906},
	series = {{HCOMP} '10},
	pages = {64--67},
	booktitle = {Proceedings of the {ACM} {SIGKDD} Workshop on Human Computation},
	publisher = {{ACM}},
	author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
	urldate = {2012-02-19},
	date = {2010},
	keywords = {{citeDiss}, {citeDissProp}}
}

@inproceedings{dekel_vox_2009,
	title = {Vox Populi: Collecting High-Quality Labels from a Crowd},
	abstract = {With the emergence of search engines and crowdsourcing
 websites, machine learning practitioners are faced with datasets that are labeled by a large heterogeneous set of teachers. These datasets test the limits of our existing learning theory, which largely assumes that data is sampled i.i.d. from a fixed distribution. In many cases, the number of teachers actually scales with the number of examples, with each teacher providing just a handful of labels, precluding any statistically reliable assessment of an individual teacher’s quality. In this paper, we study the problem of pruning low-quality teachers in a crowd, in order to improve the label quality of our training set. Despite the hurdles mentioned above, we show that this is in fact achievable with a simple and efficient algorithm, which does not require that each example be repeatedly labeled by multiple teachers. We provide a theoretical analysis of our algorithm and back our findings with empirical evidence.},
	eventtitle = {{COLT} 2009},
	author = {Dekel, Ofer and Shamir, Ohad},
	date = {2009-06-24},
	keywords = {{citeDiss}, {hcirCITE}}
}

@inproceedings{raykar_supervised_2009,
	location = {New York, {NY}, {USA}},
	title = {Supervised learning from multiple experts: whom to trust when everyone lies a bit},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1553374.1553488},
	doi = {10.1145/1553374.1553488},
	series = {{ICML} '09},
	shorttitle = {Supervised learning from multiple experts},
	pages = {889--896},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Raykar, Vikas C. and Yu, Shipeng and Zhao, Linda H. and Jerebko, Anna and Florin, Charles and Valadez, Gerardo Hermosillo and Bogoni, Luca and Moy, Linda},
	urldate = {2012-02-19},
	date = {2009},
	keywords = {{citeDiss}, {citeDissProp}}
}

@article{eickhoff_increasing_2012,
	title = {Increasing cheat robustness of crowdsourcing tasks},
	issn = {1386-4564, 1573-7659},
	url = {http://www.springerlink.com/content/70807017421n1462/},
	doi = {10.1007/s10791-011-9181-9},
	abstract = {Crowdsourcing successfully strives to become a widely used means of collecting large-scale scientific corpora. Many research fields, including Information Retrieval, rely on this novel way of data acquisition. However, it seems to be undermined by a significant share of workers that are primarily interested in producing quick generic answers rather than correct ones in order to optimise their time-efficiency and, in turn, earn more money. Recently, we have seen numerous sophisticated schemes of identifying such workers. Those, however, often require additional resources or introduce artificial limitations to the task. In this work, we take a different approach by investigating means of a priori making crowdsourced tasks more resistant against cheaters.},
	journaltitle = {Information Retrieval},
	author = {Eickhoff, Carsten and Vries, Arjen P.},
	urldate = {2012-02-19},
	date = {2012-02-14},
	keywords = {{citeDiss}, {citeDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	file = {Eickhoff and Vries - 2012 - Increasing cheat robustness of crowdsourcing tasks.html:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\PM8NP24R\\Eickhoff and Vries - 2012 - Increasing cheat robustness of crowdsourcing tasks.html:text/html}
}

@inproceedings{wallace_who_2011,
	title = {Who should label what? Instance allocation in multiple expert active learning},
	shorttitle = {Who should label what?},
	booktitle = {Proceedings of the {SIAM} International Conference on Data Mining ({SDM})},
	author = {Wallace, B. and Small, K. and Brodley, C. and Trikalinos, T.},
	date = {2011},
	keywords = {{citeDiss}, {citeDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	file = {Google Scholar Linked Page:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\R4E4ACD6\\Wallace et al. - 2011 - Who should label what Instance allocation in mult.pdf:application/pdf}
}

@inproceedings{sheng_get_2008,
	location = {New York, {NY}, {USA}},
	title = {Get another label? improving data quality and data mining using multiple, noisy labelers},
	isbn = {978-1-60558-193-4},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1401890.1401965},
	doi = {10.1145/1401890.1401965},
	series = {{KDD} '08},
	shorttitle = {Get another label?},
	pages = {614--622},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {{ACM}},
	author = {Sheng, Victor S. and Provost, Foster and Ipeirotis, Panagiotis G.},
	urldate = {2012-02-19},
	date = {2008},
	keywords = {{citeDiss}, {citeDissProp}, data preprocessing, data selection, {hcirCITE}}
}

@inproceedings{welinder_multidimensional_2010,
	title = {The multidimensional wisdom of crowds},
	volume = {6},
	pages = {8},
	booktitle = {Neural Information Processing Systems Conference ({NIPS})},
	author = {Welinder, P. and Branson, S. and Belongie, S. and Perona, P.},
	date = {2010},
	keywords = {{citeDiss}},
	file = {Google Scholar Linked Page:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\E6SSEEP6\\Welinder et al. - 2010 - The multidimensional wisdom of crowds.pdf:application/pdf}
}

@inproceedings{welinder_online_2010,
	title = {Online crowdsourcing: Rating annotators and obtaining cost-effective labels},
	isbn = {978-1-4244-7029-7},
	doi = {10.1109/CVPRW.2010.5543189},
	shorttitle = {Online crowdsourcing},
	abstract = {Labeling large datasets has become faster, cheaper, and easier with the advent of crowdsourcing services like Amazon Mechanical Turk. How can one trust the labels obtained from such services? We propose a model of the labeling process which includes label uncertainty, as well a multi-dimensional measure of the annotators' ability. From the model we derive an online algorithm that estimates the most likely value of the labels and the annotator abilities. It finds and prioritizes experts when requesting labels, and actively excludes unreliable annotators. Based on labels already obtained, it dynamically chooses which images will be labeled next, and how many labels to request in order to achieve a desired level of confidence. Our algorithm is general and can handle binary, multi-valued, and continuous annotations (e.g. bounding boxes). Experiments on a dataset containing more than 50,000 labels show that our algorithm reduces the number of labels required, and thus the total cost of labeling, by a large factor while keeping error rates low on a variety of datasets.},
	eventtitle = {2010 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	pages = {25--32},
	booktitle = {2010 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	publisher = {{IEEE}},
	author = {Welinder, P. and Perona, P.},
	date = {2010-06-13},
	keywords = {Adaptation model, amazon mechanical turk, {citeDiss}, {citeDissProp}, Computer vision, cost effective label, costing, Costs, dataset labeling service, Error analysis, {hcirCITE}, {hcirMidtermCITE}, image classification, Labeling, label uncertainty, multidimensional annotator ability measurement, multivalued continuous annotation, Noise figure, online crowdsourcing, Outsourcing, rating annotator, Web services}
}

@inproceedings{grady_crowdsourcing_2010,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Crowdsourcing document relevance assessment with Mechanical Turk},
	url = {http://dl.acm.org.proxy2.library.illinois.edu/citation.cfm?id=1866696.1866723},
	series = {{CSLDAMT} '10},
	pages = {172--179},
	booktitle = {Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
	publisher = {Association for Computational Linguistics},
	author = {Grady, Catherine and Lease, Matthew},
	urldate = {2012-02-19},
	date = {2010},
	keywords = {{citeDiss}, {citeDissProp}}
}

@article{whitehill_whose_2009,
	title = {Whose vote should count more: Optimal integration of labels from labelers of unknown expertise},
	volume = {22},
	shorttitle = {Whose vote should count more},
	pages = {2035--2043},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Whitehill, J. and Ruvolo, P. and Wu, T. and Bergsma, J. and Movellan, J.},
	date = {2009},
	keywords = {{citeDiss}, {citeDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	file = {Google Scholar Linked Page:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\PV9EWG5K\\Whitehill et al. - 2009 - Whose vote should count more Optimal integration .pdf:application/pdf}
}

@article{moyle_manuscript_2010,
	title = {Manuscript transcription by crowdsourcing: Transcribe Bentham},
	volume = {20},
	url = {http://liber.library.uu.nl/publish/issues/2010-3_4/index.html?000514},
	shorttitle = {Manuscript transcription by crowdsourcing},
	abstract = {Transcribe Bentham is testing the feasibility of outsourcing the work of manuscript transcription to members of the public.  {UCL} Library Services holds 60,000 folios of manuscripts of the philosopher and jurist Jeremy Bentham (1748-1832).  Transcribe Bentham will digitise 12,500 Bentham folios, and, through a wiki-based interface, allow volunteer transcribers to take temporary ownership of manuscript images and to create {TEI}-encoded transcription text for final approval by {UCL} experts.  Approved transcripts will be stored and preserved, with the manuscript images, in {UCL}'s public Digital Collections repository.  

The project makes innovative use of traditional Library material. It will stimulate public engagement with {UCL}'s scholarly archive collections and the challenges of palaeography and manuscript transcription; it will raise the profile of the work and thought of Jeremy Bentham; and it will create new digital resources for future use by  professional researchers.  Towards the end of the project, the transcription tool will be made available to other projects and services.},
	number = {3},
	journaltitle = {{LIBER} Quarterly},
	author = {Moyle, M. and Tonra, J. and Wallace, V.},
	urldate = {2012-02-01},
	date = {2010},
	keywords = {{citeDiss}, {citeDissProp}, crowdsourcing, digital humanities, {swiftCite}},
	file = {Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\TDIV9BMK\\20474.html:text/html}
}

@article{golder_structure_2007,
	title = {The Structure of Collaborative Tagging Systems},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.133.9217},
	author = {Golder, Scott A and Huberman, Bernardo A},
	date = {2007},
	keywords = {{citeDiss}},
	file = {Golder_Huberman_2007_The Structure of Collaborative Tagging Systems.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\2007\\Golder_Huberman_2007_The Structure of Collaborative Tagging Systems.pdf:application/pdf}
}

@report{holley_many_2009,
	title = {Many Hands Make Light Work: Public Collaborative {OCR} Text Correction in Australian Historic Newspapers},
	url = {http://www-prod.nla.gov.au/openpublish/index.php/nlasp/article/viewArticle/1406},
	abstract = {The {ANDP} team had from the outset in January 2007 decided to make a considerable investment in software development in order to be able to quality assure digital outputs to ensure they met minimum standards and to future proof the files in case they could be improved further in the future.

Contributors had all expressed concern that the digital outputs (image quality, {OCR} text) may not be good enough to enable adequate full text retrieval or to meet user expectations. The {ANDP} team had regularly brainstormed and reviewed ideas to improve the quality of outputs and had implemented a number of ideas to achieve this3. At this stage the team were assuming that quality of data entirely relied on the Library’s digitisation technique. It had not yet occurred to the team or the contributors that the public may play a role in improving and enhancing the quality of the data.},
	institution = {National Library of Australia},
	author = {Holley, Rose},
	date = {2009},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14, {ICcited}, sorted}
}

@inproceedings{spiteri_social_2011,
	title = {Social discovery tools: Cataloguing meets user convenience},
	volume = {3},
	url = {http://journals.lib.washington.edu/index.php/nasko/article/view/12790},
	abstract = {This paper examines how library users access, use, and interact with two social discovery systems used in two Canadian public library systems. How do public library users interact with social discovery systems? How does usage between the two social discovery systems compare? Daily transaction logs of the social discovery systems used by the two libraries were compiled from May-August, 2010. Fifty sets of bibliographic records were compared to evaluate user-contributed content. Results indicate that features that allow for user-generated content are underused in both systems. Future research will thus focus on clients' motivations for engaging with the social features of social discovery systems, and their perceptions of, and satisfaction with, the benefits of these features.},
	booktitle = {Proceedings from North American Symposium on Knowledge Organization},
	author = {Spiteri, Louise F.},
	date = {2011},
	keywords = {bibliocommons, {citeDiss}, {citeDissProp}, {ICcited}, libraries, opac, sorted}
}

@online{_lists_,
	title = {Lists},
	url = {http://help.bibliocommons.com/en-ca/045faq/060faq_lists},
	titleaddon = {Bibliocommons},
	urldate = {2011-10-11},
	keywords = {{citeDiss}, {citeDissProp}, {ICcited}, sorted}
}

@inproceedings{sen_tagging_2006,
	location = {New York, {NY}, {USA}},
	title = {tagging, communities, vocabulary, evolution},
	isbn = {1-59593-249-6},
	url = {http://doi.acm.org.proxy2.library.illinois.edu/10.1145/1180875.1180904},
	doi = {10.1145/1180875.1180904},
	series = {{CSCW} '06},
	pages = {181--190},
	booktitle = {Proceedings of the 2006 20th anniversary conference on Computer supported cooperative work},
	publisher = {{ACM}},
	author = {Sen, Shilad and Lam, Shyong K. and Rashid, Al Mamunur and Cosley, Dan and Frankowski, Dan and Osterhouse, Jeremy and Harper, F. Maxwell and Riedl, John},
	urldate = {2011-11-09},
	date = {2006},
	keywords = {{citeDiss}, Communities, evolution, social book-marking, tagging, vocabulary},
	file = {Sen et al_2006_Tagging, Communities, Vocabulary, Evolution.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\ACM2006\\Sen et al_2006_Tagging, Communities, Vocabulary, Evolution.pdf:application/pdf}
}

@inproceedings{quinn_human_2011,
	title = {Human computation},
	isbn = {978-1-4503-0228-9},
	url = {http://dl.acm.org.proxy2.library.illinois.edu/citation.cfm?id=1979148},
	doi = {10.1145/1978942.1979148},
	pages = {1403},
	publisher = {{ACM} Press},
	author = {Quinn, Alexander J. and Bederson, Benjamin B.},
	urldate = {2011-10-19},
	date = {2011},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14, crowdsourcing, data mining, human computation, literature review, social computing, survey, Taxonomy},
	file = {Quinn_Bederson_2011_Human computation.pdf:C\:\\Users\\Peter\\Dropbox\\papers\\ACM Press2011\\Quinn_Bederson_2011_Human computation.pdf:application/pdf;Shibboleth Authentication Request:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\K8IRQATF\\login.html:text/html}
}

@thesis{organisciak_why_2010,
	location = {Edmonton, Alberta},
	title = {Why Bother? Examining the motivations of users in large-scale crowd-powered online initiatives},
	url = {http://hdl.handle.net/10048/1370},
	abstract = {This study examines the motivations of participants in networked, large-scale content production and research – a paradigm of distributed work magnified by the Internet. This has come to be called crowdsourcing. The approach taken in examining the crowdsourcing paradigm is of retrospection, with a study focused on observed examples and existing theories. Thirteen cases of existing crowdsourcing sites were selected for study, from a larger sample of 300. These cases were coded by their site properties and analyzed, identifying possible motivational mechanisms. Subsequent interviews with eight medium to heavy Internet users further explored these features, with an emphasis on ranking relative importance of various motivators. This study concludes with a series of recommendations on motivating crowds in such projects, emphasizing among others the importance of topical interest, ease of participation, and appeals to the individuals’ knowledge. In addition to base motivators, a number of support, or secondary, motivators are outlined.},
	pagetotal = {167},
	institution = {University of Alberta},
	type = {Thesis},
	author = {Organisciak, Peter},
	date = {2010-08-31},
	keywords = {{citeDiss}, {citeDissProp}, {citeiConf}14, crowdsourcing, {ICcited}, motivation, sorted}
}

@book{shirky_here_2009,
	title = {Here comes everybody},
	publisher = {Penguin Books},
	author = {Shirky, C.},
	date = {2009},
	keywords = {{citeDiss}, {citeDissProp}, cognitive surplus, gamification, {ICcited}, sorted},
	file = {Google Scholar Linked Page:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\BP8E9DKC\\Shirky - 2009 - Here comes everybody.pdf:application/pdf}
}

@inproceedings{snow_cheap_2008,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks},
	url = {http://dl.acm.org.proxy2.library.illinois.edu/citation.cfm?id=1613715.1613751},
	series = {{EMNLP} '08},
	shorttitle = {Cheap and fast—but is it good?},
	abstract = {Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.},
	pages = {254--263},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Snow, R. and O'Connor, B. and Jurafsky, D. and Ng, A.Y.},
	date = {2008},
	keywords = {annotation, {citeDiss}, {citeDissProp}, expertise, experts, {hcirCITE}, {hcirMidtermCITE}, mechanical turk, {mturkCITE}, tagging, turk},
	file = {Google Scholar Linked Page:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\BQTJ4D2F\\login.html:text/html}
}

@inproceedings{novotney_cheap_2010,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Cheap, fast and good enough: Automatic speech recognition with non-expert transcription},
	shorttitle = {Cheap, fast and good enough},
	eventtitle = {{HLT} '10},
	pages = {207--215},
	booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	author = {Novotney, S. and Callison-Burch, C.},
	date = {2010},
	keywords = {annotation, {citeDiss}, {citeDissProp}, expertise, experts, {hcirCITE}, {hcirMidtermCITE}, mechanical turk, {mturkCITE}, turk},
	file = {Google Scholar Linked Page:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\4P7694FR\\login.html:text/html}
}

@inproceedings{bernstein_soylent_2010,
	location = {New York, {NY}},
	title = {Soylent: a word processor with a crowd inside.},
	isbn = {978-1-4503-0271-5},
	url = {http://dl.acm.org/citation.cfm?id=1866078},
	doi = {10.1145/1866029.1866078},
	eventtitle = {{UIST} '10},
	pages = {313--322},
	booktitle = {Proceedings of the 23nd annual {ACM} symposium on User interface software and technology},
	publisher = {{ACM} Press},
	author = {Bernstein, Michael S. and Little, Greg and Miller, Robert C. and Hartmann, Björn and Ackerman, Mark S. and Karger, David R. and Crowell, David and Panovich, Katrina},
	urldate = {2011-09-12},
	date = {2010},
	keywords = {candidates, {citeDiss}, {citeDissProp}, favorites, hcir, {mturkCITE}, rate4, {swiftCite}},
	file = {Soylent:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\6T3DT5QQ\\citation.html:text/html}
}

@article{von_ahn_games_2006,
	title = {Games with a purpose},
	volume = {39},
	issn = {0018-9162},
	url = {http://scholar.google.ca.login.ezproxy.library.ualberta.ca/scholar?hl=en&lr=&cluster=7220788619130524050},
	abstract = {Through online games, people can collectively solve large-scale computational problems.},
	pages = {96--98},
	number = {6},
	journaltitle = {Computer},
	author = {von Ahn, L.},
	urldate = {2009-04-17},
	date = {2006-06},
	keywords = {{citeDiss}, {citeDissProp}, gamification, serious games},
	file = {ieee-gwap.pdf:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\9UGX3936\\ieee-gwap.pdf:application/pdf}
}

@online{_what_,
	title = {What is {reCAPTCHA}?},
	url = {http://recaptcha.net/learnmore.html},
	titleaddon = {Recaptcha},
	urldate = {2008-09-27},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {What is reCAPTCHA?:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\M3CIH96B\\learnmore.html:text/html}
}

@inproceedings{ahn_labeling_2004,
	location = {Vienna, Austria},
	title = {Labeling images with a computer game},
	isbn = {1-58113-702-8},
	url = {http://dl.acm.org/citation.cfm?id=985733},
	abstract = {We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.},
	pages = {319--326},
	booktitle = {Proceedings of the {SIGCHI} conference on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Ahn, Luis von and Dabbish, Laura},
	urldate = {2008-11-03},
	date = {2004},
	keywords = {{citeDiss}, {citeDissProp}, distributed knowledge acquisition, hcir, humans in {IR}, {ICcited}, image labeling, online games, rate5, World Wide Web}
}

@book{howe_crowdsourcing_2008,
	edition = {1},
	title = {Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business},
	isbn = {0-307-39620-7},
	shorttitle = {Crowdsourcing},
	pagetotal = {320},
	publisher = {Crown Business},
	author = {Howe, Jeff},
	date = {2008-08-26},
	keywords = {{citeDiss}, {citeDissProp}}
}

@book{mackay_memoirs_1852,
	title = {Memoirs of Extraordinary Popular Delusions and the Madness of Crowds},
	pagetotal = {503},
	author = {Mackay, Charles},
	date = {1852},
	keywords = {{citeDiss}, {citeDissProp}}
}

@book{surowiecki_wisdom_2004,
	title = {The Wisdom of Crowds},
	publisher = {Doubleday},
	author = {Surowiecki, James},
	date = {2004},
	keywords = {{citeDiss}, {citeDissProp}, crowdsourcing, {ICcited}, read, sorted}
}

@book{benkler_wealth_2006,
	location = {New Haven},
	title = {Wealth of Networks},
	url = {http://cyber.law.harvard.edu/wealth_of_networks/Download_PDFs_of_the_book},
	publisher = {Yale University Press},
	author = {Benkler, Yochai},
	urldate = {2008-10-20},
	date = {2006},
	keywords = {{citeDiss}, {citeDissProp}},
	file = {Benkler_Wealth_Of_Networks.pdf:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\EATAADGC\\Benkler_Wealth_Of_Networks.pdf:application/pdf}
}

@online{howe_crowdsourcing_2006,
	title = {Crowdsourcing: A Definition},
	url = {http://crowdsourcing.typepad.com/cs/2006/06/crowdsourcing_a.html},
	shorttitle = {Crowdsourcing},
	titleaddon = {Crowdsourcing: Tracking the rise of the amateur},
	author = {Howe, J.},
	date = {2006},
	keywords = {{citeDiss}, {citeDissProp}}
}

@article{howe_rise_2006,
	title = {The rise of crowdsourcing},
	volume = {14},
	number = {6},
	journaltitle = {Wired Magazine},
	author = {Howe, J.},
	date = {2006},
	keywords = {{citeDiss}, {citeDissProp}}
}

@book{le_bon_crowd_1896,
	title = {The Crowd: A Study of the Popular Mind},
	url = {http://socserv.socsci.mcmaster.ca/~econ/ugcm/3ll3/lebon/Crowds.pdf},
	author = {Le Bon, Gustav},
	urldate = {2008-10-20},
	date = {1896},
	keywords = {{citeDiss}, {citeDissProp}}
}

@article{lakhani_how_2003,
	title = {How open source software works: "free" user-to-user assistance},
	volume = {32},
	url = {http://www.sciencedirect.com/science/article/B6V77-479TM54-1/2/5672b73de696a2d8a1d68e6d5747a2cb},
	doi = {10.1016/S0048-7333(02)00095-1},
	shorttitle = {How open source software works},
	abstract = {Research into free and open source software development projects has so far largely focused on how the major tasks of software development are organized and motivated. But a complete project requires the execution of "mundane but necessary" tasks as well. In this paper, we explore how the mundane but necessary task of field support is organized in the case of Apache web server software, and why some project participants are motivated to provide this service gratis to others. We find that the Apache field support system functions effectively. We also find that, when we partition the help system into its component tasks, 98\% of the effort expended by information providers in fact returns direct learning benefits to those providers. This finding considerably reduces the puzzle of why information providers are willing to perform this task "for free." Implications are discussed.},
	pages = {923--943},
	number = {6},
	journaltitle = {Research Policy},
	author = {Lakhani, Karim R. and Hippel, Eric von},
	urldate = {2008-10-20},
	date = {2003-06},
	keywords = {{citeDiss}, {citeDissProp}, Open source software, User innovation, User support, Virtual community},
	file = {ScienceDirect Snapshot:C\:\\Users\\Peter\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\zec9madg.default\\zotero\\storage\\PPCWABCT\\science.html:text/html}
}
