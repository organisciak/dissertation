Literature Review
===================

## Intro

Crowdsourcing is a simple concept that has received considerable research attention in the past few years, alongside a realization in the power of the internet for effectively connecting people in great numbers.

Perhaps unsurprisingly, this concept precedes the language that we have developed around it in recent years, and the research in crowdsourcing has not been evenly distributed.

In this section, I provide an overview of crowdsourcing and the notable research within it.

## Definitions and History

Crowdsourcing broadly describes the use of distributed crowds to complete a task that would otherwise have been done by one or a few people.
Beneath this large umbrella, there are many concepts that are either in its purview or overlap with it.

### Crowdsourcing

Crowdsourcing refers to "groups of disparate people, connected through technology, contributing to a common product" [@organisciak_why_2010].
It is an umbrella term that broadly captures the abilities of the internet, as a communications medium, to efficient connect people.

Nothing about crowdsourcing is fundamentally tied to the internet.
It is entirely possible to bring together large groups of people in different ways, but the access and efficiency of the internet is both
what made the concept seem so novel in the 20th century and what gave it value in the various realms where it was applied.

Whereas crowds have long been noted for their collective simplicity [@le_bon_crowd:_1896]<!--_--> or irrationality [@mackay_memoirs_1852],
through the internet one can perform human-specific tasks at a scale usually only seen for computational tasks.

To understand crowdsourcing, a brief understanding of its history is necessary.
The term is recent and has an unambiguous source, but immediately upon introduction, it was adopted and expanded by society.

The term _crowdsourcing_ was coined in a 2006 Wired article by Jeff Howe [@howe_rise_2006].
Howe was writing from a labor perspective, looking at online marketplaces for people to solve problems and create content.
His focus was on systems like InnoCentive, a site for companies to outsource research and development problems for a bounty, and iStockPhoto, a website that allowed amateur photographers to sell their images as stock photos.
The article briefly looked at user-generated online content, though in the context of television programs that use online video as content.

Despite the somewhat specific initial definition, the term _crowdsourcing_ struck a chord more broadly and was culturally co-opted.
This happened very quickly: within nine days Howe noted a jump from three Google results to 189,000 [@howe_birth_2006].
Within a month, Howe address the co-opting of the term, "noticing that the word is being used somewhat interchangably with Yochai Benkler's concept of commons-based peer production" [@howe_crowdsourcing:_2006]<!--_-->.
He gives his own definition, but also notes that language is slippery and he's "content to allow the crowd define the term for itself (in no small part because [he's] powerless to stop it.)".

Thus, crowdsourcing was adopted to refer broadly to a series of related concepts, all related to people being connected online.
These concepts included
 the 'wisdom of the crowds'[@surowiecki_wisdom_2004], 
 human computation[@ahn_labeling_2004],
 commons-based peer production[@benkler_wealth_2006], and
 free and open-source development [@lakhani_how_2003; @raymond_cathedral_1999].

### Wisdom of the Crowds

_The Wisdom of the Crowds_[@surowiecki_wisdom_2004] is a book written by journalist James Surowiecki in 2004.
The book observes the strength of human decision-making when one in aggregate, and the term 'wisdom of the crowds' has survived the book to refer to cases that make use of this.

 

### Human-Computation

_Human Computation_ emerged from from the doctoral dissertation work by Luis von Ahn in 2005, popularized alongside the ESP Game [@ahn_labeling_2004; @von_ahn_games_2006]. <!--_-->
It refers to a process of computation -- the "mapping of some input representation to some output representation using an explicit, finite set of instruction" [@law_defining_2011] -- performed by humans.

In synthesizing the definition of human computation in relation to crowdsourcing, collection intelligence, and social computing, Quinn and Bederson note two characteristics of consensus:
 that "the problems fit the general paradigm of computation, and as such might someday be solvable by computers", and 
 that "the human participation is direction by the computational system or process" [@quinn_human_2011].

As noted by @[TODO cite law], Turing defined the purpose of computers as carrying out operations that humans would normally do.
Human Computation, then, refers to utilizing humans for operations that computers simply are not capable of performed yet.

By this definition, much human computation aligns with crowdsourcing, but large swaths of crowdsourcing are not relevant to human computation.
For example, the types of large creative crowdsourcing projects like t-shirt design website Threadless and online encyclopedia Wikipedia are not human computation.
Inversely, human computation does not have to sustained by self-selected workers; a more traditionally hired closed system can suffice [@law_defining_2011].

### Open-Source

### User Innovation

von Hippel 2003, 2005, 1988

### Social Computing

<!-- See Law 2011, and see Bederson 2011 -->

### Collective Intelligence

<!-- also see law and bederson papers -->

### Commons-based Peer Production

### Classical Work on crowds

#### Extraodinary Madness of the Crowds (Mackay [sp?])

#### The Crowd (LeBon)

## Taxonomies of Crowdsourcing

The space of crowdsourcing is large, and there have been a number of attempts to organize the sub-concepts within it or to reconcile it in a space alongside other areas of research.
Some of the most important questions in differentiating crowdsourcing include:

<!--Contributors-->
 * Who are the contributors? What are their skills?
 * How are contributors motivated? Are they paid or do they volunteer for other incentives?
<!--Contributions-->
 * Are contributions new, or do they react to existing documents or entities?
 * Are contributions presented or used as a whole, or are they combined into a larger contribution?
 * What do the contributions look like? Are they subjective or objective?
<!--Beneficiaries and directors -->
 * Who is asking for the contributions? Who is benefiting?
<!--Collaboration-->
 * Is the collaboration indirect (i.e. contributors work on parts independently) or manifest?
<!--Design-->
 * Is the crowdsourcing central to the system?
 * How is quality controlled for?

### Overview

 * Motivation
   * Primary/Secondary, Contribution/commitment
   * Extrinsic/Intrinsic
   * Paid/Volunteer
     * Form of renumeration
 * Type of Work
   * Creative / Reactive
 * Aggregation
  * Selective
  * Integrative
     * Summative / Iterative / Averaged
 * Type of Crowd 
 * Beneficiary
   * Autonomous / sponsored
   * Requester/contributor relationship: Symbiosis / parasitism / commensalism
 * Object of Interest
 * Centrality 
   * Core / Peripheral (incidental)
 * Quality Control



### Motivation

The incentives for contributors to participate in crowdsourcing are complex and not always... <!--TODO-->

#### Intrinsic / Extrinsic Motivation

Motivation in crowdsourcing follows related work in the motivations of humans in general (TODO: cite Deci, Maslow, Aldoufer?).
While a review of that work is beyond the scope of this work, many views<!--(TODO: 'many views' needs citations)--> of crowdsourcing motivation adopt the lens of motivation as a mixture of _intrinsic_ factors and _extrinsic_ factors [@ryan_intrinsic_2000].
In the former, fulfillment is internal to the contributor, psychologically motivated, while in the latter the rewards are external.

The spectrum of intrinsic to extrinsic motivators is commonly paralleled in crowdsourcing literation in a dichotomy of paid and volunteer crowdsourcing [].

Paid and volunteer crowdsourcing are not exclusive, and there are extrinsic motivators beyond money.
However, this separation is common because of it accounts for some of the starkest differences between how crowdsourcing is implemented and motivated.
There are differing design implications around people being paid and performing work for other reasons: money is a direct currency for obtaining labor, while convincing volunteers to contribute requires a greater sensitivity of their needs and ultimately more complexity in engineering the crowdsourcing system or mechanic.

It has been shown that intrinsic motivation still plays a part in paid crowdsourcing [@mason_financial_2010], and some systems mix intrinsically motivated tasks with payment or the chance at renumeration (e.g. Quirky, GasBuddy, 99Designs).

Some taxonomies make a distinction between forms of payment.
@geiger_managing_2011 makes the distinction between fixed renumeration, with a pre-agreed fee, and success-based renumeration, such as contest winnings or bonus.

#### Specific Motivators

TODO section notes:

 - primary / secondary motivators
 - commitment/contribution
 - add more from @kraut_building_2011, @kraut_encouraging_2012

Taxonomies of specific motivators seen in crowdsourcing have been previously attempted, with varying results that touch on similar issues.
In @organisciak_why_2010, for example, I identified a series of primary and secondary motivators from a diverse set of crowdsourcing websites.
Below I adopt the categories from that study, as they accommodate related work well.

_Primary motivators_ are those that are considered critical parts of a system's interaction.
Systems do not require all of them, but to attract and retain contributions, they need one or more of them.
In contrast, _secondary motivators_ are system mechanics that generally were not observed as necessary parts of a system, but were elements that encourage increased interaction by people that are already contributors.
@kraut_building_2011 similarly differential between encouraging contributions, and encouraging commitment.

The motivators of @organisciak_why_2010 were observed from a content analysis of 13 crowdsourcing websites and subsequent user interviews.
For sampling, 300 websites most commonly described as 'crowdsourcing' in online bookmarks were classified with a bottom-up ontology, then the 13 final sites were selected through purposive stratified sampling, to represent the breadth of the types of crowdsourcing seen.

The primary motivators seen in @organisciak_why_2010 were:

 * __Money and extrinsic reward__. Paying crowds is not particularly novel, but it is the most reliable approach for collecting contributions.
In the absence of other motivators or where certainty is required, reimbursement will attract contributors.
However, it also introduces bottlenecks of scale, and negates some of the benefits of intrinsic motivation.
@mason_financial_2010 note that, while intrinsic motivation still exists on paid crowdsourcing platforms, it is overwhelmed when tasks are too closely tied to reimbursement, resulting in contributions that are done minimally, briskly, and with less enjoyment.
<!-- TODO Mark Twain quote -->
 * __Interest in the Topic__. Sites that cater to people that have a pre-existing interest in their subject matter or outcomes tend to get longer, more consistent engagement.
For example, the Australian Newspaper Digitisation Project found that that amateur genealogists, with pre-existing communities and a willingness to learn new technologies, took “to text correction like ducks to water” @holley_many_2009.
Similarly, Galaxy Zoo found similar success with amateur astronomers.
 * __Ease of entry and ease of participation__. Low barriers to entry and participation were cited by every single user interviewed for the study.
TODO: repurpose old text: Wikipedia has a low barrier to entry – just click ‘Edit’ – but has been criticized for the barrier to participation created by the interface and the overzealous core community (Angwin and Fowler 2009, Sanger 2009). As a result, its new participant numbers have been trending downward, and it has been singled out in the most recent Wikimedia annual report as a high priority problem.
 * __Altruism and Meaningful contribution__. People like to help if they believe in what they’re helping, whether they are donating money to an artist <!--TODO cite Kickstarter--> or collaboratively trudging through Senate bills <!--TODO cite MP finance scandal-->.
Writing about Flickr Commons, Library of Congress noted that they “appear to have tapped into the Web community’s altruistic substratum by asking people for help.
People wanted to participate and liked being asked to contribute”. [@springer_for_2008].
Likewise with Galaxy Zoo <!-- TODO cite GZ study and my own-->, people often cite the fact that it’s not simply an outreach project, but a tangible way to contribute to science.
 * __Sincerity__.
 * __Appeal to knowledge__.
 * __Appeal to opinions__.

<!---

Talking through taxonomies
Centrality
Sincerity
Seen in academia with scholars getting weary of free work for Elsevier
Novelty
vs. curiosity?

Appeal to knowledge
Implicit vs. explicit
irritation as implicit appeal to knowledge: Mike’s first Wikipedia contribution

--> 

One ephemeral but nonetheless observable motivator overlooking in @organisciak_why_2010 is _novelty_: a unique idea can attract contributions for a short amount of time.

The supplemental secondary motivators observed in the study were:

 * __"Cred": External indicators of progress and reputation__.
 * __Feedback and impression of change__.
 * __Recommendations and the social__.
 * __Window fixing__.

### Aggregation

@schenk_crowdsourcing:_2009 and @geiger_managing_2011 discuss two types of aggregation: _integrative_ and _selective_.
Integrative aggregation pools contributions into a common product, like a wiki, while selective aggregation tries to choose the best contributions, such as in contests.

This simple separation hides some of the complexity seen in aggregation approaches.
Integrative aggregation can be approached in a number of ways.
I offer the following finer views on integrative aggregation:
 * __Summative__. In summative aggregation, people contribute to an ever-expanding base of information. Contributions are clearly part of a bigger whole, but their individual form is retained. Examples: online reviews
 * __Iterative__. In versioned aggregation, multiple contributions are used toward a larger product, but the contributions are permutations of a common work. Examples: wikis
 * __Averaged__. In averaged aggregation. Contributions are still pooled, but a consensus-seeking processing tries to reconcile them. Examples: ratings, multiple-keyed classifications 

<!-- See also @quinn_human_2011 -->

### Director / Beneficiary

Who directs the crowdsourcing activities and who benefits from the contributions?

Considering the director of a crowdsourcing task, @zwass_cocreation:_2010 distinguishes between _autonomous_ and _sponsored_ forms of crowdsourcing.

_Sponsored_ crowdsourcing is when there is a entity at the top soliciting the contributions: a client of sorts. 
In contrast _autonomous_ crowdsourcing serves the community itself.
Autonomous crowdsourcing can be in a centralized location, like a community-written wiki or video-sharing website, or exist loosely, as in blogs.
@zwass_cocreation:_2010 explains: "Marketable value is not necessarily consigned to the market—it may be placed in the commons, as is the case with Wikipedia".

Considering the soliciting party as a case of sponsorship or autonomy is useful, though a further distinction should be made between the collective (the _crowd_) and the individual (the _contributors_).
Crowds collaborate toward a shared goal, as with Wikipedia or a type of open-source software development, while individuals are more self-possessed.
For example, in citation analysis through web links, as was done with PageRank (<!--TODO cite pagerank paper-->), the large-scale benefits of the crowds were ...

The relationship between the director 
My effort vs. my benefit vs. their benefit

Symbiosis (both benefit) vs. Parasitism (one benefits at the expense of the other) vs. Commensalism (relationship between two organisms where one organism benefits without affecting the other)


### Object of Interest

* Creation vs. Reaction

### Criticality of Crowdsourcing
<!-- TODO I used "centrality above, is it better or worse than criticality?-->

 * How important in the crowdsourcing to the larger project? 
 * Peripheral / Core
 * Fairly neglected in literature
 * Relates to incidental crowdsourcing, which looked at the space of 'peripheral and non-critical' crowdsourcing [@organisciak_incidental_2013]
 * Notes
   * Soylent: is Soylent incidental or full-system?

### Type of Work

The type of work performed by crowds can vary greatly in its complexity and style. 

One notable facet here is represented in the concept of human computation, where "the problems fit the general paradigm of computation, and as such might someday be solvable by computers" [@quinn_human_2011].
In the understanding that crowdsourcing is not solely human computation tasks, the inferred companion to these types of tasks are those that are expected to be too complex for computers: creative, judgmental, or requiring critical thinking.

@schenk_crowdsourcing:_2009 distinguish between three types of crowdsourcing.
First are routine tasks, such as crowdsourcing of OCR text correction with ReCaptcha.
The majority of human computation tasks would likely fall within this category of rote tasks.
Second are complex tasks, such are open-source software development.
Finally, they suggest creative tasks. An examples would be a system like MyStarbucksIdea, where people suggest changes they would like to see at the coffee chain Starbucks.
Since @schenk_crowdsourcing:_2009 focus on crowdsourcing when there is a client, usually a corporate client, they do not consider the wider space of creative crowdsourcing tasks.

#### Subjective vs. Objective Crowdsourcing

Another parallel being drawn in recent years is that of objective or subjective crowdsourcing tasks.

Objective tasks are assumed to have an authoritative truth, even if it is unknown.
For example, in transcribing scanned texts, it is assumed that there is a 'correct' passage in the work that has been scanned.

In contrast, Subjective tasks have a variable concept of correctness, as they are are not expected to be consistent between contributors.
<!--TODO: write more-->

Commonly human computation are objective tasks, while Schenk's split of routine, complex, and creative does not make in any particular way to this split.

<!-- TODO: citations -->

This designation also applies to aggregation, because sometimes subjective tasks are treated by a system as if they were objective, trying to determine a consensus while not treating deviations form the consensus as flawed contributions.

### Type of Crowd

Vukovic and Batolini (TODO proper cite) define two extremes of crowd types: _internal_ and _external_.
Internal crowds are composed solely of contributors from the organization that is crowdsourcing, if it is thus centralized.
External crowds are members outside of the institution.
Vukovic and Batolini also note that _mixed_ crowds are observable.

#### Necessary Skills

 * mentioned briefly in @quinn_human_2011, anywhere else?

### Quality Control

see @quinn_human_2011, others

### Common Design Patterns

 * Microtasking
 * Gamification
   * Eickhoff, Harris et. al talk about gamification on MTurk [TODO]
 * Opinion Ratings
 * Platforms
 * Contest
 * Wisdom of crowds

### Full Taxonomies

@geiger_managing_2011 identify crowdsourcing processes by four defining characteristics: the pre-selection process for contributors, the accessibility of peer contributions, the aggregation of contributions, and the form of renumeration for contributors.

@quinn_human_2011 look at crowdsourcing along six facets: motivation, quality control, aggregation, human skill, process order, and task-request cardinality.

## Top research in crowdsourcing

### ESP Game

### VizWiz

### Soylent

### Mason and Watts

### Benkler - Wealth of Networks


## Crowdsourcing in the Wild
Notable crowdsourcing in the wild

### Wikipedia

### Threadless

### Academic

#### Zooniverse

#### FoldIt

#### ReCaptcha


## Failures and Concerns



### Ethics


## Research in Information Science



## Research in Information Retrieval

In information retrieval, the focus on crowdsourcing has been predominantly in the use of paid crowds for generating evaluation datasets, though there have been efforts to use crowds to improve document representation or even query specific ranking.

### Crowdsourcing for evaluation

- relevance judgments
- alonso et al.
- TREC crowdsourcing track

### Crowdsourcing in the machine

- PageRank
- Lamere - social tagging

## Research relevant to study

### Coding Aggregation / Ground Truth generation


<!--TODO: Rename header-->

