\chapter*{Abstract}\label{abstract}

Crowdsourcing offers a valuable approach to improving our understanding of information objects and people's interpretations of them.

Human contributions open the door to latent information, subjective judgments, and other encoding of data that is otherwise difficult to infer.
However, such contributions are also subject to variance from the inconsistencies of human interpretation.

This dissertation studies the problem of such variance in crowdsourcing for descriptive purposes and investigates how it can be controlled both in already collected data and in collecting new data.
Doing so is pursued on three stages: interpreting already collected objective data, 
designing better objective collection instruments, and designing for subjective contexts.

In interpreting crowdsourced data, this work looks at whether factors including time, experience, and agreement with others provide indicators of contributions quality, which can then be used to improve modelling and use of the contributions.

Since crowdsourcing is borne out of an interaction, not all crowdsourcing data corrections are posterior: it also matters how you collect that data.
Thus, this dissertation turns its focus away from poor contributions in isolation to the circumstances of those contributions: the collection interface.
Controlling for identical tasks, a number of design manipulations are tested. Among these, designs that provide training intervention and performance feedback are found to improve the quality of crowd contributions with minimal extra effort or cost. Later, the effect of design changes are tested in a practical information retrieval evaluation setting, providing notable improvements.

Finally, designing for a different type of task is considered: subjective contexts. Crowds are well positioned to teach us about how information can be adapted to different person-specific needs. Looking at crowdsourcing for personalization over on-demand contexts, two protocols are introduced for collecting subjective data. Evaluated over different contexts, both protocols offer improvements over an unpersonalized baseline, each with its own strengths.

The primary contribution of this work is an understanding of crowd data quality improvements from non-adversarial perspectives.
The findings in this dissertation contribution to the crowdsourcing research space as well as providing practical improvements to crowd collection best practices.
