Abstract
========

Crowdsourcing offers a valuable method to improve information retrieval indexing by using humans to improve the indexable data about documents or entities.
Human contributions open the door to latent information, subjective judgments, and other encoding of difficult to extract data.
However, such contributions are also subject to variance from the inconsistencies of human interpretation.

The proposed dissertation studies the use of crowdsourcing for collecting descriptive metadata, which I refer to by the shorthand _descriptive crowdsourcing_.

problem of such variance in crowdsourcing for information retrieval, and investigates how it can be controlled both in already collected data and in collecting new data.
Doing so will incorporate the body of existing research, my past work in the area, and two new studies of pertinent sub-problems within the larger research problem.

At the heart of this study is the assumption of honest-but-biased contributors.
Rather than focusing on finding dishonest or unreliable contributors, a well-studied problem in crowdsourcing, this dissertation focuses on strategies that understand the quirks and inconsistencies of humans in trying to account data reliability problems.

Often, there is a trade-off between well-engaged crowds and the amount of restrictiveness that a task designer can exert on a task.
This dissertation admits that in practice, the engagement of an accessible and non-restrictive task is more encouraging both to contributors and the health of a community.
How do you encourage the humanness<!-- I know this is a real word used in literature, but it seems awkwark... --> of contributors while still convincing them to be consistent and reliable, and how do you model the subsequent contributions?

