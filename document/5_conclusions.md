Conclusions
=============

## What would surprise me? What can fail? Backup options and fallback.

## Workplan

<!-- TODO1: Tasks and Goals -->
What's necessary?

## Designing Crowdsourcing Tasks

 * Admin
    * IRB Application
    * Search for funding opportunity
 * Design Tasks
    1. Additional literature review?
    2. Find or collect Twitter dataset
    3. Annotate gold standard set?
    4. Build measurement instruments
        1. Basic Design
            * Prepare Infrastructure for collecting data (Backbone + Node.js + Express + Heroku)
        2. Indicators of Quality Design
        3. Training Design
            * Should measure when people get the task wrong
        4. Clock Ticking Design
    5. Data Collection
        * First task: offer bonus for workers that return for more
            * Since both the Indicators of Quality Design and the Training Design require at least two task sets to be completed
        * Payment amount: determined based on time spent statistics
       

## Modeling Document Metadata in Retrieval

 * Data collection
    * Build crawler
    * Secure disk space and server access. Here, Amazon EC2 is the default.
 * System development
    * Prepare baseline system
    * Develop front-end for evaluation (Connect to work performed for design study.)
 * Research
    * Generate a test collection (Mechanical Turk)
 * Evaluation
 * Writing

## Publication targets

There are a number of conferences that represent the target audience of this dissertation, generally mixed-domain areas that combine computer science or information science with sensitivity to social or human-centered issues.
Research from the chapter on design of crowdsourcing tasks would be appropriate at HCOMP, CSCW, CHI, or ASIS&T.
The research from the chapter on retrieval over crowdsourced information on Pinterest would be appropriate at JCDL, CIKM, or potentially SIGIR.
