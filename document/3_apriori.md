A Priori Corrections for Bias
==============================

## Introduction



## Problem

#### Work up to now

##### ASIS&T Paper

In <!--TODO:cite ASIS&T paper-->, we found that workers 

##### Incidental Crowdsourcing

In <!--TODO: cite incidental crowdsourcing paper -->, I looked at a facet of crowdsourcing that I called _incidental crowdsourcing_ (IC).

Incidental crowdsourcing refers to crowdsourcing in the periphery.
These are elements that are unobtrusive and non-critical to the user or the system.
They generally
 describe existing information,
 collect contributions in low-granularity mechanisms,
 and favor interface choices over statements.
An example an incidental crowdsourcing mechanic mike be a 'thumbs-up' voting button on a video-sharing website or tagging functionality on an image-sharing website that does not force itself upon users.
By not taking users hostage or asking users of overly attentive or time-consuming contributions, 

##### System 1 vs System 2

In peripheral research conducted with Adam Kalai, Jaime Teevan, Susan Dumais, and Rob Miller for Microsoft, <!--(how to cite unpublished research to make clear that this is MS IP?)--> we found a similar effect on Mechanical Turk.



## Scope

### Paid Crowdsourcing
In looking at the design of contribution tasks, I hope to concentrate on paid crowdsourcing.
For the scope of this study, it would be intractable to look at at the design of both paid and volunteer crowdsourcing, so I will pursue the facet more pertinent to information retrieval.
<!--'More pertinent': Is this true? TODO: Rewrite-->
Paying workers is only one part of crowdsourcing, and one that arguably tethers the scalability of a task by anchoring it
to financial means.
However, it is easier to control for by removing much of the complexities of motivation.
Information retrieval researchers are also using the predictability of paid crowd markets like Amazon's Mechanical Turk to generate on-demand data, making design for those systems important.
<!--Add citations?-->

More importantly, much information retrieval research occurs parallel to the system of content
<!--Users on Flickr, for example, contribute semantic tags-->
 information retrieval systems 

### Literature


#### Prior Work 

### Proposed Research

In new research for this chapter, I will investigate the effect of different parameterizations of the task, 

##### Data

##### Parameterization

##### Evaluation

<!--Talk about Mechanical Turk. -->
<!--Talk about the the real world use of crowdsourcing. Google has internal systems, so does MS. Researchers are using it for on-demand data -->
