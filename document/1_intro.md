INTRODUCTION
=============

> In these democratic days, any investigation in the trustworthiness and peculiarities of popular judgments is of interest
> -- Francis Gaston, 1907
<!--TODO: Add citation to refs-->

## Notes

## Introduction

Strong information retrieval depends on reliable, detailed information to index.
The broad phenomenon of crowdsourcing has the potential to improve retrieval over web documents by producing descriptive metadata about documents.
Since crowdsourcing considers humans at large-scales, it can be used for qualitative and subjective information at scales useful to retrieval.
<!--Written roughly to dump, TODO wordsmith-->

<!-- Old text
The internet is growing increasingly interactive as it matures.
Rather than simply transmitting information to readers, web pages allow their audience to react and interact with their information.
The products of these interactions are a trove of qualitative judgements, valuable to understanding information objects.
-->

### Broad Research Question

However, humans have predictable and unpredictable biases that make it difficult to systematically adopt their contributions in an information system.
How do we interpret qualitative user contributions in an inherently quantitative system?
This study looks at the effect of human biases on crowdsourcing in information retrieval and how they affects the product of human contributions.

### Specific Research Question

Specifically: can bias in _descriptive crowdsourcing_ be accounted for in a manner that improves the information-theoretic quality of the contribution, either at the time of data collection or afterward?
<!--
 (TODO: lowers entropy?) of the contribution. ( for use in ranked retrieval?)
-->

#### Hypothesis

The proposed study makes an assumption that crowd contributors are honest but inherently biased, with the hypothesis that such a assumption leads to 
a) more algorithmically valuable crowdsourced description and 
b) a greater proportion of useful contributions.

#### Approach

This hypothesis will be applied in two different sites of crowdsourcing:
in the design of contribution tasks in order to minimize bias, and
in the normalization of contributions after they have already been collected.
Doing so will both adopt work that I have performed during my doctoral studies and perform new research.

In looking at the design of contribution tasks, I hope to concentrate on paid crowdsourcing.
For the scope of this study, it would be intractable to look at at the design of both paid and volunteer crowdsourcing, so I will pursue the facet more pertinent to information retrieval.
Paying workers is only a subset of crowdsourcing approaches, and one that arguably tethers the scalability of a task by anchoring it
to financial means.
However, it is easier to control for by removing much of the complexities of motivation.
Information retrieval researchers are using the predictability of paid crowd markets like Amazon's Mechanical Turk to generate on-demand data, making design for those systems important.
<!--Add citations?-->

More importantly, much information retrieval research occurs parallel to the system of content
<!--Users on Flickr, for example, contribute semantic tags-->
 information retrieval systems 

#### Practical application

The contribution of this work is the application of human corrective techniques to the encoding of metadata about existing information object, and the broader understanding of the nature of such contributions.

<!--TODO: crowdsourcing for encoding existing information with more informative metadata, with a goal of improving information retrieval systems. By focusing on a mix -->

### Relevance to IS and IR

Information science deals with many information objects, giving crowdsourcing considerable potential as a tool for item description. By collecting human judgments about the quality of information... 
<!--
TODO

#### Notes

Discuss relevance of problem to information science and specifically information retrieval.

This is partially strategic for myself: I should think of the problem right out of the gate: carefully and precisely. 
 Relevance to my areas of study should be returned to in-depth later.

## Problem

### Relevance in Information Science

### Notes

 * Much crowdsourcing research makes an adversarial assumption

 * Crowdsourcing aggregates contributions from human participants/workers. While such contributions are helpful for understanding the content in an information system, they are 

### Text


## Methodology

### Definitions

Before proceeding, the terminology of this study should be established. As this work spans multiple domains, and makes reference to recently introduced concepts, it is important to establish a shared understanding of language within these pages.

Note that the treatment here is cursory; a more in-depth look can be found in Chapter 2. (TODO better reference)

#### Crowdsourcing

#### Descriptive crowdsourcing

#### Human computation

#### Worker (paid)

#### Volunteer, contributor

#### Human bias?
			
### A Priori Corrections of Bias

#### Introduction

#### Literature

#### My Research Thus Far

#### Proposed Research

In new research for this chapter, I will investigate the effect of different parameterizations of the task, 

##### Data

##### Parameterization

##### Evaluation


<!--Talk about Mechanical Turk. -->
<!--Talk about the the real world use of crowdsourcing. Google has internal systems, so does MS. Researchers are using it for on-demand data -->

### Posterior Corrections of Bias

## Chapter Outline

The proposed dissertation will follow the following structure, delineated by chapters.

#### Introduction

The first chapter will introduce the problem of human bias in crowdsourcing and how it affects computational uses of contributed data. Subsequently, the assumption of honest but biased contributors will be outlined, and the hypothesis on this assumption will be outlined along with the study that will be pursued to test it.

#### Literature Review

#### A Priori Corrections for Bias

#### Posterior Corrections for Bias

