CHAPTER 1: INTRODUCTION
=======================

> In these democratic days, any investigation in the trustworthiness and peculiarities of popular judgments is of interest
> -- Francis Gaston, 1907i

% TODO: Add citation to refs

# Notes

# Introduction

The internet is growing increasingly interactive as it matures. Rather than simply transmitting information to readers, web pages allow their audience to react and interact with their information. The products of these interactions are a trove of qualitative judgements, valuable to understanding information objects.

## Broad Research Question

How do we interpret these qualitative user contributions in a quantitative system? Humans have individual biases that results in differences between how a contribution is created: this study looks at how such variance is introduced and how it affects the product of human contributions.

# Specific Research Question

Specifically, we focus on whether bias in /descriptive crowdsourcing/ be accounted for --either a priori or posterior to data collection-- in a manner that improves the information-theoretic quality (TODO: lowers entropy?) of the contribution. ( for use in ranked retrieval?)

### Hypothesis

The proposed study makes an assumption that crowd contributors are honest but inherently biased, with the hypothesis that such a assumption can result in a) more algorithmically valuable crowdsourced discription and b) a greater proportion of useful contributions.

### Practical application

The contribution of this work is the application of human corrective techniques to the encoding of metadata about existing information object, and the broader understanding of the nature of such contributions.


TODO: crowdsourcing for encoding existing information with more informative metadata, with a goal of improving information retrieval systems. By focusing on a mix 

## Relevance to IS and IR

Information science deals with many information objects, giving crowdsourcing considerable potential as a tool for item description. By collecting human judgments about the quality of information  

TODO

### Notes

Discuss relevance of problem to information science and specifically information retrieval.

This is partially strategic for myself: I should think of the problem right out of the gate: carefully and precisely. 
 Relevance to my areas of study should be returned to in-depth later.

# Problem

## Relevance in Information Science

## Notes

 * Much crowdsourcing research makes an adversarial assumption

 * Crowdsourcing aggregates contributions from human participants/workers. While such contributions are helpful for understanding the content in an information system, they are 

## Text


# Methodology

## Definitions

Before proceeding, the terminology of this study should be established. As this work spans multiple domains, and makes reference to recently introduced concepts, it is important to establish a shared understanding of language within these pages.

Note that the treatment here is cursory; a more in-depth look can be found in Chapter 2. (TODO better reference)

### Crowdsourcing

Crowdsou

### Descriptive crowdsourcing

### Human computation

### Worker (paid)

### Volunteer, contributor

### Human bias?
			
## A Priori Corrections of Bias

## Posterior Corrections of Bias

# Chapter Outline

The proposed dissertation will follow the following structure, delineated by chapters.

### Introduction

The first chapter will introduce the problem of human bias in crowdsourcing and how it affects computational uses of contributed data. Subsequently, the assumption of honest but biased contributors will be outlined, and the hypothesis on this assumption will be outlined along with the study that will be pursued to test it.

### Literature Review

### A Priori Corrections for Bias

### Posterior Corrections for Bias

