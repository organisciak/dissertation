Point-form outline
====================

As per Mike's suggestion, a point form outline of how I see the dissertation

* Broad Question: How do human biases affect crowdsourcing for information retrieval
* Specific Research Question: Can human bias in _descriptive crowdsourcing_ be accounted for, either at the time of data collection or afterward, in a manner that maximizes the information gain for information retrieval?
* Core assumption: That crowd contributors are honest but biased
* Hypothesis: That pursuing the honest-but-biases assumption leads:
	* Contributions that are more valuable for algorithmic use
	* A greater proportion of useful contributions
* This high-level theme for the dissertation is explored in two parts of the dissertation:
	1) A chapter on accounting for human bias at the time of collection
	2) A chapter on accounting for human bias in data that has already been collected

## Accounting for human bias at collection

* This chapter focuses 
* Problem: there is increasing evidence showing 
* Crowdsourcing Task: identifying the topic of a tweet
	*  


