%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% iConference Article
% LaTeX Template
% Version 2.1 (25/10/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% Modified by:
% Joseph Helsing (2012)
% Heinz-Alexander Fütterer, Maxi Kindling, Stephanie van de Sandt (2013)
% Cory Knobel (2014)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%        LIST OF NECESSARY PACKAGES
%----------------------------------------------------------------------------------------

% The packages used in this template were: fontenc, babel, microtype, indentfirst,
% geometry, multicol, graphicx, apacite, caption, booktabs, float, footnote, paralist,
% titlesec, fancyhdr, xcolor, tabularx, lipsum

%----------------------------------------------------------------------------------------
%        PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[10pt, letterpaper]{article}
\usepackage{lipsum} % Package to generate dummy text throughout this template
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{lmodern}
\usepackage[english]{babel}
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\setlength{\parindent}{0.5in} % Indent 0,5 inch
\usepackage[top=.8in, bottom=0.8in, left=1in, right=1in]{geometry} % Document margins
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[implicit=false,colorlinks=true,allcolors={black},urlcolor={black}]{hyperref}

\usepackage{apacite} % Allows for the correct APA citation of references
\renewcommand{\bibliographytypesize}{\fontsize{10pt}{10pt}} % Set the bibliography font size to 10pt

\usepackage{url}
\usepackage{tabularx}
\usepackage{booktabs} % Horizontal rules in tables
\usepackage[sf]{titlesec} % Font of Headings

\usepackage[normal]{caption} % Custom captions under/above floats in tables or figures
\captionsetup{format=plain,justification=RaggedRight,singlelinecheck=false}
\usepackage{footnote} %Enables footnotes

\makeatletter
\def\blfootnote{\selectfont\xdef\@thefnmark{}\@footnotetext}
\makeatother %This is used to allow for blank footnotes

\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhf{}
\fancyhead[L]{\vspace{-15mm}\fontsize{10pt}{10pt}\selectfont{iConference 2015} }%This is the header
%\fancyhead[R]{\vspace{-15mm}\fontsize{10pt}{10pt}\selectfont{[Name of the first author] et. al (if two, write both)} }
\fancyhead[R]{\vspace{-15mm}\fontsize{10pt}{10pt}\selectfont{Peter Organisciak and Michael Twidale} }
\fancyfoot[C]{\thepage} % Custom footer text

\usepackage{tocloft} % Change List of Figures / Tables
\renewcommand{\cftfigpresnum}{Figure }
\setlength{\cftfignumwidth}{5em}
\renewcommand{\cfttabpresnum}{Table }
\setlength{\cfttabnumwidth}{5em}
\renewcommand\cftloftitlefont{\sffamily \Large}
\renewcommand\cftlottitlefont{\sffamily \Large}

%----------------------------------------------------------------------------------------
%        TITLE SECTION
%----------------------------------------------------------------------------------------

\title{Design Facets of Crowdsourcing}
%----------------------------------------------------------------------------------------

\date{} % no date

% Formatting title and authors.
\makeatletter
\renewcommand{\maketitle}{\bgroup\setlength{\parindent}{0pt}
\begin{flushleft}
{\sffamily \Large {\@title} }
\vspace{12pt}\\
\@author
\end{flushleft}\egroup
}
\makeatother

%----------------------------------------------------------------------------------------
% AUTHORS
%----------------------------------------------------------------------------------------

\author{%
{\large Peter Organisciak$^{1}$, Michael Twidale$^{1}$\\} % Enter authors.
$^{1}$Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign\\ % Enter affiliations.
}

%----------------------------------------------------------------------------------------



\begin{document}

\newenvironment{blockquote}{\list{}{\leftmargin=0.5in\rightmargin=0.0in}\item[]}{\endlist}
\renewcommand{\listfigurename}{Table of Figures}
\renewcommand{\listtablename}{Table of Tables}
\maketitle % Insert title
\thispagestyle{empty} % First page has no header,footer

%----------------------------------------------------------------------------------------
%        INFOBOX
%----------------------------------------------------------------------------------------

\begin{center}
\begin{tabularx}{\textwidth}{|X|}
\hline
\vspace{2pt}\\
\textbf{Abstract}\\
Crowdsourcing offers a way for information scientists to engage with the public and potentially collect valuable new data about documents. However, the space of crowdsourcing is very broad, with many design choices that differentiate existing projects significantly. This can make the space of crowdsourcing rather daunting.\\
Building upon efforts from other fields to conceptualize, we develop a typology of crowdsourcing for information science. Through a number of dimensions within the scope of motivation, centrality, beneficiary, aggregation, type of work, and type of crowd, our typology provides a way to understand crowdsourcing.
\\
{\footnotesize \textbf{Keywords:} crowdsourcing, online communities, social computing }\\
{\footnotesize \textbf{Citation:} Editor will add citation.}\\
{\footnotesize \textbf{Copyright:} Copyright is held by the authors.}\\
{\footnotesize \textbf{Contact:} organis2@illinois.edu}\\ % Please enter at least one contact e-mail address.
\vspace{2pt}\\
\hline
\end{tabularx}
\end{center}

%----------------------------------------------------------------------------------------
%        ARTICLE CONTENTS
%----------------------------------------------------------------------------------------
\section{Introduction}

Crowdsourcing is a production model where many people, often non-professionals, contribute to a common product. The pattern is potentially very useful for information science, because the ability for large numbers of diverse people to react to and enrich information items offers new ways to represent them.

The scope of crowdsourcing is broad, however, and the myriad approaches to collaboration among distributed crowds lend a lack of coherence which may intimidate a practitioner. In order to address this, we present a typology of crowdsourcing for information science.

Crowdsourcing, the collaboration of distribution contributors on a common product, promises value to library and information science in a variety of ways. Information systems and digital repositories deal with overwhelming amounts of materials that can be manageably annotated with help from many hands, and the relationship that cultural heritage collections hold with their audience can potentially be strengthened by pursuing meaningful collaboration between the two. Holley \citeyear{holley_crowdsourcing:_2010} notes some potential uses to crowdsourcing, including tapping into the expertise of the community, building loyalty of users while tapping into their altruistic tendencies, adding value to data such as with quality ratings, and improving information access to materials.

There have been earlier attempts at crowd taxonomies, such as 
\cite<e.g.,>{geiger_managing_2011, vukovic_towards_2010, schenk_crowdsourcing:_2009, rouse_preliminary_2010}. However, these have primarily emerged from other domains, with a focus on economic or quantitative variables. Perhaps the most valuable prior work is in Quinn and Bederson's taxonomy of human computation \citeyear{quinn_human_2011}, a field focusing on humans performed work in the mode of computing. Human computation often overlaps with crowdsourcing but focuses on a more narrow type of labour and is not necessarily performed by distributed crowds.

This work collects and builds on prior studies of crowdsourcing, including earlier typologies, while offering new facets which aid in a more human-centric treatment of crowdsourcing.

\section{Crowdsourcing}

Crowdsourcing is broad term referring to the collaboration of large numbers of distributed people on a common product, usually organized through information technology and generally invited through an open call.

The term was coined in 2006 \cite{howe_rise_2006}, initially scoped to business uses of the term but quickly co-opted to its current, wide-ranging usage \cite{howe_birth_2006}. There are a number of concepts under the umbrella of crowdsourcing that have been studied individually, including:

\begin{itemize}

\item \textbf{Free and Open Source Software (FOSS)}. The FOSS movement started with the sharing of software source code for interested parties, but underwent a sea change with the popularity of Linux in the early nineties, which heartily accepted code contributions and bug `fixes from whoever had the talent to make them \cite{raymond_cathedral_1999}. This model of distributed collaboration showed that, with the right communication tools, crowds had the capacity to work more intelligently than traditionally given credit for \cite<e.g.,>{mackay_memoirs_1852, le_bon_crowd:_1896}. The journalist that introduced the term 'crowdsourcing' offers one definition as "The application of Open Source principles to fields outside of software" \cite{howe_crowdsourcing:_2006}.

\item \textbf{Commons-based peer production}. In \emph{Wealth of Networks}, Benkler takes a political economy view on what he calls the networked information economy, arguing that its properties of increasingly low-cost access and efficient organization of communities would empower the commons \cite{benkler_wealth_2006}. He discusses commons-based peer production as an alternative to firms for economic and cultural value creation. Earlier, von Hippel introduced \emph{user innovation}, predicting a similar effect of technology: a shift of innovation away from those selling products to those using products \cite{von_hippel_sources_1988,von_hippel_democratizing_2006}.

\item \textbf{Citizen science}. Citizen science refers to collaboration between scientific communities and members of the public on research. Early crowdsourcing projects, such as galaxy annotation site \emph{Galaxy Zoo} and protein-folding competition \emph{FoldIt}, were noted as a form of citizen science, and crowdsourcing has been used for numerous successful results in the field. \citeA{wiggins_goals_2012} present a typology of citizen science projects, binning them into action-oriented, conservation-focused, investigative, wholly virtual, and educational projects.

\item \textbf{Wisdom of the crowds}. \emph{The Wisdom of the Crowds} \cite{surowiecki_wisdom_2004} observed the collected effectiveness of crowds when properly aggregated. Building from Francis Galton's \emph{Vox Populi} \citeyear{galton_vox_1907}, where Galton aggregated guesses at a steer weight guessing competition and found that the median guess was more accurate that any individual guess, Surowiecki argues that the ability of many autonomous amateurs to aggregate into a product comparable to something an expert would produce has important ramifications on the internet. For example, the wisdom of the crowds is utilized in crowdsourcing opinions (e.g. product reviews on Amazon, film reviews on Netflix) and in filtering (e.g. liking of starring posts on a social network).

\item \textbf{Human computation}. Human computation was introduced in the doctoral work of von Ahn, accompanying work on the \emph{ESP Game}, a game where the players tag online images during the course of playing \cite{ahn_labeling_2004, von_ahn_games_2006}. Human computation refers to work performed by humans in the style of computation: the "mapping of some input representation to some output representation using an explicit, finite set of instructions" \cite{law_human_2011}. \citeA{quinn_human_2011} offer a taxonomy of human computation that overlaps with our treatment of crowdsourcing in places, classifying along dimensions of motivation, quality control, aggregation, human skill, process order, and task-request cardinality. While many human computation projects are also crowdsourcing, neither is fully encapsulated by the other. The paradigm of computation in human computation is just a subset of ways that crowds can collaborate in crowdsourcing, and human computation can be performed without the modality of multiple collaborators seen in crowdsourcing.

\end{itemize}

What does crowdsourcing look like? One particularly popular system is \emph{Wikipedia}, a crowd-written online encyclopedia. Wikipedia is written, policed, and managed by a mostly volunteer crowd, and in most cases accepts page edits from anybody, regardless of credentials or status. An example of more skilled parallel to Wikipedia is \emph{Suda Online}, a project translating and annotating a Byzantine Greek encyclopedia since 1998.

Other crowdsourcing projects, especially those in library and information science, involve encoding tasks that are less open-ended than simply writing. \emph{Old Weather} is one such example, where contributors encode weather data from scans of old ship logs. \emph{What's on the Menu} pursues a similar transcription task, of menu items from scanned restaurant menus. In both these cases, the act of participating allows contributors to interact with novel, interesting archive materials.

Not all crowdsourcing uses involve new systems: sometimes valuable contributions are gathered incidentally. For example, the Library of Congress found that user comments on archival photos provided a valuable source of additional information about the holding \cite{springer_for_2008}, and steve.museum has researched the ability of free-text tagging for providing descriptive keywords more inline with the language which information-seeking individuals use \cite{trant_investigating_2006}.

In addition to the aforementioned typology of citizen science \cite{wiggins_goals_2012} and taxonomy of human computation \cite{quinn_human_2011}, attempts to provide the same for crowdsourcing have been attempted \cite{geiger_managing_2011, vukovic_towards_2010, schenk_crowdsourcing:_2009, rouse_preliminary_2010}.

\citeA{geiger_managing_2011} identify crowdsourcing processes by four defining characteristics: the pre-selection process for contributors, the accessibility of peer contributions, the aggregation of contributions, and the form of remuneration for contributors. While these are all valid ways of viewing crowdsourcing, more qualitative or naturalistic models are also necessary in order to understand crowdsourcing websites, such as motivation or centrality.

\citeA{schenk_crowdsourcing:_2009} provide a management science view on crowdsourcing, with a typology along two dimensions. First, crowdsourcing is distinguished by how work is collaborated on: in an integrative or selective manner. Secondly, the type of work that is performed is faceted into routine, complex, and creative tasks. \citeA{vukovic_towards_2010} take yet another frame, of business-centric crowdsourcing uses. Crowd type, incentives, quality assurance, government and legal, and social factors play into a parsing of crowdsourcing in this scope. Finally, \citeA{rouse_preliminary_2010} propose a taxonomy based on the nature of the crowdsourcing, focusing on capabilities (simple, moderate, sophisticated), benefits (community, individual, or mixed), and motivation. Their hierarchical taxonomy notes motivations relative to the other two conditions.

\section{An Information Science Typology of Crowdsourcing} \label{taxonomies-of-crowdsourcing}

The space of crowdsourcing is large, and there have been a number of attempts to organize the sub-concepts within it or to reconcile it in a space alongside other areas of research. Some of the most important questions in differentiating crowdsourcing include:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item Who are the contributors? What are their skills?
\item How are contributors motivated? Are they paid or do they volunteer for other incentives?
\item Are contributions new, or do they react to existing documents or entities?
\item Are contributions preserved in the form they are submitted, or are they combined into a larger contribution?
\item What do the contributions look like? Are they subjective (involving opinions or ranking) or objective (there is an agreed upon best response)?
\item Who is asking for the contributions? Who is benefiting?
\item Is the collaboration indirect (i.e.~contributors work on parts independently) or manifest?
\item Is the crowdsourcing central to the system?
\item How is quality controlled for?
\end{itemize}

Table 1 provides an overview of our crowdsourcing typology, including references when the dimensions are influenced closely by prior work. In the next section, we consider existing work more thoroughly, adapting it into our typology, explain how we reinterpret it, and argue for new facets not present in non-IS taxonomies or classifications.

\begin{table}[h]

\begin{tabular}{p{3cm}p{6cm}p{6cm}} %This sets the number of columns, one l per column
\toprule
Category &
Description &
Sub-categories \\
\midrule


Motivation &
How are contributors incentivized? &
Primary/Secondary \cite{organisciak_why_2010}, Contribution/commitment \cite{kraut_building_2011} \\
&
&
Extrinsic/Intrinsic \\
\midrule


Centrality &
How central is the crowdsourcing to the overall project? &
Core / Peripheral \cite{organisciak_incidental_2013} \\
\midrule


Beneficiary &
Who benefits? What is their relationship to contributors? &
Autonomous / sponsored \cite{zwass_co-creation:_2010}\\
&
&
Crowd / individual \\
\midrule


Aggregation &
How are diverse contributions reconciled into a common product? &
Selective /Integrative \cite{geiger_managing_2011, schenk_crowdsourcing:_2009}\\
&
&
Summative / Iterative / Averaged \\
\midrule


Type of Work &
What is the nature of the work? &
Human computation / Creative \\
&
&
Generative / Reactive \\
&
&
Subjective / Objective \\
\midrule


Type of Crowd &
What are the dimensions of the crowd and how they are expected to perform? &
Unskilled, locally trained, specialized \\
&
&
heterogenous / diverse \\
\bottomrule
\end{tabular}
\caption{Overview of facets in our crowdsourcing typology} %This sets the table's title
\end{table}

\subsection{Motivation}\label{motivation}

The incentives for contributors to participate in crowdsourcing are complex and not always consistent from contributor to contributor.

\subsubsection{Intrinsic / Extrinsic
Motivation}\label{intrinsic-extrinsic-motivation}

Motivation in crowdsourcing follows related work in the motivations of humans in general
\cite{maslow_theory_1943,alderfer_empirical_1969,ryan_intrinsic_2000}.
While a review of that work is beyond the scope of this paper, many views of crowdsourcing motivation adopt the lens of motivation as a mixture of \emph{intrinsic} factors and \emph{extrinsic} factors \cite{ryan_intrinsic_2000}. In the former, fulfillment is internal to the contributor, psychologically motivated, while in the latter the rewards are external.

The spectrum of intrinsic to extrinsic motivators is commonly paralleled in crowdsourcing literature through a dichotomy of paid and volunteer crowdsourcing\cite{rouse_preliminary_2010,geiger_managing_2011,kraut_building_2011,schenk_crowdsourcing:_2009}.

Paid and volunteer crowdsourcing are not exclusive, and there are extrinsic motivators beyond money. However, this separation is common because of it accounts for some of the starkest differences between how crowdsourcing is implemented and motivated. There are differing design implications around people being paid and performing work for other reasons: money is a direct currency for obtaining labor, while convincing volunteers to contribute requires a greater sensitivity of their needs and ultimately more complexity in engineering the
crowdsourcing system.

It has been shown that intrinsic motivation still plays a part in paid crowdsourcing \cite{mason_financial_2010}, and some systems mix intrinsically motivated tasks with payment or the chance at remuneration. For example, some contest-based marketplaces are popular among users looking to practice their skills, such 99Designs for designers or Quirky for aspiring inventors.

Some taxonomies make a distinction between forms of payment. \citeA{geiger_managing_2011} makes the distinction between fixed remuneration, with a pre-agreed fee, and success-based remuneration, such as contest winnings or bonus.

\subsubsection{Specific Motivators}\label{specific-motivators}

Taxonomies of specific motivators seen in crowdsourcing have been previously attempted, with varying results that touch on similar issues. \citeA{organisciak_why_2010} identified a series of primary and secondary motivators from a diverse set of crowdsourcing websites. We adopt the categories from that study for our typology, as related work is accommodated well.

\emph{Primary motivators} are those that are considered critical parts of a system's interaction. Systems do not require all of them, but to attract and retain contributions, they need one or more of them. In contrast, \emph{secondary motivators} are system mechanics that generally were not observed as necessary components of a system, but were elements that encourage increased interaction by people that are already contributors. \cite{kraut_building_2011} parallel the primary/secondary split by differentiating between encouraging contributions and encouraging commitment.

The motivators in \cite{organisciak_why_2010} were observed from a content analysis of 13 crowdsourcing websites and subsequent user interviews. For sampling, 300 websites most commonly described as `crowdsourcing' in online bookmarks were classified with a bottom-up ontology, then the 13 final sites were selected through purposive stratified sampling, to represent the breadth of the types of crowdsourcing seen. These cases were studied in case studies followed by user studies.

Below is a list of primary motivators seen in \citeA{organisciak_why_2010}, but also paralleled and supported by the similar broad view social study published by \citeA{kraut_building_2011}.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt

\item \textbf{Money and extrinsic reward}. Paying crowds is the most reliable approach for collecting contributions, and is an option in the absence of other motivators or where certainty is required. However, it also introduces bottlenecks of scale, and negates some of the benefits of intrinsic motivation. \citeA{mason_financial_2010} note that, while intrinsic motivation still exists on paid crowdsourcing platforms, it is overwhelmed when tasks are too closely tied to reimbursement, resulting in contributions that are done minimally, briskly, and with less enjoyment. \citeA{kraut_building_2011} point to psychology research that shows the ability of reward in other settings to subvert intrinsic motivation, leading to less interested contributors.

\item \textbf{Interest in the Topic}. Projects catering to people that have a pre-existing interest in their subject matter or outcomes tend to get longer, more consistent engagement. For example, the Australian Newspaper Digitisation Project (now part of a larger project called \emph{Trove}) found that that amateur genealogists, with pre-existing communities and a willingness to learn new technologies, took ``to text correction like ducks to water'' \cite{holley_many_2009}.
Similarly, Galaxy Zoo found similar success with amateur astronomers helping annotate galaxies. Kraut and Resnick likewise argue that asking people to perform tasks that interest them results in more engagement than asking people at random.

\item \textbf{Ease of entry and ease of participation}. Low barriers to entry and participation were cited by every user interviewed in \citeA{organisciak_why_2010}. Wikipedia has a low barrier to entry but its interface and demanding community standards have been criticized in recent years for raising the barrier to participation \cite{angwin_volunteers_2009,sanger_fate_2009}. ``Simple
requests'' generally lead to more productive contributions, according to \citeA{kraut_building_2011}.

\item \textbf{Altruism and Meaningful contribution}. People like to help if they believe in what they're helping. Writing about Flickr Commons, Library of Congress noted that they ``appear to have tapped into the Web community's altruistic substratum by asking people for help.
People wanted to participate and liked being asked to contribute'' \cite{springer_for_2008}. With Galaxy Zoo, the appeal for many contributors that that it offers a tangible way to contribute to real science. \citeA{rouse_preliminary_2010} also argues for altruism's place in a taxonomy of crowd motivation. \citeA{kraut_building_2011} argue that appeals to the value of a contributions are more effective for people that care about the domain.

\item \textbf{Sincerity}. ``People are more likely to comply with requests the more they like the requester,'' \citeA{kraut_building_2011} note. A recurring theme among interview participants in \citeA{organisciak_why_2010} was whether a project seems sincere or exploitative. Since crowd contributions often exist as a parallel to labour, crowds are often weary of anything that smells like them being taken advantage of.

\item \textbf{Appeal to knowledge and opinions}. One curious source of motivation is simply asking the right people. Online visitors presented with a question are often compelled to answer it simply because they know the response, be it part of their knowledge, skills, circumstance, or opinions. The `appeal' itself can be explicit or implicit. \cite{kraut_building_2011} refer to this sort of appeal as ``Ask and Ye Shall Receive'', asserting that online communities stand to benefit from easily accessible lists of what work
needs to be done. They also assert that direct requests for contribution are better than broadcast. \end{itemize}

One motivator overlooked in \citeA{organisciak_why_2010}  is \emph{novelty}. Novelty or curiosity is ephemeral and unsustainable, but nonetheless a unique idea can attract contributions for a short amount of time. \citeA{kraut_building_2011} also note structure, goals, and deadlines as incentives. Such an effect is strongly felt on Kickstarter, where the tenor of crowdfunding for projects changes relative to the funding end date.

The supplemental secondary motivators based on \citeA{organisciak_why_2010}, which encourage more engagement but not initial contribution, are:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item \textbf{External indicators of progress and reputation}. Using games, badges, or leaderboards encourages more contribution among certain people. An important caveat is that this form of performance feedback needs to be perceived as sincere \cite{kraut_building_2011}. 

\item \textbf{Feedback and impression of change}. Showing the contribution in the system or conveying how it fits into the whole. \cite{kraut_building_2011} tie feedback to goals, emphasizing the importance of showing progress relative to personal or site-wide
goals.

\item \textbf{Recommendations and the social}. Prodding by friends, colleagues, and like-minded individuals. Simply seeing that other people have contributed makes a person more likely to contribute \cite{kraut_building_2011}. This motivator factors into the taxonomy by \citeA{rouse_preliminary_2010} as \emph{social status}.

\item \textbf{Window fixing}. Nurturing a well-maintained community where the members are compelled to support it's health.
\end{itemize}

\subsection{Centrality}\label{centrality}

How central, or necessary, is the crowdsourcing to the task at hand? Is
it \emph{peripheral}, or \emph{core}?

The work in \citeA{organisciak_incidental_2013} tried to counterbalance a perceived focus on whole-hog crowdsourcing -- the large, highly novel initiatives like Wikipedia -- by introducing
\emph{incidental crowdsourcing}. Incidental crowdsourcing focused on types of crowdsourcing -- like rating, commenting, or tagging -- that are peripheral and non-critical. The shift to an incidental mode brings with it its own design tendencies, such as lower bandwidth forms of contribution and fallback strategies for low engagement cases.

This distinction between peripheral and core is important to an information science treatment of crowdsourcing. It shows that the benefits of crowdsourcing are not only attainable by those with the infrastructure and resources to commit to a new large system. It can be an augmentative feature, that engages with users and accepts useful feedback from them in addition to a non-crowdsourcing primary objective. Peripheral crowdsourcing also often accompanies a pattern of reacting to existing information objects, pertinent to those that deal with museum repositories or digital libraries.

\subsection{Aggregation}\label{aggregation}

\citeA{schenk_crowdsourcing:_2009} and \citeA{geiger_managing_2011} discuss two types of aggregation: \emph{integrative} and \emph{selective}. Integrative aggregation pools contributions into a common product, like a wiki, while selective aggregation tries to choose
the best contributions, such as in contests.

This simple separation hides some of the complexity seen in aggregation approaches. Reconciling multiple different contributions can be difficult, and integrative aggregation can be approached in a number of ways. We argue the following finer views on integrative aggregation are useful:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item \textbf{Summative}. In summative aggregation, people contribute to an ever-expanding base of information. Contributions are clearly part of a bigger whole, but their individual form is retained. For example, with online reviews, each individual contributor writes their own review with their own interpretation of the given product, movie, travel destination, etc.; at the same time, the collection of reviews forms a more comprehensive document of people's attitudes.

\item \textbf{Iterative}. In versioned aggregation, multiple contributions are used toward a larger product, but the contributions are permutations of a common work. For example, with collaboratively written wikis, such as Wikipedia, each user's iterates on the work of all the previous writers of the page.

\item \textbf{Averaged}. In averaged aggregation, contributions are still pooled, but a consensus-seeking process tries to reconcile them. Our use of \emph{averaged} here alludes to quantified consensus seeking, even when it is not simply a case of derive a statistical mean. With contributions such as opinion ratings of information objects the process might be to average; with multiple-keyed classification, the aggregation process may be a vote majority; with starring (sometimes referred to as favoriting, liking, or recommending), the averaged aggregation may simply show the number of people that have performed the action.
\end{itemize}
A consideration related to aggregation is that of quality control, something other typologies have considered as a top-level dimension in its own right. \citeA{quinn_human_2011} consider how the system protects against poor contributions, such as reputation systems, input or output agreement, multi-contribution redundancy, a crowd review workflow, expert review, and designs that disincentive poor quality or obstruct the ability to do so. \citeA{vukovic_towards_2010} likewise look at quality assurance, noting the large focus on improving quality for quantifiable contributions.

In our typology, we consider quality as a best practice issue that follows from how users are aggregated. With summative aggregation, for example, quality is often pursued by a separate crowdsourcing step: allowing online visitors to flag low-quality or otherwise problematic contributions. Other times, such as with question and answer websites \emph{Stack Overflow} or \emph{Quora}, visitors vote on the quality of answers to surface the best ones. With iterative contributions, peer review is sometimes used, as in the versioned workflow of many open-source projects or with the concept of watching pages and reversions on Wikipedia. As noted, averaged aggregation recieves a lot of focus because it lends itself to quantification, and numerous studies focus on the quality increases of adding redundant contributors or methods to identify low-quality contributors \cite<e.g.,>{sheng_get_2008, snow_cheap_2008, welinder_online_2010, wallace_who_2011}.

\subsection{Director / Beneficiary}\label{director-beneficiary}

Who directs the crowdsourcing activities and who benefits from the contributions?

Considering the director of a crowdsourcing task, \cite{zwass_co-creation:_2010} distinguishes between \emph{autonomous} and \emph{sponsored} forms of crowdsourcing. \emph{Sponsored} crowdsourcing is when there is a entity at the top soliciting the contributions: a client of sorts. In contrast \emph{autonomous} crowdsourcing serves the community itself. Autonomous crowdsourcing can be in a centralized location, like a community-written wiki or video-sharing website, or exist loosely, as in blogs.
\cite{zwass_co-creation:_2010} explains: ``Marketable value is not necessarily consigned to the market---it may be placed in the commons, as is the case with Wikipedia.''

Considering the soliciting party as a case of sponsorship or autonomy is useful, though a further distinction should be made between the collective (the \emph{crowd}) and the individual (the \emph{contributors}). Crowds collaborate toward a shared goal, as with Wikipedia or certain kinds of open-source software development, while individuals are more self-motivated. For example, in citation analysis through web links, as was done with PageRank \cite{page_pagerank_1999}, the large-scale benefits of the crowds are unrelated to what the individuals creating the links are thinking. \citeA{rouse_preliminary_2010} offers a similar designation in the beneficiary, between individual, crowd, and a mix of the two.

One way to view this relationship between contributor and director is in light of effort against benefit. Do both director and contributor benefit (symbiosis)? Does one benefit at the expense of the other (parasitism)? Or is it a case of commensalism, where both benefit
but in mutually exclusive ways.

\subsection{Type of Work}\label{type-of-work}

The type of work performed by crowds can vary greatly in its complexity
and style.

\subsubsection{Human Computation vs.~Creative}

One notable form of crowdsourced work is represented by the concept of human computation, where ``the problems fit the general paradigm of computation, and as such might someday be solvable by computers'' \cite{quinn_human_2011}. Understanding that crowdsourcing is not
solely human computation tasks, the inferred corollary to these types of tasks are those that are expected to be too complex for computers: creative, judgment-based, or requiring critical thinking. Creative crowdsourcing might take the form of artistic human expression, such as
online contributors collectively animating a music video (\emph{Johnny Cash Project}) or the sum of YouTube. Opinion or judgement-based crowdsourcing often does not have a definitive answer, and is seen in areas such as movie reviews or product ratings. More complex critical thinking tasks do not fit the paradigm of computation and are much more complex, such as Wikipedia or protein-folding project FoldIt.

\citeA{schenk_crowdsourcing:_2009} have previously distinguished between three types of crowdsourcing. First are routine tasks, such as crowdsourcing of OCR text correction with ReCaptcha. The majority of human computation tasks would likely fall within this category of rote tasks. Second are complex tasks, such are open-source software development. Finally, they suggest creative tasks, with a slightly different meaning than our typology's usage as a disjunct to human computation. An example of their final category would be a system like \emph{MyStarbucksIdea}, a space where people suggest changes they would like to see at the coffee chain Starbucks. Since \citeA{schenk_crowdsourcing:_2009} focus on crowdsourcing when there is a client, usually a corporate client, they do not consider the wider space of creative crowdsourcing tasks.

\subsubsection{Generative vs.~Reactive}
Another view that touches on the nature of the contribution is \emph{generative} versus \emph{reactive}. In the former, new intellectual products are created. With reactive work, the work is a reaction or interpretation of an existing information object: reviews, ratings, encoding.

Such a distinction is neglected in most views of crowdsourcing, but important in information science. At the heart of many projects in our community, such as those by libraries, museums, and cultural heritage institutions, is a focus on information objects. There is much effort expended in archiving, enriching, appreciating, and sharing works, and a reactive view of crowdsourcing products places the public within this tradition.

\subsubsection{Subjective vs.~Objective Crowdsourcing}\label{subjective-vs.objective-crowdsourcing}

Another parallel being drawn in recent years is that of objective or subjective crowdsourcing tasks.

Objective tasks are assumed to have an authoritative truth, even if it is unknown. For example, in transcribing scanned texts, it is assumed that there is a `correct' passage in the work that has been scanned.

In contrast, subjective tasks have a variable concept of correctness, as they are are not expected to be consistent between contributors.

Human computation undertakings are commonly objective tasks, and taxonomic efforts for human computation -- such as \citeauthor{schenk_crowdsourcing:_2009}'s split of routine, complex, and creative  \citeyear{schenk_crowdsourcing:_2009} -- do not touch on the subjective/objective separation in a direct way.

The subjective-objective distinction has consequences for training and quality control. Objective tasks lead to a training approach where the ideal result is that everyone performs the task in the same one right way. Quality control on those tasks can employ approaches such as interrater reliability, since it can be assumed that there is an object result to be reliable about. Subjective tasks can still need training and quality control, but it will necessarily be of a different kind. For example, certain subjective tasks want to take advantage of the diversity of human activity and so explicitly do not want everyone to do the same thing in the same way.

This distinction is still present with different forms of aggregation. Multiple contributions can be aggregated with an objective assumption, expecting a truth and deviations from it as bad work or data. Other systems try to aggregate a normative opinion or judgment of subjective contributions. This latter assumption is seen often in opinion ratings, such as film or restaurant ratings: just because there is an aggregated rating presented, there is an understanding that some people might disagree and that they are not incorrect for doing so.

\subsection{Type of Crowd}\label{type-of-crowd}

\citeA{vukovic_towards_2010} define two extremes of crowd types: \emph{internal} and \emph{external}. Internal crowds are composed solely of contributors from the organization that is crowdsourcing, if it is thus centralized. External crowds are members outside of the institution. \citeauthor{vukovic_towards_2010} also note that \emph{mixed} crowds are observable.

\subsubsection{Necessary Skills}\label{necessary-skills}

A point of separation between crowd methods is the skills required to perform the work. \emph{Unskilled}, \emph{locally training}, and \emph{specialized} are all seen among crowdsourcing systems. Where unskilled labour encourages contributions from anybody at anytime, systems that use methods for authority control leave certain tasks to
long-term, involved contributors. For example, on question and answer service \emph{Stack Overflow}, a user's administrative ability grows more open as they contribute more to the management of the system, a way of ensuring that those users have learned the proper management of the site.

\subsubsection{Diversity}\label{diversity}

In additional to what the crowd is, there is a distinction to be made on what the crowd is desired to be. Here, it is helpful to think of a spectrum between \emph{diverse} and \emph{homogeneous} crowds. In some cases, the crowdsourcing task benefits from multiple unique viewpoints. When online players compete to fold proteins in the most efficient way possible for \emph{FoldIt}, the project's success is predicated on the ability of people to problem-solve in variable ways. In contrast, for a project like \emph{Building Inspector} where participants outline building boundaries from scanned survey records, the desire is for the participants to perform in a standard way. Here, reliability and consistency are important traits.

\subsection{Common Design Patterns}\label{common-design-patterns}

A number of design patterns have been established and repeated in crowdsourcing, some organically and some, like the ESP Game, carefully engineered. These include:

\emph{Microtasking}. the concept of splitting a large task into many smaller parts to be worked on by different people was an important tide change in the history of open-source software \cite{raymond_cathedral_1999}, and the same models have been emulated in crowdsourcing. With so-called `microtasks', the overhead to participation is low, and the pressure or dependence on any one contributor is low.

\emph{Gamification}. Gamification is predicated on a reframing of what would traditionally be labour into a game-like or leisurely tasks. Gamification follows in the philosophy, as Twain wrote, ``that work consists of whatever a body is obliged to do, and that play consists of
whatever a body is not obliged to do '' \citeyear{twain_adventures_1920}. 
The ethics of gamification have been argued for as an extension of contributors' desire to perform meaningful work. Shirky, for example, argues that people have a `cognitive surplus' to give during their leisure time, a desire to spend their free time doing useful, creative or stimulating tasks \citeyear{shirky_here_2009}. Gamification is an extension of serious games -- games meant to do more than simply entertain \cite{abt_serious_1987, michael_serious_2005, ritterfeld_serious_2010}. In areas of crowdsourcing and human computation, Games with a Purpose \cite{von_ahn_games_2006} is an extension of serious games in the context of distributed, collaborative crowds. Harris and Srinivasan \cite{harris_applying_2012} consider the applicability of applying games with a purpose to various facets of information retrieval, concluding it is a feasible approach for tasks such as term resolution, document classification, and relevance judgment. \citeA{eickhoff_quality_2012} have investigated the gamification of relevance judgements further, augmenting the financial incentive on paid crowdsourcing platforms.

\emph{Opinion Ratings}. A standard and highly familiar activity online is soliciting qualitative judgments from visitors. These ratings have different granularities, most commonly 5-level (e.g.~1 to 5 stars) or binary (e.g.~thumbs up/thumbs down). Unary judgments have grown in popularity as ways of showing support with minimal effort. Their popularity seems to stem from when social network \emph{Friendfeed} implement a unary voting button labelled, succinctly, ``I like this'' \cite{taylor_friendfeed_2007} and subsequently when similar wording was adopted by Facebook after acquiring Friendfeed.

\emph{Platforms}. There is a cottage industry of services that offer the infrastructure for requesters to crowdsourcing, using in domain-specific ways. For example, \emph{Kickstarter} and \emph{Indiegogo} ease crowdfunding, \emph{99Designs} enables contest-based design tasks, and Mechanical Turk offers the tools and people for microtasks.

\emph{Contests}. In the contest design pattern, a requester offers a bounty to the best solution to a problem or task of their choosing, such as in design (e.g.~99Designs), coding (e.g.~/TopCoder/), and research and development (e.g.~Innocentive). Here the ``crowdsourcing'' is simply using internet to connect to many potentially talented individuals, though contests have been integrated into more collaborative workflows.
For example, with the collaborative product incubator Quirky, the community votes on the best ideas to develop into products, discussing how to improve the ideas openly. One reason for this may be that, in addition to the large portion of future profits that an idea originator may earn if it is voted into development, the rest of the community also receive points for supporting the best ideas.

\emph{Wisdom of crowds}. Wisdom of the crowds is a design pattern which emphasizes the effectiveness of human judgment in aggregate \cite{surowiecki_wisdom_2004}, provided the participants are rationally organized. This is embodied by multiple-keying for tasks which are expected to have a real answer, such as classifying galaxies,
or averaging opinions for subjective tasks to derive a normative judgment.

\section{Practitioner's questions}

Below we offer examples in the wild to aid crowdsourcing planning using our typology.

\bigskip
\noindent \textbf{Q:} \emph{Are you augmenting existing data, which already exists in an online system or repository or which is appropriate for presentation already?}
\begin{itemize}
\renewcommand{\labelitemi}{$\Rightarrow$}

\item \emph{Yes.} Peripheral crowdsourcing is an option to consider, because it collects information from people that are already interested in the content and consuming it. \emph{Trove} does this with newspaper scans: visitors can read the scans and the poor computer transcription, but are also given an option to fix the transcriptions.

\item \emph{No.} Core crowdsourcing requires more technical overhead, but results in some of the most exciting examples of crowdsourcing. \emph{Old Weather} or \emph{Transcribe Bentham} show how archives can engage with interested members of the public, while arguably providing a strong form of material appreciation than passive reading would offer.

\end{itemize}

\noindent \textbf{Q:} \emph{Does your data compile, iterate, or combine?}

\begin{itemize}
\renewcommand{\labelitemi}{$\Rightarrow$}

\item \emph{Compile.} Summative aggregation is seen in digital history projects like \emph{Make History}, a 9/11 Memorial Museum project compiling people's photos and stories of the 9/11 terrorist attacks. However, simpler crowdsourcing mechanics, such as commenting and tagging on Flickr's \emph{The Commons}, also follow this pattern.

\item \emph{Iterate.} Digital archive transcription projects such as \emph{Transcribe Bentham} work with the model of iteration. One concern with these forms of projects is that contributors sometimes do not want to conflict with a previous author; a way to encourage iteration is to mark unfinished pages and discourage single edit perfection, as is done on Wikipedia. 

\item \emph{Combine.} Information science has a tradition of considering averaged aggregation in the context of multiple-coder classification. For an example of a novel, notably low-tech version of this pattern, \cite{simon_participatory_2010} writes of voting bins at the exit of the Minnesota Historical Society's History Center. Visitors, who are given pins to show they have paid admission, can vote on their favourite exhibits by disposing of the pins in one of a set of labeled containers.

\end{itemize}

\noindent \textbf{Q:} \emph{Can your data be collected while contributors work for themselves?}

\begin{itemize}
\renewcommand{\labelitemi}{$\Rightarrow$}

\item \emph{Yes.} Social OPACs like Bibliocommons collect various user-generated metadata about materials, such as tags or comments. A study into two such systems found that the features are generally underutilized, but are most popular in cases where participants are creating things for themselves: compiling list bibliographies, personal collection bibliographies, or use a "save for later" feature \cite{spiteri_social_2011}.

\end{itemize}

\noindent \textbf{Q:} \emph{Does your project have any primary motivators to incentivize contributions, such as an existing community of interest or a compelling, easy to answer question?}

\begin{itemize}
\renewcommand{\labelitemi}{$\Rightarrow$}

\item \emph{Have primary motivators.} Systems such as Galaxy Zoo or Trove provide examples of how a system can emphasize the incentives they offer to potential volunteers. Most of the successful projects noted in this study offer some of the primary motivators.

\item \emph{Don't have motivators.} For trickier or less intrinsically interesting data, it is possible to hire on-demand workers through a platform like \emph{Mechanical Turk}. Examples of efficient routing on these sorts of systems include \emph{Soylent} -- crowdsourced writing assistance \cite{bernstein_soylent:_2010} -- and \emph{VizWiz}, an accessibility application that allows visually impaired users to receive transcribed descriptions of photos that they take \cite{bigham_vizwiz:_2010}.

\end{itemize}

%Are your potential contributors skilled, do they need to be taught, or can anybody contribute?
%Skilled:
%Taught:
%Open:

\section{Conclusion}

Crowdsourcing offers potential for involving the public and improving data in digital libraries and cultural heritage repositories. However, the scope of crowdsourcing is so large and the implementation possibilities so varied that it can seem rather daunting to pursue it. 

In this paper we attempt to provide a way of making a bit more sense of the patterns that emerge when considering these projects not so much from the perspective of what they are for (eg rating books, movies or restaurants versus citizen science or digital humanities) but rather in terms of how they were designed to achieve particular ends.

We consolidate a number of past taxonomies of crowdsourcing and project examples to develop a typology of crowdsourcing for information science. In addition to modifications on previously studied dimensions such as motivation, aggregation, and beneficiary, we also offer new dimensions regarding centrality of crowdsourcing, the diversity needs of the crowd, and the dichotomy of generative or reactive types of work. This typology offers a framework to making sense of the differences between crowdsourcing projects and thinking through practical possibilities for implementing crowdsourcing mechanics in new projects.

The design of a crowdsourcing activity, like any design activity is an exploration of a design space navigating goals (often multiple goals, some of which may be contradictory), and constraints, while exploiting technological and social opportunities, and taking account of certain issues such as privacy, security. For any given desired product, there are many experiences that could be constructed. The dimensions that we provide offer help in comprehending the alternatives and how they are practiced.

%----------------------------------------------------------------------------------------
%        BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\fontsize{10pt}{10pt}\selectfont
\urlstyle{same} %Sets the URL fonts to be the same as the current font.
\bibliographystyle{apacite}
\bibliography{crowd-taxonomy-iconf} % You can use another bibtex file if you desire, or you can just remove the contents of confbib and add in your own entries.

%----------------------------------------------------------------------------------------
%        LIST OF FIGURES, TABLES
%----------------------------------------------------------------------------------------
\tocloftpagestyle{fancy} % Header, Footer on pages with table of figures, tables
\listoffigures % Will create List of Figures
\listoftables % Will create List of Tables

\end{document}

