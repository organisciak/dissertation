
@book{abt_serious_1987,
	title = {Serious Games},
	isbn = {9780819161482},
	abstract = {The author explores the ways in which games can be used to instruct and inform as well as provide pleasure. He uses innovative approaches to problem solving through individualized game techniques. Topics include: improving education with games; educational games for the physical and social sciences; games for the learning disadvantaged; games for occupational choice and training; games for planning and problem solving in government and industry; and the future of serious games. This book was originally published in 1970 by Viking Press.},
	language = {en},
	publisher = {University Press of America},
	author = {Abt, Clark C.},
	month = jan,
	year = {1987},
	keywords = {{citeDissProp}, Education / Experimental Methods, gamification, Psychology / Psychotherapy / Counseling, serious games}
}

@inproceedings{wiggins_goals_2012,
	title = {Goals and Tasks: Two Typologies of Citizen Science Projects},
	shorttitle = {Goals and Tasks},
	doi = {10.1109/HICSS.2012.295},
	abstract = {Citizen science is a form of research collaboration involving members of the public in scientific research projects to address real-world problems. Often organized as a virtual collaboration, these projects are a type of open movement, with collective goals addressed through open participation in research tasks. We conducted a survey of citizen science projects to elicit multiple aspects of project design and operation. We then clustered projects based on the tasks performed by participants and on the project's stated goals. The clustering results group projects that show similarities along other dimensions, suggesting useful divisions of the projects.},
	booktitle = {2012 45th Hawaii International Conference on System Science ({HICSS})},
	author = {Wiggins, A and Crowston, Kevin},
	month = jan,
	year = {2012},
	keywords = {Approximation methods, Citizen science, citizen science project, collaboration, communities, Educational institutions, Electronic mail, groupware, Monitoring, open movement, Production, project design, project operation, real-world problem, research collaboration, scientific information systems, scientific research project, virtual collaboration},
	pages = {3426--3435},
	
}

@inproceedings{agichtein_improving_2006,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '06},
	title = {Improving Web Search Ranking by Incorporating User Behavior Information},
	isbn = {1-59593-369-7},
	doi = {10.1145/1148170.1148177},
	abstract = {We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting. We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features. We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine. We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31\% relative to the original performance.},
	booktitle = {Proceedings of the 29th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Agichtein, Eugene and Brill, Eric and Dumais, Susan},
	year = {2006},
	keywords = {{citeDissProp}, implicit relevance feedback, web search, web search ranking},
	pages = {19--26}
}

@inproceedings{ahn_labeling_2004,
	address = {Vienna, Austria},
	title = {Labeling images with a computer game},
	isbn = {1-58113-702-8},
	abstract = {We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.},
	booktitle = {Proceedings of the {SIGCHI} conference on Human factors in computing systems},
	publisher = {{ACM}},
	author = {Ahn, Luis von and Dabbish, Laura},
	year = {2004},
	keywords = {{citeDissProp}, distributed knowledge acquisition, hcir, humans in {IR}, {ICcited}, image labeling, online games, rate5, World Wide Web},
	pages = {319--326},
	
}

@article{alderfer_empirical_1969,
	title = {An empirical test of a new theory of human needs},
	volume = {4},
	number = {2},
	journal = {Organizational behavior and human performance},
	author = {Alderfer, Clayton P.},
	year = {1969},
	keywords = {{citeDissProp}, {citeiConf}14},
	pages = {142--175},
	
}

@incollection{alonso_design_2011,
	series = {Lecture Notes in Computer Science},
	title = {Design and Implementation of Relevance Assessments Using Crowdsourcing},
	copyright = {Springer Berlin Heidelberg},
	isbn = {978-3-642-20160-8, 978-3-642-20161-5},
	abstract = {In the last years crowdsourcing has emerged as a viable platform for conducting relevance assessments. The main reason behind this trend is that makes possible to conduct experiments extremely fast, with good results and at low cost. However, like in any experiment, there are several details that would make an experiment work or fail. To gather useful results, user interface guidelines, inter-agreement metrics, and justification analysis are important aspects of a successful crowdsourcing experiment. In this work we explore the design and execution of relevance judgments using Amazon Mechanical Turk as crowdsourcing platform, introducing a methodology for crowdsourcing relevance assessments and the results of a series of experiments using {TREC} 8 with a fixed budget. Our findings indicate that workers are as good as {TREC} experts, even providing detailed feedback for certain query-document pairs. We also explore the importance of document design and presentation when performing relevance assessment tasks. Finally, we show our methodology at work with several examples that are interesting in their own.},
	number = {6611},
	booktitle = {Advances in Information Retrieval},
	publisher = {Springer Berlin Heidelberg},
	author = {Alonso, Omar and Baeza-Yates, Ricardo},
	editor = {Clough, Paul and Foley, Colum and Gurrin, Cathal and Jones, Gareth J. F. and Kraaij, Wessel and Lee, Hyowon and Mudoch, Vanessa},
	month = jan,
	year = {2011},
	keywords = {Artificial Intelligence (incl. Robotics), {citeDissProp}, Data Mining and Knowledge Discovery, Database Management, design, Information Storage and Retrieval, Information Systems Applications (incl.Internet), Multimedia Information Systems},
	pages = {153--164},
	
}

@article{alonso_crowdsourcing_2008,
	title = {Crowdsourcing for relevance evaluation},
	volume = {42},
	issn = {0163-5840},
	doi = {10.1145/1480506.1480508},
	abstract = {Relevance evaluation is an essential part of the development and maintenance of information retrieval systems. Yet traditional evaluation approaches have several limitations; in particular, conducting new editorial evaluations of a search system can be very expensive. We describe a new approach to evaluation called {TERC}, based on the crowdsourcing paradigm, in which many online users, drawn from a large community, each performs a small evaluation task.},
	number = {2},
	journal = {{SIGIR} Forum},
	author = {Alonso, Omar and Rose, Daniel E. and Stewart, Benjamin},
	month = nov,
	year = {2008},
	keywords = {candidates, {citeDissProp}, crowdsourcing, humans in {IR}, Miles suggestions},
	pages = {9--15},
	
}

@article{angwin_volunteers_2009,
	title = {Volunteers log off as Wikipedia ages},
	volume = {23},
	journal = {Wall Street Journal},
	author = {Angwin, Julia and Fowler, Geoffrey A.},
	year = {2009},
	keywords = {{citeDissProp}, {citeiConf}14}
}

@inproceedings{bao_optimizing_2007,
	address = {New York, {NY}, {USA}},
	series = {{WWW} '07},
	title = {Optimizing Web Search Using Social Annotations},
	isbn = {978-1-59593-654-7},
	doi = {10.1145/1242572.1242640},
	abstract = {This paper explores the use of social annotations to improve websearch. Nowadays, many services, e.g. del.icio.us, have been developed for web users to organize and share their favorite webpages on line by using social annotations. We observe that the social annotations can benefit web search in two aspects: 1) the annotations are usually good summaries of corresponding webpages; 2) the count of annotations indicates the popularity of webpages. Two novel algorithms are proposed to incorporate the above information into page ranking: 1) {SocialSimRank} ({SSR})calculates the similarity between social annotations and webqueries; 2) {SocialPageRank} ({SPR}) captures the popularity of webpages. Preliminary experimental results show that {SSR} can find the latent semantic association between queries and annotations, while {SPR} successfully measures the quality (popularity) of a webpage from the web users' perspective. We further evaluate the proposed methods empirically with 50 manually constructed queries and 3000 auto-generated queries on a dataset crawledfrom delicious. Experiments show that both {SSR} and {SPRbenefit} web search significantly.},
	booktitle = {Proceedings of the 16th International Conference on World Wide Web},
	publisher = {{ACM}},
	author = {Bao, Shenghua and Xue, Guirong and Wu, Xiaoyuan and Yu, Yong and Fei, Ben and Su, Zhong},
	year = {2007},
	keywords = {{citeDissProp}, evaluation, social annotation, social page rank, social similarity, web search},
	pages = {501--510},
	
}

@article{bell_bellkor_2008,
	title = {The {BellKor} 2008 Solution to the Netflix Prize},
	journal = {Statistics Research Department at {AT}\&T Research},
	author = {Bell, Robert M. and Koren, Yehuda and Volinsky, Chris},
	year = {2008},
	keywords = {{citeDissProp}}
}

@book{benkler_wealth_2006,
	address = {New Haven},
	title = {Wealth of Networks},
	publisher = {Yale University Press},
	author = {Benkler, Yochai},
	year = {2006},
	keywords = {{citeDissProp}},
	
}

@inproceedings{bigham_vizwiz:_2010,
	address = {New York, {NY}, {USA}},
	series = {{UIST} '10},
	title = {{VizWiz}: nearly real-time answers to visual questions},
	isbn = {978-1-4503-0271-5},
	shorttitle = {{VizWiz}},
	doi = {10.1145/1866029.1866080},
	abstract = {The lack of access to visual information like text labels, icons, and colors can cause frustration and decrease independence for blind people. Current access technology uses automatic approaches to address some problems in this space, but the technology is error-prone, limited in scope, and quite expensive. In this paper, we introduce {VizWiz}, a talking application for mobile phones that offers a new alternative to answering visual questions in nearly real-time - asking multiple people on the web. To support answering questions quickly, we introduce a general approach for intelligently recruiting human workers in advance called {quikTurkit} so that workers are available when new questions arrive. A field deployment with 11 blind participants illustrates that blind people can effectively use {VizWiz} to cheaply answer questions in their everyday lives, highlighting issues that automatic approaches will need to address to be useful. Finally, we illustrate the potential of using {VizWiz} as part of the participatory design of advanced tools by using it to build and evaluate {VizWiz}::{LocateIt}, an interactive mobile tool that helps blind people solve general visual search problems.},
	booktitle = {Proceedings of the 23nd annual {ACM} symposium on User interface software and technology},
	publisher = {{ACM}},
	author = {Bigham, Jeffrey P. and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C. and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and Yeh, Tom},
	year = {2010},
	keywords = {blind users, non-visual interfaces, real-time human computation},
	pages = {333--342}
}

@inproceedings{bernstein_soylent:_2010,
	address = {New York, {NY}},
	title = {Soylent: a word processor with a crowd inside.},
	isbn = {9781450302715},
	doi = {10.1145/1866029.1866078},
	booktitle = {Proceedings of the 23nd annual {ACM} symposium on User interface software and technology},
	publisher = {{ACM} Press},
	author = {Bernstein, Michael S. and Little, Greg and Miller, Robert C. and Hartmann, Björn and Ackerman, Mark S. and Karger, David R. and Crowell, David and Panovich, Katrina},
	year = {2010},
	keywords = {candidates, {citeDissProp}, favorites, hcir, {mturkCITE}, rate4, {swiftCite}},
	pages = {313--322},
	
}

@inproceedings{bischoff_can_2008,
	address = {New York, {NY}, {USA}},
	series = {{CIKM} '08},
	title = {Can All Tags Be Used for Search?},
	isbn = {978-1-59593-991-3},
	doi = {10.1145/1458082.1458112},
	abstract = {Collaborative tagging has become an increasingly popular means for sharing and organizing Web resources, leading to a huge amount of user generated metadata. These tags represent quite a few different aspects of the resources they describe and it is not obvious whether and how these tags or subsets of them can be used for search. This paper is the first to present an in-depth study of tagging behavior for very different kinds of resources and systems - Web pages (Del.icio.us), music (Last.fm), and images (Flickr) - and compares the results with anchor text characteristics. We analyze and classify sample tags from these systems, to get an insight into what kinds of tags are used for different resources, and provide statistics on tag distributions in all three tagging environments. Since even relevant tags may not add new information to the search procedure, we also check overlap of tags with content, with metadata assigned by experts and from other sources. We discuss the potential of different kinds of tags for improving search, comparing them with user queries posted to search engines as well as through a user survey. The results are promising and provide more insight into both the use of different kinds of tags for improving search and possible extensions of tagging systems to support the creation of potentially search-relevant tags.},
	booktitle = {Proceedings of the 17th {ACM} Conference on Information and Knowledge Management},
	publisher = {{ACM}},
	author = {Bischoff, Kerstin and Firan, Claudiu S. and Nejdl, Wolfgang and Paiu, Raluca},
	year = {2008},
	keywords = {{citeDissProp}, collaborative tagging, query classification, tag classification, tag search, tagging system analysis and comparison},
	pages = {193--202},
	
}

@article{causer_transcription_2012,
	title = {Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham},
	volume = {27},
	issn = {0268-1145, 1477-4615},
	shorttitle = {Transcription maximized; expense minimized?},
	doi = {10.1093/llc/fqs004},
	abstract = {This article discusses the crowdsourced manuscript transcription project Transcribe Bentham, and how it will impact upon long-established editorial practices at the Bentham Project, University College London, which is producing the new and authoritative edition of The Collected Works of Jeremy Bentham. We site Transcribe Bentham in the burgeoning field of scholarly crowdsourcing projects, and, by detailing our experiences of running and administering the project, attempt to assess the potential benefits of engaging the public in humanities research. The article examines the conceptualization and development of Transcribe Bentham, and how editorial practices at the Bentham Project may change as a result. We account for the design of the bespoke transcription tool which is at the project's heart, and which allows volunteers to transcribe the material and encode it in {TEI}-compliant {XML}. We attempt to answer five key questions: is crowdsourcing the transcription of complex manuscripts cost-effective? Is crowdsourcing exploitative? Are volunteer-produced transcripts of sufficient quality for editorial use and uploading to a digital repository, and what quality controls are required? Does crowdsourcing ensure sustainability and widen access to this priceless material? And finally, should the success of a project like Transcribe Bentham be measured solely according to cost-effectiveness or the volume of work produced, or do considerations of public engagement and access outweigh such concerns?},
	language = {en},
	number = {2},
	journal = {Literary and Linguistic Computing},
	author = {Causer, Tim and Tonra, Justin and Wallace, Valerie},
	month = jun,
	year = {2012},
	keywords = {{citeDissProp}},
	pages = {119--137},
	
}

@misc{chen_improving_2013,
	title = {Improving Twitter search with real-time human computation},
	abstract = {One of the magical things about Twitter is that it opens a window to the world in real-time. An event happens, and seconds later, people share it across the planet. Consider, for example, what happ......},
	journal = {Twitter Engineering Blog},
	author = {Chen, Edwin and Jain, Alpa},
	month = jan,
	year = {2013},
	keywords = {{citeDissProp}},
	
}

@article{cortese_proposal_2011,
	chapter = {Opinion},
	title = {A Proposal to Allow Small Private Companies to Get Investors Online},
	issn = {0362-4331},
	abstract = {A proposal on crowdfunding would make it legal for ordinary investors to put some money (but not enough to bankrupt them) into small, private companies online.},
	journal = {The New York Times},
	author = {Cortese, Amy},
	month = sep,
	year = {2011},
	keywords = {{citeDissProp}, Computers and the Internet, Entrepreneurship, Finances, Kickstarter, Kiva.org, {McHenry}, Patrick T, Obama, Barack, Regulation and Deregulation of Industry, Securities and Exchange Commission, Small Business},
	
}

@article{cortese_crowdfunding_2013,
	chapter = {Business Day},
	title = {Crowdfunding for Small Business Is Still an Unclear Path},
	issn = {0362-4331},
	abstract = {For new businesses, the regulatory path to crowdfunding — Internet-based financing that involves ordinary investors — is still far from clear.},
	journal = {The New York Times},
	author = {Cortese, Amy},
	month = jan,
	year = {2013},
	keywords = {Banking and Financial Institutions, {CircleUp} Network Inc, {citeDissProp}, Crowdfunding (Internet), Entrepreneurship, Regulation and Deregulation of Industry, Securities and Exchange Commission, Small Business, {SoMoLend} Holdings {LLC}},
	
}

@inproceedings{dong_time_2010,
	address = {New York, {NY}, {USA}},
	series = {{WWW} '10},
	title = {Time is of the Essence: Improving Recency Ranking Using Twitter Data},
	isbn = {978-1-60558-799-8},
	shorttitle = {Time is of the Essence},
	doi = {10.1145/1772690.1772725},
	abstract = {Realtime web search refers to the retrieval of very fresh content which is in high demand. An effective portal web search engine must support a variety of search needs, including realtime web search. However, supporting realtime web search introduces two challenges not encountered in non-realtime web search: quickly crawling relevant content and ranking documents with impoverished link and click information. In this paper, we advocate the use of realtime micro-blogging data for addressing both of these problems. We propose a method to use the micro-blogging data stream to detect fresh {URLs}. We also use micro-blogging data to compute novel and effective features for ranking fresh {URLs}. We demonstrate these methods improve effective of the portal web search engine for realtime web search.},
	booktitle = {Proceedings of the 19th International Conference on World Wide Web},
	publisher = {{ACM}},
	author = {Dong, Anlei and Zhang, Ruiqiang and Kolari, Pranam and Bai, Jing and Diaz, Fernando and Chang, Yi and Zheng, Zhaohui and Zha, Hongyuan},
	year = {2010},
	keywords = {{citeDissProp}, recency modeling, recency ranking, twitter},
	pages = {331--340},
	
}

@inproceedings{efron_hashtag_2010,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '10},
	title = {Hashtag Retrieval in a Microblogging Environment},
	isbn = {978-1-4503-0153-4},
	doi = {10.1145/1835449.1835616},
	abstract = {Microblog services let users broadcast brief textual messages to people who "follow" their activity. Often these posts contain terms called hashtags, markers of a post's meaning, audience, etc. This poster treats the following problem: given a user's stated topical interest, retrieve useful hashtags from microblog posts. Our premise is that a user interested in topic x might like to find hashtags that are often applied to posts about x. This poster proposes a language modeling approach to hashtag retrieval. The main contribution is a novel method of relevance feedback based on hashtags. The approach is tested on a corpus of data harvested from twitter.com.},
	booktitle = {Proceedings of the 33rd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Efron, Miles},
	year = {2010},
	keywords = {{citeDissProp}, hashtag, microblog, relevance feedback, twitter},
	pages = {787--788},
	
}

@article{efron_information_2011,
	title = {Information search and retrieval in microblogs},
	volume = {62},
	issn = {1532-2890},
	doi = {10.1002/asi.21512},
	abstract = {Modern information retrieval ({IR}) has come to terms with numerous new media in efforts to help people find information in increasingly diverse settings. Among these new media are so-called microblogs. A microblog is a stream of text that is written by an author over time. It comprises many very brief updates that are presented to the microblog's readers in reverse-chronological order. Today, the service called Twitter is the most popular microblogging platform. Although microblogging is increasingly popular, methods for organizing and providing access to microblog data are still new. This review offers an introduction to the problems that face researchers and developers of {IR} systems in microblog settings. After an overview of microblogs and the behavior surrounding them, the review describes established problems in microblog retrieval, such as entity search and sentiment analysis, and modeling abstractions, such as authority and quality. The review also treats user-created metadata that often appear in microblogs. Because the problem of microblog search is so new, the review concludes with a discussion of particularly pressing research issues yet to be studied in the field.},
	language = {en},
	number = {6},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Efron, Miles},
	year = {2011},
	keywords = {{citeDissProp}},
	pages = {996--1008},
	
}

@inproceedings{eickhoff_quality_2012,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '12},
	title = {Quality Through Flow and Immersion: Gamifying Crowdsourced Relevance Assessments},
	isbn = {978-1-4503-1472-5},
	shorttitle = {Quality Through Flow and Immersion},
	doi = {10.1145/2348283.2348400},
	abstract = {Crowdsourcing is a market of steadily-growing importance upon which both academia and industry increasingly rely. However, this market appears to be inherently infested with a significant share of malicious workers who try to maximise their profits through cheating or sloppiness. This serves to undermine the very merits crowdsourcing has come to represent. Based on previous experience as well as psychological insights, we propose the use of a game in order to attract and retain a larger share of reliable workers to frequently-requested crowdsourcing tasks such as relevance assessments and clustering. In a large-scale comparative study conducted using recent {TREC} data, we investigate the performance of traditional {HIT} designs and a game-based alternative that is able to achieve high quality at significantly lower pay rates, facing fewer malicious submissions.},
	booktitle = {Proceedings of the 35th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Eickhoff, Carsten and Harris, Christopher G. and de Vries, Arjen P. and Srinivasan, Padmini},
	year = {2012},
	keywords = {{citeDissProp}, clustering, crowdsourcing, gamification, relevance assessments, serious games},
	pages = {871--880},
	
}

@article{eickhoff_increasing_2012,
	title = {Increasing cheat robustness of crowdsourcing tasks},
	issn = {1386-4564, 1573-7659},
	doi = {10.1007/s10791-011-9181-9},
	abstract = {Crowdsourcing successfully strives to become a widely used means of collecting large-scale scientific corpora. Many research fields, including Information Retrieval, rely on this novel way of data acquisition. However, it seems to be undermined by a significant share of workers that are primarily interested in producing quick generic answers rather than correct ones in order to optimise their time-efficiency and, in turn, earn more money. Recently, we have seen numerous sophisticated schemes of identifying such workers. Those, however, often require additional resources or introduce artificial limitations to the task. In this work, we take a different approach by investigating means of a priori making crowdsourced tasks more resistant against cheaters.},
	journal = {Information Retrieval},
	author = {Eickhoff, Carsten and Vries, Arjen P.},
	month = feb,
	year = {2012},
	keywords = {{citeDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	
}

@inproceedings{finin_annotating_2010,
	address = {Stroudsburg, {PA}, {USA}},
	series = {{CSLDAMT} '10},
	title = {Annotating Named Entities in Twitter Data with Crowdsourcing},
	abstract = {We describe our experience using both Amazon Mechanical Turk ({MTurk}) and Crowd-Flower to collect simple named entity annotations for Twitter status updates. Unlike most genres that have traditionally been the focus of named entity experiments, Twitter is far more informal and abbreviated. The collected annotations and annotation techniques will provide a first step towards the full study of named entity recognition in domains like Facebook and Twitter. We also briefly describe how to use {MTurk} to collect judgements on the quality of "word clouds."},
	booktitle = {Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
	publisher = {Association for Computational Linguistics},
	author = {Finin, Tim and Murnane, Will and Karandikar, Anand and Keller, Nicholas and Martineau, Justin and Dredze, Mark},
	year = {2010},
	keywords = {{citeDissProp}, crowdsourcing, mechanical turk, paid crowdsourcing, twitter},
	pages = {80--88},
	
}

@article{fung_larry_????,
	title = {Larry Lessig’s super {PAC} to end super {PACs} raised \$2.5 million in just 2 days. Here’s what comes next.},
	issn = {0190-8286},
	abstract = {The average donation was around \$140, and other numbers.},
	language = {en-{US}},
	journal = {The Washington Post},
	author = {Fung, Brian},
	month = jul,
	keywords = {{citeDissProp}},
	
}

@inproceedings{trant_investigating_2006,
	title = {Investigating social tagging and folksonomy in art museums with steve. museum},
	booktitle = {Proceedings of the {WWW} 2006 Collaborative Web Tagging Workshop},
	author = {Trant, Jennifer and Wyman, Bruce},
	date = {2006},
    year = {2006},
	keywords = {{citeDissProp}},
	
}

@article{galton_vox_1907,
	title = {Vox populi},
	volume = {75},

	journal = {Nature},
	author = {Galton, Francis},
	year = {1907},
	keywords = {{citeDissProp}},
	pages = {450--451},
	
}

@inproceedings{geiger_managing_2011,
	title = {Managing the crowd: towards a taxonomy of crowdsourcing processes},
	shorttitle = {Managing the crowd},

	booktitle = {Proceedings of the seventeenth Americas conference on information systems, Detroit, Michigan},
	author = {Geiger, David and Seedorf, Stefan and Schulze, Thimo and Nickerson, Robert and Schader, Martin},
	year = {2011},
	keywords = {{citeDissProp}, {citeiConf}14},
	pages = {1--15},
	
}

@inproceedings{grady_crowdsourcing_2010,
	address = {Stroudsburg, {PA}, {USA}},
	series = {{CSLDAMT} '10},
	title = {Crowdsourcing document relevance assessment with Mechanical Turk},

	booktitle = {Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
	publisher = {Association for Computational Linguistics},
	author = {Grady, Catherine and Lease, Matthew},
	year = {2010},
	keywords = {{citeDissProp}},
	pages = {172--179}
}

@article{harris_applying_2012,
	title = {Applying human computation mechanisms to information retrieval},
	volume = {49},
	issn = {1550-8390},
	doi = {10.1002/meet.14504901050},
	abstract = {Crowdsourcing and Games with a Purpose ({GWAP}) have each received considerable attention in recent years. These two human computation mechanisms assist with tasks that cannot be solved by computers alone. Despite this increased attention, much of this transformation has been limited to a few aspects of Information Retrieval ({IR}). In this paper, we examine these two mechanisms' applicability to {IR}. Using an {IR} model, we apply criteria to determine the suitability of these crowdsourcing and {GWAP} mechanisms to each step of the model. Our analysis illustrates that these mechanisms can apply to several of these steps with good returns.},
	language = {en},
	number = {1},
	journal = {Proceedings of the American Society for Information Science and Technology},
	author = {Harris, Christopher G. and Srinivasan, Padmini},
	month = jan,
	year = {2012},
	keywords = {{citeDissProp}, crowdsourcing, human computation, Information Retrieval},
	pages = {1--10},
	
}

@inproceedings{heymann_can_2008,
	address = {New York, {NY}, {USA}},
	series = {{WSDM} '08},
	title = {Can Social Bookmarking Improve Web Search?},
	isbn = {978-1-59593-927-2},
	doi = {10.1145/1341531.1341558},
	abstract = {Social bookmarking is a recent phenomenon which has the potential to give us a great deal of data about pages on the web. One major question is whether that data can be used to augment systems like web search. To answer this question, over the past year we have gathered what we believe to be the largest dataset from a social bookmarking site yet analyzed by academic researchers. Our dataset represents about forty million bookmarks from the social bookmarking site del.icio.us. We contribute a characterization of posts to del.icio. us: how many bookmarks exist (about 115 million), how fast is it growing, and how active are the {URLs} being posted about (quite active). We also contribute a characterization of tags used by bookmarkers. We found that certain tags tend to gravitate towards certain domains, and vice versa. We also found that tags occur in over 50 percent of the pages that they annotate, and in only 20 percent of cases do they not occur in the page text, backlink page text, or forward link page text of the pages they annotate. We conclude that social bookmarking can provide search data not currently provided by other sources, though it may currently lack the size and distribution of tags necessary to make a significant impact},
	booktitle = {Proceedings of the 2008 International Conference on Web Search and Data Mining},
	publisher = {{ACM}},
	author = {Heymann, Paul and Koutrika, Georgia and Garcia-Molina, Hector},
	year = {2008},
	keywords = {{citeDissProp}, collaborative tagging, social bookmarking, web search},
	pages = {195--206},
	
}

@article{hofmann_latent_2004,
	title = {Latent semantic models for collaborative filtering},
	volume = {22},
	issn = {1046-8188},
	doi = {10.1145/963770.963774},
	abstract = {Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.},
	number = {1},
	journal = {{ACM} Trans. Inf. Syst.},
	author = {Hofmann, Thomas},
	month = jan,
	year = {2004},
	keywords = {{citeDissProp}},
	pages = {89--115}
}


@article{holley_crowdsourcing:_2010,
	title = {Crowdsourcing: How and Why Should Libraries Do It?},
	volume = {16},
	issn = {1082-9873},
	shorttitle = {Crowdsourcing},
	doi = {10.1045/march2010-holley},
	number = {3/4},
	journal = {D-Lib Magazine},
	author = {Holley, Rose},
	month = mar,
	year = {2010}
}

@techreport{holley_many_2009,
	title = {Many Hands Make Light Work: Public Collaborative {OCR} Text Correction in Australian Historic Newspapers},
	abstract = {The {ANDP} team had from the outset in January 2007 decided to make a considerable investment in software development in order to be able to quality assure digital outputs to ensure they met minimum standards and to future proof the files in case they could be improved further in the future.

Contributors had all expressed concern that the digital outputs (image quality, {OCR} text) may not be good enough to enable adequate full text retrieval or to meet user expectations. The {ANDP} team had regularly brainstormed and reviewed ideas to improve the quality of outputs and had implemented a number of ideas to achieve this3. At this stage the team were assuming that quality of data entirely relied on the Library’s digitisation technique. It had not yet occurred to the team or the contributors that the public may play a role in improving and enhancing the quality of the data.},
	institution = {National Library of Australia},
	author = {Holley, Rose},
	year = {2009},
	keywords = {{citeDissProp}, {citeiConf}14, {ICcited}, sorted}
}

@book{hotho_information_2006,
	title = {Information retrieval in folksonomies: Search and ranking},
	shorttitle = {Information retrieval in folksonomies},

	publisher = {Springer},
	author = {Hotho, Andreas and Jäschke, Robert and Schmitz, Christoph and Stumme, Gerd},
	year = {2006},
	keywords = {{citeDissProp}, folksonomy, Information Retrieval},
	
}

@misc{howe_crowdsourcing:_2006,
	title = {Crowdsourcing: A Definition},
	shorttitle = {Crowdsourcing},
	journal = {Crowdsourcing: Tracking the rise of the amateur},
	author = {Howe, J.},
	year = {2006},
	keywords = {{citeDissProp}}
}

@book{raymond_cathedral_1999,
	title = {The Cathedral and the Bazaar},
	abstract = {I anatomize a successful open-source project, fetchmail, that was run as a deliberate test of the surprising theories about software engineering suggested by the history of Linux. I discuss these theories in terms of two fundamentally different development styles, the ``cathedral'' model of most of the commercial world versus the ``bazaar'' model of the Linux world. I show that these models derive from opposing assumptions about the nature of the software-debugging task. I then make a sustained argument from the Linux experience for the proposition that ``Given enough eyeballs, all bugs are shallow'', suggest productive analogies with other self-correcting systems of selfish agents, and conclude with some exploration of the implications of this insight for the future of software.},
	pagetotal = {241},
	publisher = {O'Reilly Media},
	author = {Raymond, Eric S.},
	year = {1999},
	keywords = {{citeDissProp}},
	
}

@article{howe_rise_2006,
	title = {The rise of crowdsourcing},
	volume = {14},
	number = {6},
	journal = {Wired Magazine},
	author = {Howe, J.},
	year = {2006},
	keywords = {{citeDissProp}},
	
}

@misc{howe_birth_2006,
	title = {Birth of a Meme},

	journal = {Crowdsourcing},
	author = {Howe, Jeff},
	month = may,
	year = {2006},
	keywords = {{citeDissProp}}
}

@book{howe_crowdsourcing:_2008,
	edition = {1},
	title = {Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business},
	isbn = {0307396207},
	shorttitle = {Crowdsourcing},
	publisher = {Crown Business},
	author = {Howe, Jeff},
	month = aug,
	year = {2008},
	keywords = {{citeDissProp}}
}

@inproceedings{ipeirotis_quality_2010,
	address = {New York, {NY}, {USA}},
	series = {{HCOMP} '10},
	title = {Quality management on Amazon Mechanical Turk},
	isbn = {978-1-4503-0222-7},
	doi = {10.1145/1837885.1837906},
	booktitle = {Proceedings of the {ACM} {SIGKDD} Workshop on Human Computation},
	publisher = {{ACM}},
	author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
	year = {2010},
	keywords = {{citeDissProp}},
	pages = {64--67}
}

@article{khatib_algorithm_2011,
	title = {Algorithm discovery by protein folding game players},
	volume = {108},
	number = {47},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Khatib, Firas and Cooper, Seth and Tyka, Michael D. and Xu, Kefan and Makedon, Ilya and Popović, Zoran and Baker, David and Players, Foldit},
	year = {2011},
	keywords = {{citeDissProp}},
	pages = {18949--18953},
	
}

@inproceedings{komarov_crowdsourcing_2013,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '13},
	title = {Crowdsourcing Performance Evaluations of User Interfaces},
	isbn = {978-1-4503-1899-0},
	doi = {10.1145/2470654.2470684},
	abstract = {Online labor markets, such as Amazon's Mechanical Turk ({MTurk}), provide an attractive platform for conducting human subjects experiments because the relative ease of recruitment, low cost, and a diverse pool of potential participants enable larger-scale experimentation and faster experimental revision cycle compared to lab-based settings. However, because the experimenter gives up the direct control over the participants' environments and behavior, concerns about the quality of the data collected in online settings are pervasive. In this paper, we investigate the feasibility of conducting online performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via {MTurk}. We implemented three performance experiments to re-evaluate three previously well-studied user interface designs. We conducted each experiment both in lab and online with participants recruited via {MTurk}. The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings: All statistically significant differences detected in lab were also present on {MTurk} and the effect sizes were similar. In addition, there were no significant differences between the two settings in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments. These results suggest that {MTurk} may be a productive setting for conducting performance evaluations of user interfaces providing a complementary approach to existing methodologies.},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Komarov, Steven and Reinecke, Katharina and Gajos, Krzysztof Z.},
	year = {2013},
	keywords = {{citeDissProp}, crowdsourcing, mechanical turk, user interface evaluation},
	pages = {207--216}
}

@book{kraut_building_2011,
	address = {Cambridge, {MA}},
	title = {Building Successful Online Communities},
	publisher = {{MIT} Press},
	author = {Kraut, Robert E. and Resnick, Paul},
	year = {2011},
	keywords = {{citeDissProp}, {citeiConf}14, crowdsourcing},
	
}

@article{lakhani_how_2003,
	title = {How open source software works: "free" user-to-user assistance},
	volume = {32},
	shorttitle = {How open source software works},
	doi = {10.1016/S0048-7333(02)00095-1},
	abstract = {Research into free and open source software development projects has so far largely focused on how the major tasks of software development are organized and motivated. But a complete project requires the execution of "mundane but necessary" tasks as well. In this paper, we explore how the mundane but necessary task of field support is organized in the case of Apache web server software, and why some project participants are motivated to provide this service gratis to others. We find that the Apache field support system functions effectively. We also find that, when we partition the help system into its component tasks, 98\% of the effort expended by information providers in fact returns direct learning benefits to those providers. This finding considerably reduces the puzzle of why information providers are willing to perform this task "for free." Implications are discussed.},
	number = {6},
	journal = {Research Policy},
	author = {Lakhani, Karim R. and Hippel, Eric von},
	month = jun,
	year = {2003},
	keywords = {{citeDissProp}, Open source software, User innovation, User support, Virtual community},
	pages = {923--943},
	
}

@article{lamere_social_2008,
	title = {Social tagging and music information retrieval},
	volume = {37},
	number = {2},
	journal = {Journal of New Music Research},
	author = {Lamere, Paul},
	year = {2008},
	keywords = {{citeDissProp}},
	pages = {101--114},
	
}

@article{law_human_2011,
	title = {Human Computation},
	volume = {5},
	issn = {1939-4608, 1939-4616},
	doi = {10.2200/S00371ED1V01Y201107AIM013},
	number = {3},
	journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Law, Edith and Ahn, Luis von},
	month = jun,
	year = {2011},
	keywords = {{citeDissProp}},
	pages = {1--121},
	
}

@book{surowiecki_wisdom_2004,
	title = {The Wisdom of Crowds},
	publisher = {Doubleday},
	author = {Surowiecki, James},
	date = {2004},
    year = {2004},
	keywords = {{citeDissProp}, crowdsourcing, {ICcited}, read, sorted}
}

@book{le_bon_crowd:_1896,
	title = {The Crowd: A Study of the Popular Mind},

	author = {Le Bon, Gustav},
	year = {1896},
    date = {1896},
	keywords = {{citeDissProp}},
	
}

@inproceedings{lease_overview_2011,
	title = {Overview of the {TREC} 2011 Crowdsourcing Track (Conference Notebook)},
	booktitle = {Text Retrieval Conference Notebook},
	author = {Lease, Matthew and Kazai, Gabriella},
	year = {2011},
	keywords = {{citeDissProp}, {hcirCITE}, {TREC} crowd}
}

@inproceedings{liu_cluster-based_2004,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '04},
	title = {Cluster-based retrieval using language models},
	isbn = {1-58113-881-4},
	doi = {10.1145/1008992.1009026},
	abstract = {Previous research on cluster-based retrieval has been inconclusive as to whether it does bring improved retrieval effectiveness over document-based retrieval. Recent developments in the language modeling approach to {IR} have motivated us to re-examine this problem within this new retrieval framework. We propose two new models for cluster-based retrieval and evaluate them on several {TREC} collections. We show that cluster-based retrieval can perform consistently across collections of realistic size, and significant improvements over document-based retrieval can be obtained in a fully automatic manner and without relevance information provided by human.},
	booktitle = {Proceedings of the 27th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Liu, Xiaoyong and Croft, W. Bruce},
	year = {2004},
	keywords = {{citeDissProp}, cluster model, cluster-based language model, cluster-based retrieval, hierarchical clustering, {INFORMATION} retrieval, language model, query-specific clustering, smoothing, static clustering, topic model},
	pages = {186--193}
}

@book{mackay_memoirs_1852,
	title = {Memoirs of Extraordinary Popular Delusions and the Madness of Crowds},
	author = {Mackay, Charles},
	year = {1852},
	keywords = {{citeDissProp}}
}

@article{maslow_theory_1943,
	title = {A theory of human motivation},
	volume = {50},
	copyright = {(c) 2012 {APA}, all rights reserved},
	issn = {1939-1471(Electronic);0033-295X(Print)},
	doi = {10.1037/h0054346},
	abstract = {After listing the propositions that must be considered as basic, the author formulates a theory of human motivation in line with these propositions and with the known facts derived from observation and experiment. There are 5 sets of goals (basic needs) which are related to each other and are arranged in a hierarchy of prepotency. When the most prepotent goal is realized, the next higher need emerges. "Thus man is a perpetually wanting animal." Thwarting, actual or imminent, of these basic needs provides a psychological threat that leads to psychopathy.},
	number = {4},
	journal = {Psychological Review},
	author = {Maslow, A.H.},
	year = {1943},
	keywords = {*Motivation, Antisocial Personality Disorder, {citeDissProp}, {citeiConf}14, Human Development},
	pages = {370--396}
}

@article{mason_financial_2010,
	title = {Financial incentives and the "performance of crowds"},
	volume = {11},
	issn = {1931-0145},
	doi = {10.1145/1809400.1809422},
	abstract = {The relationship between financial incentives and performance, long of interest to social scientists, has gained new relevance with the advent of web-based "crowd-sourcing" models of production. Here we investigate the effect of compensation on performance in the context of two experiments, conducted on Amazon's Mechanical Turk ({AMT}). We find that increased financial incentives increase the quantity, but not the quality, of work performed by participants, where the difference appears to be due to an "anchoring" effect: workers who were paid more also perceived the value of their work to be greater, and thus were no more motivated than workers paid less. In contrast with compensation levels, we find the details of the compensation scheme do matter--specifically, a "quota" system results in better work for less pay than an equivalent "piece rate" system. Although counterintuitive, these findings are consistent with previous laboratory studies, and may have real-world analogs as well.},
	number = {2},
	journal = {{SIGKDD} Explor. Newsl.},
	author = {Mason, Winter and Watts, Duncan J.},
	month = may,
	year = {2010},
	keywords = {candidates, {citeDissProp}, {citeiConf}14, crowdsourcing, crowd-sourcing, extrinsic motivation, hcir, humans in {IR}, incentives, Intrinsic motivation, mechanical turk, peer production, performance},
	pages = {100--108},
	
}

@inproceedings{mccreadie_crowdsourcing_2011,
	title = {Crowdsourcing blog track top news judgments at {TREC}},

	booktitle = {Proceedings of the workshop on crowdsourcing for search and data mining ({CSDM}) at the fourth {ACM} international conference on web search and data mining ({WSDM})},
	author = {McCreadie, Richard and Macdonald, Craig and Ounis, Iadh},
	year = {2011},
	keywords = {{citeDissProp}, {TREC} crowd},
	pages = {23--26},
	
}

@inproceedings{mccreadie_university_2011,
	title = {University of Glasgow at {TREC} 2011: Experiments with Terrier in Crowdsourcing, Microblog, and Web Tracks.},
	shorttitle = {University of Glasgow at {TREC} 2011},

	booktitle = {{TREC}},
	author = {McCreadie, Richard and Macdonald, Craig and Santos, Rodrygo LT and Ounis, Iadh},
	year = {2011},
	keywords = {{citeDissProp}, {TREC} crowd},
	
}

@book{michael_serious_2005,
	title = {Serious Games: Games That Educate, Train, and Inform},
	isbn = {1592006221},
	shorttitle = {Serious Games},
	abstract = {Coverage includes- David "{RM}" Michael has been a successful independent software developer for over 10 years, working in a variety of industries, including video games. He is the owner of {DavidRM} Software (www.davidrm.com) and co-owner of Samu Games (www.samugames.com). Michael is the author of The Indie Game Development Survival Guide, and his articles about game design, development, and the game development industry have appeared on {GameDev}.net (www.gamedev.net) and in the book Game Design Perspectives. His blog about independent games, serious games, and independent software is Joe Indie (www.joeindie.com). Sande Chen has been active in the gaming industry for over five years. She has written for mainstream and industry publications, including Secrets of the Game Business, and was a speaker at the 2005 Game Developers Conference. Her past game credits include Independent Games Festival winner Terminus, Scooby-Doo, and {JamDat} Scrabble. Chen holds dual degrees in economics and in writing and humanistic studies from the Massachusetts Institute of Technology, an M.Sc. in economics from the London School of Economics, and an M.F.A. in cinema-television from the University of Southern California. In 1996, she was nominated for a Grammy in music video direction. She currently works as a freelance writer/game designer. Covers techniques to make entertainment-oriented games richer and provide a deeper experience. The focus on serious games continues to grow--from coverage in the media to conferences and buzz within the game development community. Provides an overview of the major markets for serious games, including current examples and future anticipation.},
	publisher = {Muska \& Lipman/Premier-Trade},
	author = {Michael, David R. and Chen, Sandra L.},
	year = {2005},
	keywords = {{citeDissProp}, gamification, serious games}
}

@article{moyle_manuscript_2010,
	title = {Manuscript transcription by crowdsourcing: Transcribe Bentham},
	volume = {20},
	shorttitle = {Manuscript transcription by crowdsourcing},
	abstract = {Transcribe Bentham is testing the feasibility of outsourcing the work of manuscript transcription to members of the public.  {UCL} Library Services holds 60,000 folios of manuscripts of the philosopher and jurist Jeremy Bentham (1748-1832).  Transcribe Bentham will digitise 12,500 Bentham folios, and, through a wiki-based interface, allow volunteer transcribers to take temporary ownership of manuscript images and to create {TEI}-encoded transcription text for final approval by {UCL} experts.  Approved transcripts will be stored and preserved, with the manuscript images, in {UCL}'s public Digital Collections repository.  

The project makes innovative use of traditional Library material. It will stimulate public engagement with {UCL}'s scholarly archive collections and the challenges of palaeography and manuscript transcription; it will raise the profile of the work and thought of Jeremy Bentham; and it will create new digital resources for future use by  professional researchers.  Towards the end of the project, the transcription tool will be made available to other projects and services.},
	number = {3-4},
	journal = {{LIBER} Quarterly},
	author = {Moyle, M. and Tonra, J. and Wallace, V.},
	year = {2010},
	keywords = {{citeDissProp}, crowdsourcing, digital humanities, {swiftCite}},
	
}

@book{neuendorf_content_2002,
	address = {Thousand Oaks, {CA}, {USA}},
	title = {The Content Analysis Guidebook},
	publisher = {Sage Publications},
	author = {Neuendorf, Kimberly A.},
	year = {2002},
	keywords = {{citeDissProp}}
}

@misc{norvig_english_????,
	title = {English Letter Frequency Counts: Mayzner Revisited or {ETAOIN} {SRHLDCU}},
	shorttitle = {English Letter Frequency Counts},

	author = {Norvig, Peter},
	keywords = {{citeDissProp}}
}

@inproceedings{novotney_cheap_2010,
	address = {Stroudsburg, {PA}, {USA}},
	title = {Cheap, fast and good enough: Automatic speech recognition with non-expert transcription},
	shorttitle = {Cheap, fast and good enough},
	booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	author = {Novotney, S. and Callison-Burch, C.},
	year = {2010},
	keywords = {annotation, {citeDissProp}, expertise, experts, {hcirCITE}, {hcirMidtermCITE}, mechanical turk, {mturkCITE}, turk},
	pages = {207--215},
	
}

@article{organisciak_evaluating_2012,
	title = {Evaluating rater quality and rating difficulty in online annotation activities},
	volume = {49},
	issn = {1550-8390},
	doi = {10.1002/meet.14504901166},
	abstract = {Gathering annotations from non-expert online raters is an attractive method for quickly completing large-scale annotation tasks, but the increased possibility of unreliable annotators and diminished work quality remains a cause for concern. In the context of information retrieval, where human-encoded relevance judgments underlie the evaluation of new systems and methods, the ability to quickly and reliably collect trustworthy annotations allows for quicker development and iteration of research.In the context of paid online workers, this study evaluates indicators of non-expert performance along three lines: temporality, experience, and agreement. It is found that user performance is a key indicator for future performance. Additionally, the time spent by raters familiarizing themselves with a new set of tasks is important for rater quality, as is long-term familiarity with a topic being rated.These findings may inform large-scale digital collections' use of non-expert raters for performing more purposive and affordable online annotation activities.},
	language = {en},
	number = {1},
	journal = {Proceedings of the American Society for Information Science and Technology},
	author = {Organisciak, Peter and Efron, Miles and Fenlon, Katrina and Senseney, Megan},
	year = {2012},
	keywords = {{citeDissProp}},
	pages = {1--10},
	
}

@inproceedings{organisciak_personalized_2013,
	address = {Palm Spring, {CA}},
	title = {Personalized Human Computation},
	author = {Organisciak, Peter and Teevan, Jaime and Dumais, Susan and Miller, Robert C. and Kalai, Adam Tauman},
	year = {2013},
	keywords = {{citeDissProp}}
}

@phdthesis{organisciak_why_2010,
	address = {Edmonton, Alberta},
	type = {Thesis},
	title = {Why Bother? Examining the motivations of users in large-scale crowd-powered online initiatives},
	abstract = {This study examines the motivations of participants in networked, large-scale content production and research – a paradigm of distributed work magnified by the Internet. This has come to be called crowdsourcing. The approach taken in examining the crowdsourcing paradigm is of retrospection, with a study focused on observed examples and existing theories. Thirteen cases of existing crowdsourcing sites were selected for study, from a larger sample of 300. These cases were coded by their site properties and analyzed, identifying possible motivational mechanisms. Subsequent interviews with eight medium to heavy Internet users further explored these features, with an emphasis on ranking relative importance of various motivators. This study concludes with a series of recommendations on motivating crowds in such projects, emphasizing among others the importance of topical interest, ease of participation, and appeals to the individuals’ knowledge. In addition to base motivators, a number of support, or secondary, motivators are outlined.},
	school = {University of Alberta},
	author = {Organisciak, Peter},
	month = aug,
	year = {2010},
	keywords = {{citeDissProp}, {citeiConf}14, crowdsourcing, {ICcited}, motivation, sorted}
}

@inproceedings{organisciak_incidental_2013,
	address = {Lincoln, Nebraska},
	title = {Incidental Crowdsourcing: Crowdsourcing in the Periphery},
	booktitle = {Digital Humanities 2013}, 
	abstract = {As the customs of the Internet grow increasingly collaborative, crowdsourcing offers an appealing frame for looking at the interaction of users with online systems and each other. However, it is a broad term that fails to emphasize the use of crowds in subtler system augmentation.

This paper introduces incidental crowdsourcing ({IC}): an approach to user-provided item description that adopts crowdsourcing as a frame for thinking about augmentative features of system design. {IC} is intended to frame discussion around peripheral and non-critical system design choices.

A provisional definition of incidental crowdsourcing will be defined in this paper, and then refined based on examples seen in practice. {IC} will be examined from both the user and system ends, positioned within existing work, and considered in the context of its benefits and drawbacks. This approach allows us to explore the robustness and feasibility of {IC}, looking at the implications inherent to accepting the provisional definition.

The consequences of considering system design on a scale between {IC} and non-{IC} design choices remain to be seen. Toward this goal, the second part of this paper shows a study comparing the participation habits of users in two online systems — one that is representative of {IC} properties and one that is not. This study finds differences in user engagement between the two systems.},
	author = {Organisciak, Peter},
	month = jul,
	year = {2013},
	keywords = {{citeDissProp}}
}

@techreport{page_pagerank_1999,
	type = {Technical Report},
	title = {The {PageRank} Citation Ranking: Bringing Order to the Web.},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes {PageRank}, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare {PageRank} to an idealized random Web surfer. We show how to efficiently compute {PageRank} for large numbers of pages. And, we show how to apply {PageRank} to search and to user navigation.},
	number = {1999-66},
	institution = {Stanford {InfoLab}},
	author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
	month = nov,
	year = {1999},
	keywords = {{citeDissProp}, {citeiConf}14}
}

@inproceedings{ponte_language_1998,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '98},
	title = {A Language Modeling Approach to Information Retrieval},
	isbn = {1-58113-015-5},
	doi = {10.1145/290941.291008},
	booktitle = {Proceedings of the 21st Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Ponte, Jay M. and Croft, W. Bruce},
	year = {1998},
	keywords = {candidates, {citeDissProp}},
	pages = {275--281},
	
}

@inproceedings{quinn_human_2011,
	title = {Human computation},
	isbn = {9781450302289},
	doi = {10.1145/1978942.1979148},
	publisher = {{ACM} Press},
	author = {Quinn, Alexander J. and Bederson, Benjamin B.},
	year = {2011},
	keywords = {{citeDissProp}, {citeiConf}14, crowdsourcing, data mining, human computation, literature review, social computing, survey, Taxonomy},
	pages = {1403},
	
}

@inproceedings{raykar_supervised_2009,
	address = {New York, {NY}, {USA}},
	series = {{ICML} '09},
	title = {Supervised learning from multiple experts: whom to trust when everyone lies a bit},
	isbn = {978-1-60558-516-1},
	shorttitle = {Supervised learning from multiple experts},
	doi = {10.1145/1553374.1553488},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Raykar, Vikas C. and Yu, Shipeng and Zhao, Linda H. and Jerebko, Anna and Florin, Charles and Valadez, Gerardo Hermosillo and Bogoni, Luca and Moy, Linda},
	year = {2009},
	keywords = {{citeDissProp}},
	pages = {889--896}
}

@book{raymond_cathedral_1999,
	title = {The Cathedral and the Bazaar},
	abstract = {I anatomize a successful open-source project, fetchmail, that was run as a deliberate test of the surprising theories about software engineering suggested by the history of Linux. I discuss these theories in terms of two fundamentally different development styles, the ``cathedral'' model of most of the commercial world versus the ``bazaar'' model of the Linux world. I show that these models derive from opposing assumptions about the nature of the software-debugging task. I then make a sustained argument from the Linux experience for the proposition that ``Given enough eyeballs, all bugs are shallow'', suggest productive analogies with other self-correcting systems of selfish agents, and conclude with some exploration of the implications of this insight for the future of software.},
	publisher = {O'Reilly Media},
	author = {Raymond, Eric S.},
	year = {1999},
	keywords = {{citeDissProp}},
	
}

@book{ritterfeld_serious_2010,
	title = {Serious Games: Mechanisms and Effects},
	isbn = {9781135848910},
	shorttitle = {Serious Games},
	language = {en},
	publisher = {Routledge},
	author = {Ritterfeld, Ute and Cody, Michael and Vorderer, Peter},
	month = apr,
	year = {2010},
	keywords = {{citeDissProp}, Education / Computers \& Technology, Games / Video \& Electronic, gamification, Language Arts \& Disciplines / Communication Studies, serious games, Social Science / Media Studies}
}

@article{rouse_preliminary_2010,
	title = {A Preliminary Taxonomy of Crowdsourcing},
	journal = {{ACIS} 2010 Proceedings},
	author = {Rouse, Anne},
	month = jan,
	year = {2010},
	keywords = {{citeDissProp}, {citeiConf}14, crowdsourcing, Taxonomy},
	
}

@article{ryan_intrinsic_2000,
	title = {Intrinsic and Extrinsic Motivations: Classic Definitions and New Directions},
	volume = {25},
	issn = {0361-476X},
	shorttitle = {Intrinsic and Extrinsic Motivations},
	doi = {10.1006/ceps.1999.1020},
	abstract = {Intrinsic and extrinsic types of motivation have been widely studied, and the distinction between them has shed important light on both developmental and educational practices. In this review we revisit the classic definitions of intrinsic and extrinsic motivation in light of contemporary research and theory. Intrinsic motivation remains an important construct, reflecting the natural human propensity to learn and assimilate. However, extrinsic motivation is argued to vary considerably in its relative autonomy and thus can either reflect external control or true self-regulation. The relations of both classes of motives to basic human needs for autonomy, competence and relatedness are discussed.},
	number = {1},
	journal = {Contemporary Educational Psychology},
	author = {Ryan, Richard M. and Deci, Edward L.},
	month = jan,
	year = {2000},
	keywords = {{citeDissProp}, {citeiConf}14, extrinsic motivation, Intrinsic motivation, motivation},
	pages = {54--67},
	
}

@article{sanger_fate_2009,
	title = {The Fate of Expertise after Wikipedia},
	volume = {6},
	doi = {10.3366/E1742360008000543},
	number = {01},
	journal = {Episteme},
	author = {Sanger, Lawrence M.},
	year = {2009},
	keywords = {{citeDissProp}, {citeiConf}14},
	pages = {52--73},
	
}

@inproceedings{schenk_crowdsourcing:_2009,
	title = {Crowdsourcing: What can be Outsourced to the Crowd, and Why?},
	shorttitle = {Crowdsourcing},

	booktitle = {Workshop on Open Source Innovation, Strasbourg, France},
	author = {Schenk, Eric and Guittard, Claude},
	year = {2009},
	keywords = {{citeDissProp}, {citeiConf}14, crowdsourcing, Taxonomy},
	
}

@inproceedings{sheng_get_2008,
	address = {New York, {NY}, {USA}},
	series = {{KDD} '08},
	title = {Get another label? improving data quality and data mining using multiple, noisy labelers},
	isbn = {978-1-60558-193-4},
	shorttitle = {Get another label?},
	doi = {10.1145/1401890.1401965},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {{ACM}},
	author = {Sheng, Victor S. and Provost, Foster and Ipeirotis, Panagiotis G.},
	year = {2008},
	keywords = {{citeDissProp}, data preprocessing, data selection, {hcirCITE}},
	pages = {614--622},
	
}

@inproceedings{shiells_generating_2010,
	address = {New York, {NY}, {USA}},
	series = {{ESAIR} '10},
	title = {Generating Document Summaries from User Annotations},
	isbn = {978-1-4503-0372-9},
	doi = {10.1145/1871962.1871978},
	abstract = {In this paper we analyze tweets that share the same link as another form of social annotation. By extracting all tweets that contain the same link over a period of time, we can generate a summary of what the crowd is writing about that particular link.},
	booktitle = {Proceedings of the Third Workshop on Exploiting Semantic Annotations in Information Retrieval},
	publisher = {{ACM}},
	author = {Shiells, Karen and Alonso, Omar and Lee, Ho John},
	year = {2010},
	keywords = {{citeDissProp}, crowdsourcing, social search, summarization, twitter},
	pages = {25--26},
	
}

@book{shirky_here_2009,
	title = {Here comes everybody},
	publisher = {Penguin Books},
	author = {Shirky, C.},
	year = {2009},
	keywords = {{citeDissProp}, cognitive surplus, gamification, {ICcited}, sorted},
	
}

@techreport{smucker_overview_2012,
	title = {Overview of the trec 2012 crowdsourcing track},

	institution = {{DTIC} Document},
	author = {Smucker, Mark D. and Kazai, Gabriella and Lease, Matthew},
	year = {2012},
	keywords = {{citeDissProp}, {TREC} crowd},
	
}

@inproceedings{snow_cheap_2008,
	address = {Stroudsburg, {PA}, {USA}},
	series = {{EMNLP} '08},
	title = {Cheap and fast--but is it good?: evaluating non-expert annotations for natural language tasks},
	shorttitle = {Cheap and fast -- but is it good?},
	abstract = {Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Snow, R. and O'Connor, B. and Jurafsky, D. and Ng, A.Y.},
	year = {2008},
	keywords = {annotation, {citeDissProp}, expertise, experts, {hcirCITE}, {hcirMidtermCITE}, mechanical turk, {mturkCITE}, tagging, turk},
	pages = {254--263},
	
}

@inproceedings{song_general_1999,
	address = {New York, {NY}, {USA}},
	series = {{CIKM} '99},
	title = {A General Language Model for Information Retrieval},
	isbn = {1-58113-146-1},
	doi = {10.1145/319950.320022},
	abstract = {Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and {TREC}4 data sets showed that the performance of our model is comparable to that of {INQUERY} and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance.},
	booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
	publisher = {{ACM}},
	author = {Song, Fei and Croft, W. Bruce},
	year = {1999},
	keywords = {{citeDissProp}, curve-fitting functions, good-turing estimate, model combinations, statistical language modeling},
	pages = {316--321},
	
}

@inproceedings{spiteri_social_2011,
	title = {Social discovery tools: Cataloguing meets user convenience},
	volume = {3},
	abstract = {This paper examines how library users access, use, and interact with two social discovery systems used in two Canadian public library systems. How do public library users interact with social discovery systems? How does usage between the two social discovery systems compare? Daily transaction logs of the social discovery systems used by the two libraries were compiled from May-August, 2010. Fifty sets of bibliographic records were compared to evaluate user-contributed content. Results indicate that features that allow for user-generated content are underused in both systems. Future research will thus focus on clients' motivations for engaging with the social features of social discovery systems, and their perceptions of, and satisfaction with, the benefits of these features.},
	booktitle = {Proceedings from North American Symposium on Knowledge Organization},
	author = {Spiteri, Louise F.},
	year = {2011},
	keywords = {bibliocommons, {citeDissProp}, {ICcited}, libraries, opac, sorted}
}

@inproceedings{springer_for_2008,
	title = {For the common good: The Library of Congress Flickr pilot project},
	shorttitle = {For the common good},

	author = {Springer, Michelle and Dulabahn, Beth and Michel, Phil and Natanson, Barbara and Reser, David W. and Ellison, Nicole B. and Zinkham, Helena and Woodward, David},
	year = {2008},
	keywords = {{citeDissProp}, {citeiConf}14},
	
}

@misc{taylor_friendfeed_2007,
	title = {{FriendFeed} Blog: I like it, I like it},
	shorttitle = {{FriendFeed} Blog},

	journal = {friendblog},
	author = {Taylor, Bret},
	month = oct,
	year = {2007},
	keywords = {{citeDissProp}, friendfeed, rating scales, ratings},
	
}

@article{thompson_if_2008,
	chapter = {Magazine},
	title = {If You Liked This, You’re Sure to Love That},
	issn = {0362-4331},
	abstract = {Basement hackers and amateur mathematicians are competing to improve the program that Netflix uses to recommend {DVDs} — and to win \$1 million in the process.},
	journal = {The New York Times},
	author = {Thompson, Clive},
	month = nov,
	year = {2008},
	keywords = {{citeDissProp}, Computers and the Internet, {DVD} (Digital Versatile Disc), Motion Pictures, Netflix Incorporated, Ratings and Rating Systems, Retail Stores and Trade},
	
}

@book{twain_adventures_1920,
	title = {The Adventures of Tom Sawyer},
	abstract = {The book that introduced the world to the iconic American characters of Tom Sawyer and Huckleberry Finn, this 1876 novel by Mark Twain follows the mischievous exploits of the two young boys, who find themselves in situations both humorous and dangerous. Never short of ways to stir up trouble in his hometown on the Mississippi River, Tom uses his wits to get both in and out of tight spots, often with Huck at his side. Featuring moments of significant social commentary, these interconnected tales essentially served as a dry run for Twain's notably weightier sequel, Adventures of Huckleberry Finn.},
	language = {en},
	publisher = {Harper \& brothers},
	author = {Twain, Mark},
	year = {1920},
	keywords = {{citeDissProp}}
}

@article{von_ahn_games_2006,
	title = {Games with a purpose},
	volume = {39},
	issn = {0018-9162},
	abstract = {Through online games, people can collectively solve large-scale computational problems.},
	number = {6},
	journal = {Computer},
	author = {von Ahn, L.},
	month = jun,
	year = {2006},
	keywords = {{citeDissProp}, gamification, serious games},
	pages = {96--98},
	
}

@techreport{von_hippel_sources_1988,
	address = {Rochester, {NY}},
	type = {{SSRN} Scholarly Paper},
	title = {The Sources of Innovation},
	abstract = {Presents a series of studies showing that the sources of innovation vary greatly; possible sources include innovation users, suppliers of innovation-related components, and product manufacturers. These types of roles are known as functional areas. Specific areas of innovation are marked by having innovators predominantly in one specific functional area. Using empirical data from industrial histories, the authors show that this innovation-function relationship has held in scientific instrument, semiconductor and printed circuit board assembly process innovations. Users are predominantly the innovators in these fields. Also identifies a few industries where manufacturers are typically the innovators and a few others where suppliers tend to be.        Analysis of the economic rents of innovation expected by potential innovators can often, if not always, by itself predict the functional source of innovation. Innovating firms will do so only when these rents prove attractive. Two factors suggest that this will tend to limit exploitation of the innovation to a functional area. First, it is difficult for innovators to adopt new functional relationships to their innovations. Second, innovators face a poor ability to capture innovation rents by licensing their innovation-related knowledge to others. This hypothesis and its implications are tested against the empirical datasets used initially. The role of informal R\&D know-how trading is also discussed and analyzed using the Prisoner's Dilemma. Guidance is given to innovation managers and policymakers.  ({CAR})},
	number = {{ID} 1496218},
	institution = {Social Science Research Network},
	author = {von Hippel, Eric},
	year = {1988},
	keywords = {{citeDissProp}, Economic rents, Industrial research, Information behavior, Information exchange, Innovation management, Innovation policies, Innovation process, Know-how, Licensing strategies, Manufacturing firms, R\&D, Suppliers, User needs},
	
}

@article{von_hippel_democratizing_2006,
	title = {Democratizing Innovation},

	author = {von Hippel, Eric},
	year = {2006},
	keywords = {{citeDissProp}},
	
}

@article{von_ahn_recaptcha:_2008,
	title = {recaptcha: Human-based character recognition via web security measures},
	volume = {321},
	shorttitle = {recaptcha},
	number = {5895},
	journal = {Science},
	author = {Von Ahn, Luis and Maurer, Benjamin and McMillen, Colin and Abraham, David and Blum, Manuel},
	year = {2008},
	keywords = {{citeDissProp}},
	pages = {1465--1468},
	
}

@incollection{vukovic_towards_2010,
	series = {Lecture Notes in Computer Science},
	title = {Towards a Research Agenda for Enterprise Crowdsourcing},
	copyright = {Springer Berlin Heidelberg},
	isbn = {978-3-642-16557-3, 978-3-642-16558-0},
	abstract = {Over the past few years the crowdsourcing paradigm has evolved from its humble beginnings as isolated purpose-built initiatives, such as Wikipedia and Elance and Mechanical Turk to a growth industry employing over 2 million knowledge workers, contributing over half a billion dollars to the digital economy. Web 2.0 provides the technological foundations upon which the crowdsourcing paradigm evolves and operates, enabling networked experts to work collaboratively to complete a specific task. Enterprise crowdsourcing poses interesting challenges for both academic and industrial research along the social, legal, and technological dimensions. In this paper we describe the challenges that researchers and practitioners face when thinking about various aspects of enterprise crowdsourcing. First, to establish technological foundations, what are the interaction models and protocols between the Enterprise and the crowd. Secondly, how is crowdsourcing going to face the challenges in quality assurance, enabling Enterprises to optimally leverage the scalable workforce. Thirdly, what are the novel (Web) applications enabled by Enterprise crowdsourcing.},
	number = {6415},
	booktitle = {Leveraging Applications of Formal Methods, Verification, and Validation},
	publisher = {Springer Berlin Heidelberg},
	author = {Vukovic, Maja and Bartolini, Claudio},
	editor = {Margaria, Tiziana and Steffen, Bernhard},
	month = jan,
	year = {2010},
	keywords = {business process modeling, {citeDissProp}, {citeiConf}14, Computer Communication Networks, crowdsourcing, Data Mining and Knowledge Discovery, Information Systems Applications (incl.Internet), Logics and Meanings of Programs, Programming Languages, Compilers, Interpreters, Software Engineering},
	pages = {425--434},
	
}

@misc{wales_insist_2006,
	title = {Insist on Sources},
	journal = {{WikiEN}-l},
	author = {Wales, Jimmy},
	month = jul,
	year = {2006},
	keywords = {{citeDissProp}}
}

@inproceedings{wallace_who_2011,
	title = {Who should label what? Instance allocation in multiple expert active learning},
	shorttitle = {Who should label what?},
	booktitle = {Proceedings of the {SIAM} International Conference on Data Mining ({SDM})},
	author = {Wallace, B. and Small, K. and Brodley, C. and Trikalinos, T.},
	year = {2011},
	keywords = {{citeDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	
}

@inproceedings{welinder_online_2010,
	title = {Online crowdsourcing: Rating annotators and obtaining cost-effective labels},
	isbn = {978-1-4244-7029-7},
	shorttitle = {Online crowdsourcing},
	doi = {10.1109/CVPRW.2010.5543189},
	abstract = {Labeling large datasets has become faster, cheaper, and easier with the advent of crowdsourcing services like Amazon Mechanical Turk. How can one trust the labels obtained from such services? We propose a model of the labeling process which includes label uncertainty, as well a multi-dimensional measure of the annotators' ability. From the model we derive an online algorithm that estimates the most likely value of the labels and the annotator abilities. It finds and prioritizes experts when requesting labels, and actively excludes unreliable annotators. Based on labels already obtained, it dynamically chooses which images will be labeled next, and how many labels to request in order to achieve a desired level of confidence. Our algorithm is general and can handle binary, multi-valued, and continuous annotations (e.g. bounding boxes). Experiments on a dataset containing more than 50,000 labels show that our algorithm reduces the number of labels required, and thus the total cost of labeling, by a large factor while keeping error rates low on a variety of datasets.},
	language = {English},
	booktitle = {2010 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	publisher = {{IEEE}},
	author = {Welinder, P. and Perona, P.},
	month = jun,
	year = {2010},
	keywords = {Adaptation model, amazon mechanical turk, {citeDissProp}, Computer vision, cost effective label, costing, Costs, dataset labeling service, Error analysis, {hcirCITE}, {hcirMidtermCITE}, image classification, label uncertainty, Labeling, multidimensional annotator ability measurement, multivalued continuous annotation, Noise figure, online crowdsourcing, Outsourcing, rating annotator, Web services},
	pages = {25--32},
	
}

@article{whitehill_whose_2009,
	title = {Whose vote should count more: Optimal integration of labels from labelers of unknown expertise},
	volume = {22},
	shorttitle = {Whose vote should count more},
	journal = {Advances in Neural Information Processing Systems},
	author = {Whitehill, J. and Ruvolo, P. and Wu, T. and Bergsma, J. and Movellan, J.},
	year = {2009},
	keywords = {{citeDissProp}, {hcirCITE}, {hcirMidtermCITE}},
	pages = {2035--2043},
	
}

@inproceedings{yilmaz_simple_2008,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '08},
	title = {A Simple and Efficient Sampling Method for Estimating {AP} and {NDCG}},
	isbn = {978-1-60558-164-4},
	doi = {10.1145/1390334.1390437},
	abstract = {We consider the problem of large scale retrieval evaluation. Recently two methods based on random sampling were proposed as a solution to the extensive effort required to judge tens of thousands of documents. While the first method proposed by Aslam et al. [1] is quite accurate and efficient, it is overly complex, making it difficult to be used by the community, and while the second method proposed by Yilmaz et al., {infAP} [14], is relatively simple, it is less efficient than the former since it employs uniform random sampling from the set of complete judgments. Further, none of these methods provide confidence intervals on the estimated values. The contribution of this paper is threefold: (1) we derive confidence intervals for {infAP}, (2) we extend {infAP} to incorporate nonrandom relevance judgments by employing stratified random sampling, hence combining the efficiency of stratification with the simplicity of random sampling, (3) we describe how this approach can be utilized to estimate {nDCG} from incomplete judgments. We validate the proposed methods using {TREC} data and demonstrate that these new methods can be used to incorporate nonrandom samples, as were available in {TREC} Terabyte track '06.},
	booktitle = {Proceedings of the 31st Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Yilmaz, Emine and Kanoulas, Evangelos and Aslam, Javed A.},
	year = {2008},
	keywords = {average precision, {citeDissProp}, evaluation, incomplete judgments, {infAP}, {nDCG}, sampling},
	pages = {603--610},
	
}

@inproceedings{zhai_study_2001,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '01},
	title = {A study of smoothing methods for language models applied to Ad Hoc information retrieval},
	isbn = {1-58113-331-6},
	doi = {10.1145/383952.384019},
	abstract = {Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model. A core problem in language model estimation is smoothing, which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. In this paper, we study the problem of language model smoothing and its influence on retrieval performance.  We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections.},
	booktitle = {Proceedings of the 24th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Zhai, Chengxiang and Lafferty, John},
	year = {2001},
	keywords = {basic concepts, {citeDissProp}},
	pages = {334--342},
	
}

@inproceedings{zhai_model-based_2001,
	address = {New York, {NY}, {USA}},
	series = {{CIKM} '01},
	title = {Model-based Feedback in the Language Modeling Approach to Information Retrieval},
	isbn = {1-58113-436-3},
	doi = {10.1145/502585.502654},
	abstract = {The language modeling approach to retrieval has been shown to perform well empirically. One advantage of this new approach is its statistical foundations. However, feedback, as one important component in a retrieval system, has only been dealt with heuristically in this new retrieval approach: the original query is usually literally expanded by adding additional terms to it. Such expansion-based feedback creates an inconsistent interpretation of the original and the expanded query. In this paper, we present a more principled approach to feedback in the language modeling approach. Specifically, we treat feedback as updating the query language model based on the extra evidence carried by the feedback documents. Such a model-based feedback strategy easily fits into an extension of the language modeling approach. We propose and evaluate two different approaches to updating a query language model based on feedback documents, one based on a generative probabilistic model of feedback documents and one based on minimization of the {KL}-divergence over feedback documents. Experiment results show that both approaches are effective and outperform the Rocchio feedback approach.},
	booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
	publisher = {{ACM}},
	author = {Zhai, Chengxiang and Lafferty, John},
	year = {2001},
	keywords = {{citeDissProp}},
	pages = {403--410},
	
}

@inproceedings{zhou_exploring_2008,
	address = {New York, {NY}, {USA}},
	series = {{WWW} '08},
	title = {Exploring Social Annotations for Information Retrieval},
	isbn = {978-1-60558-085-2},
	doi = {10.1145/1367497.1367594},
	abstract = {Social annotation has gained increasing popularity in many Web-based applications, leading to an emerging research area in text analysis and information retrieval. This paper is concerned with developing probabilistic models and computational algorithms for social annotations. We propose a unified framework to combine the modeling of social annotations with the language modeling-based methods for information retrieval. The proposed approach consists of two steps: (1) discovering topics in the contents and annotations of documents while categorizing the users by domains; and (2) enhancing document and query language models by incorporating user domain interests as well as topical background models. In particular, we propose a new general generative model for social annotations, which is then simplified to a computationally tractable hierarchical Bayesian network. Then we apply smoothing techniques in a risk minimization framework to incorporate the topical information to language models. Experiments are carried out on a real-world annotation data set sampled from del.icio.us. Our results demonstrate significant improvements over the traditional approaches.},
	booktitle = {Proceedings of the 17th International Conference on World Wide Web},
	publisher = {{ACM}},
	author = {Zhou, Ding and Bian, Jiang and Zheng, Shuyi and Zha, Hongyuan and Giles, C. Lee},
	year = {2008},
	keywords = {{citeDissProp}, folksonomy, {INFORMATION} retrieval, language modeling, social annotations},
	pages = {715--724},
	
}

@article{zwass_co-creation:_2010,
	title = {Co-Creation: Toward a Taxonomy and an Integrated Research Perspective},
	volume = {15},
	doi = {10.2753/JEC1086-4415150101},
	number = {1},
	journal = {International Journal of Electronic Commerce},
	author = {Zwass, Vladamir},
	year = {2010},
	keywords = {{citeDissProp}, {citeiConf}14, co-creation, crowdsourcing, Taxonomy},
	pages = {11--48},
	
}

@book{simon_participatory_2010,
	title = {The participatory museum},

	publisher = {Museum 2.0},
	author = {Simon, Nina},
	year = {2010},
	
}

@misc{_requester_2011,
	title = {Requester Best Practices},
	abstract = {Amazon Mechanical Turk is a marketplace for work where businesses (aka Requesters) publish tasks (aka 
{HITS}), and human providers (aka Workers) complete them. Amazon Mechanical Turk gives businesses 
immediate access to a diverse, global, on-demand, scalable workforce and gives Workers a selection of 
thousands of tasks to complete whenever and wherever it's convenient. 
There are many ways to structure your work in Mechanical Turk. This guide helps you optimize your 
approach to using Mechanical Turk to get the most accurate results at the best price with the turnaround 
time your business needs. Use this guide as you plan, design, and test your Amazon Mechanical Turk 
{HITs}},
	publisher = {Amazon Web Services {LLC}},
	month = jun,
	year = {2011},
	keywords = {{citeDissProp}}
}

@misc{_wikipedia:size_2014,
	title = {Wikipedia:Size of Wikipedia},
	copyright = {Creative Commons Attribution-{ShareAlike} License},
	shorttitle = {Wikipedia},
	abstract = {This Wikipedia:Statistics page measures the size of the English-language edition of Wikipedia; mostly page and article count. There are currently 4,579,708 articles in the English Wikipedia.},
	language = {en},
	journal = {Wikipedia, the free encyclopedia},
	month = aug,
	year = {2014},
	note = {Page Version {ID}: 615924147},
	keywords = {{citeDissProp}},
	
}

@misc{_about_????,
	title = {About Pinterest},

	journal = {Pinterest},
	keywords = {{citeDissProp}},
	
}

@misc{_bias_????,
	title = {bias, adj., n., and adv.},
	language = {en-{GB}},
	journal = {{OED} Online},
	publisher = {Oxford University Press},
	keywords = {{citeDissProp}},
	
}

@misc{_lists_????,
	title = {Lists},

	journal = {Bibliocommons},
	keywords = {{citeDissProp}, {ICcited}, sorted}
}

@misc{_what_????,
	title = {What is {reCAPTCHA}?},

	journal = {Recaptcha},
	keywords = {{citeDissProp}},
	
}
