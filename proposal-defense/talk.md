% Crowdsourcing metadata
% _Peter Organisciak_
% Oct 15, 2014

__Committee__

* Chair: Michael B. Twidale
* J. Stephen Downie
* Miles Efron
* Jaime Teevan

<!--% Reliably augmenting documents by collecting and interpreting amateur contributions-->

# Introduction

## Crowdsourcing

<div class="notes">
Today I'm going to talk about when people create things online.
I'm referring, of course, to crowdsourcing.
You may already know this term; Wolf Blitzer knows it, and your parents may too.
If you're unfamiliar with it, that's okay, I'll return to the fundamentals of crowdsourcing in a moment.
Most basically, it refers to when many people, connected by technology, contribute to a common product.
Sometimes this is as volunteers -- an act as deliberate as Wikipedia editing or reviewing products on Amazon, or as incidental as teaching Facebook about quality posts by clicking the 'like' button.
Other times, crowdsourcing involves paid crowds, such as those that participate in the micro-task marketplace called Mechanical Turk.
</div>

## Crowdsourcing for Metadata

Crowdsourcing as a _reactive_ act, capturing human interpretations and reactions to information

<div class="notes">
Today, however, I'm talking about a specific use of crowdsourcing: to teach us more about existing information objects.
Our focus today is on place crowdsourcing as a _reactive_ act, capturing human interpretations and reactions to information, teaching us more about the nature of the object.
</div>

## Cultural Heritage Collections { data-background='images/bentham1.jpg' }

Transcribe Bentham

## In Libraries <!-- #1-->

Bibliocommons

## In Information Retrieval

PageRank

## Quote

placeholder
Rose Holley - DHQ ??

## The value of crowds

<div class=notes>
On the value of crowds for descriptive metadata

- Large-scale
- Quick

Most of all _human_.

Humans can provide latent information about documents that would not be possible to ascertain computationally, such as quality judgments or higher-level thematic description.
They are also adept at critical actions such as correcting, describing in different language, or inferring relationships with other documents.
More importantly, crowdsourcing looks at human contribution at scales that are potentially useful for retrieval.
</div>

## _The_ Question { data-background='#ff0000'}
How do we control and interpret qualitative user contributions in an inherently quantitative system?

<div class='notes'>
This, of course, is a large, overarching question, and I'll get into my study's particular scope in a moment.
What I'm hinting at here, however, is that 
</div>


## Study in two parts

1. On _collecting_ reliable contributions

> How does the way you ask change the contributions you receive?

\

2. On _using_ crowd contributions

> How should a system interpret crowd contributions?

<div class=notes>
1. How does the way you ask change the contribution you receive?
   - Here, this work will perform a comparative study of crowdsourcing task design for the same task, looking at the effect of changes to the consistency and reliability of user-contributed information.
  * This is a problem that has been noted often by other studies, but not directly measured.

2. How should a system interpret crowd contributions?
  * Here, this work will focus on information retrieval for a system that is heavy on user contributions but light on other textual information. 
  * The system I look at, image bookmarking site Pinterest, is also an example of the loosely constrained form of crowdsourcing contribution that is often required to encourage participation, a trade-off that is less structured than would be preferable for retrieval model.

</div>

## Goals

To provide a comprehensive resource for understanding crowdsourcing for descriptive metadata, tying together existing research, while filling knowledge dark spots with new research.
A humanistic system-end view of crowds, colored by my environment in information science and my focus on information retrieval.

## Assumptions, Scoping, and Biases 1

A humanistic system-end view of crowds, colored by my environment in information science and my focus on information retrieval.

## Assumptions, Scoping, and Biases 2

This study works within an assumption of _honest-but-biased_ crowds.

> - Bad, malicious, and inattentive contributors are a reality in crowdsourcing, but a well-studied reality. 


## Takeaway


## Overview

<div class="notes">
Include an incremental table of contents here
</div>

## Why

- A
- B

.... I'll get back to this question


## Scope

## Assumptions

<div class="notes">
</div>


# Crowdsourcing <!--Lit Review-->

## Types of Crowdsourcing

## Greatest Hits

## Crowdsourcing in Information Science

## Crowdsourcing In Information Retrieval { data-background="images/mountains3-glitched.png" }

## e.g

* Alonso
* Watts

# Collecting crowd contributions { data-background="images/mountains2-glitched.png" }

Overview

## Intro

# Using crowd contributions in an online system

# References

# Thank You { data-background="images/14892129119_97860d8a3d_k-glitched-a99-s74-i8-q90.png" } 

## Image Credits

- Melodysheep - https://www.flickr.com/photos/melodysheep/4918358272
- David See - https://www.flickr.com/photos/dave_see/9231275034
